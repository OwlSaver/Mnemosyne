\chapter{Introduction}
% Due 2/15/2025
% 5-7 pages
\label{chap:introduction}

\section{Background and Research Motivation}
Ensuring document quality\index{Document Quality} involves verifying completeness\index{Completeness}, consistency\index{Consistency}, and correctness\index{Correctness} \parencite{RefWorks:RefID:10-zowghi2003interplay}. While evaluating correctness often necessitates access to knowledge external to the document and understanding the document's intent, completeness and consistency can be assessed using only the document's internal content. This research focuses on developing automated methods using Large Language Models (LLMs)\index{Large Language Models (LLMs)} to address the latter two aspects. The specific focus is on converting a large document into a knowledge graph\index{Knowledge Graphs} that can be used in future research to check for document consistency and completeness.

\subsection{Background}
The increasing complexity and scale of textual documents in various domains present significant challenges to ensuring consistency and completeness. Legal codes, technical documentation, and regulatory frameworks are often drafted collaboratively over extended periods, a process that can lead to inconsistencies, redundancies, and informational gaps. Traditional manual review methods, while necessary, are labor-intensive and prone to human oversight, making automated solutions an attractive alternative. Advances in Natural Language Processing (NLP)\index{Natural Language Processing (NLP)} and Artificial Intelligence (AI) have introduced new methodologies for analyzing and structuring large bodies of text, with promising applications in document validation and knowledge extraction.

At the core of modern NLP advancements are Transformer-based models\index{Transformer Model} that rely on the Attention Mechanism\index{Attention Mechanism} to understand and generate text. LLMs, which build upon this foundation, can process and interpret vast amounts of textual data, although they are constrained by fixed context windows\index{Large Language Models (LLMs)!context window}. To address this limitation, structured approaches such as knowledge graphs have emerged, enabling explicit representation of entities\index{Knowledge Graphs!entities} and relationships\index{Knowledge Graphs!relationships} within documents. This research applies these technologies to Pennsylvania township laws\index{Pennsylvania Township Laws}, a domain where maintaining consistency is particularly critical. Given the size and complexity of municipal codes, inconsistencies in legal definitions, zoning regulations, and procedural rules can lead to legal disputes and financial losses. By leveraging AI-driven tools, this study aims to develop a framework for systematically analyzing and improving the consistency of legal documents.

Ensuring structural consistency and completeness in documents has been a longstanding challenge across various domains. Previous research has focused on methods to maintain internal coherence\index{Coherence} within documents \parencite{RefWorks:RefID:13-laban2021transformer}, while other studies have explored domain-specific approaches to consistency checking \parencite{RefWorks:RefID:27-tröls2022instant}. In academic literature, the term coherence is often used interchangeably with consistency \parencite{RefWorks:RefID:14-shen2021evaluating}, reflecting the broader goal of ensuring logical and semantic alignment within textual content.

In 2017, a research team at Google introduced the Transformer model, a neural network architecture based entirely on the Attention Mechanism \parencite{RefWorks:RefID:81-vaswani2017attention}. Unlike previous sequential models, the Transformer processes all words within a given input simultaneously, allowing it to assess how each word influences others across the text. Using self-attention\index{Attention Mechanism!self-attention}, this architecture captures long-range dependencies more effectively than earlier models. Despite advances in scaling Transformer-based models, they remain constrained by a limited attention window due to memory and computational efficiency considerations.

Large Language Models are built upon the Transformer architecture and inherit its fundamental attention-based mechanisms. These models, however, are constrained by a fixed context window, which limits the amount of text they can analyze at once. As documents grow in length, they often exceed this window, preventing comprehensive processing in a single pass. Despite this limitation, document analysis does not necessarily require attending to an entire document simultaneously. Instead, LLMs can be employed to extract key entities and concepts across different sections, enabling a more focused and structured approach to consistency checking. Through the identification of entities and the analysis of their relationships, LLMs can effectively navigate large documents while maintaining efficiency.

Knowledge graphs provide a structured, human-readable representation of information, serving as an alternative to the implicit encoding of knowledge found in neural networks. A knowledge graph is a directed acyclic graph (DAG)\index{Knowledge Graphs!directed acyclic graph} in which nodes\index{Knowledge Graphs!nodes} represent entities and edges\index{Knowledge Graphs!edges} define the relationships between them. Each node can possess attributes\index{Knowledge Graphs!attributes} that enrich its descriptive properties. For instance, a node representing a car might include attributes such as color, model, or manufacturer. A useful way to conceptualize knowledge graphs is through the framework of frames, as described by Minsky \parencite{RefWorks:RefID:78-minsky1974framework}. In contrast to LLMs, which rely on statistical inference, knowledge graphs offer explicit, interpretable relationships that can be leveraged for consistency and completeness verification in structured documents.

Pennsylvania is home to over 1,200 townships of the second class, each responsible for drafting and maintaining its own set of municipal laws\index{Municipal Laws}. These laws regulate a wide range of local governance areas, including police services, fire departments, zoning, and land development. Over time, the cumulative nature of legal amendments introduces inconsistencies and gaps, which, if left unaddressed, can lead to legal ambiguities and enforcement challenges. Although legal professionals and municipal officials work diligently to identify and resolve these issues, the complexity of these documents—often spanning thousands of pages—renders manual review error-prone and inefficient.

A key source of complexity is the interdependence of different sections within municipal codes. For example, early sections may define zoning regulations\index{Municipal Laws!zoning regulations}, specifying minimum frontage, setbacks, and other boundary constraints for different zoning districts. Inconsistencies can arise, however, when later sections introduce or reference zoning areas that were never formally defined. Similar discrepancies can emerge across other regulatory provisions, necessitating careful synchronization of legal language and definitions. Ensuring consistency across these interconnected legal elements is a critical challenge that demands a more systematic and automated approach to legal document analysis.

\subsection{Research Motivation}
Despite extensive research on the analysis of small documents or specific document sections, a significant gap remains in addressing the challenges of comprehensive, large-scale document analysis. The need for automated consistency and completeness checks\index{Consistency!automated checks}\index{Completeness!automated checks} is critical in various industries where these tasks are often performed manually, requiring substantial time and resources while still potentially yielding suboptimal results. This research aims to bridge this gap by developing an effective and efficient automated solution.

The process for publishing local regulations in Pennsylvania townships exemplifies these challenges. After a governing body enacts a law, it is sent to an organization for compilation into the township's existing legal code. This manual and intensive process involves determining if any existing laws are affected by the new one. Even with this careful review, new laws frequently render the existing legal framework incomplete or inconsistent, thereby motivating the present research.

\section{Problem Statement}
\textit{Municipal laws in Pennsylvania Townships, authored by multiple people over time, develop inconsistencies and are incomplete \parencite{RefWorks:RefID:144-curley2024municipal,RefWorks:RefID:145-rau2024municipal,RefWorks:RefID:146-sanders2024municipal}, leading to annual revenue losses of hundreds of thousands of dollars. \parencite{RefWorks:RefID:147-bosco2024leading}}\index{Problem Statement}

The complexity of municipal laws in Pennsylvania townships arises from their incremental development. Ordinances and regulations are often drafted by different individuals, including elected officials, legal counsel, and administrative staff, each contributing to the evolving legal framework. This decentralized process can lead to inconsistencies in language, overlapping provisions, and unintended gaps in regulatory coverage. As laws are amended or new ones are introduced, prior statutes may not be adequately reconciled, further exacerbating these issues. Without a systematic approach to maintaining legal coherence, townships face challenges in enforcing their laws effectively and equitably.

The consequences of these inconsistencies extend beyond legal ambiguity. Incomplete or conflicting municipal laws can create loopholes that hinder a township’s ability to collect fees, fines, and other sources of revenue. For example, unclear zoning regulations may allow developments to proceed without appropriate permits or impact fees, and ambiguous tax ordinances can lead to disputes that reduce collections. When enforcement mechanisms are weakened by gaps in the legal framework, municipalities struggle to ensure compliance, leading to significant financial losses\index{Municipal Laws!revenue loss}. These inefficiencies, compounded over time, place additional strain on local budgets and limit resources for essential public services and infrastructure improvements.

Addressing these issues requires a structured methodology for analyzing, refining, and maintaining municipal laws. Traditional legal review processes, while valuable, are labor-intensive and reactive, often identifying issues only after disputes or financial shortfalls have arisen. Advances in artificial intelligence, particularly the use of LLMs, offer a potential solution by systematically identifying inconsistencies, redundancies, and gaps within legal texts. By applying LLMs to municipal laws, townships could proactively assess their legal frameworks, thereby improving clarity, enforcement, and financial sustainability. The implementation of such an approach, however, requires careful consideration of computational constraints, document formats, and the broader applicability of AI-driven legal analysis.

\section{Thesis Statement}
\textit{This praxis demonstrates that Mnemosyne, an LLM-based tool designed to convert documents into attributed knowledge graphs\index{Knowledge Graphs!attributed} can be used to check for consistency and completeness, which will allow municipal lawyers to create consistent and complete law documents, thereby preventing costly disputes and reducing revenue losses.}\index{Thesis Statement}

The application of LLMs in legal document analysis has the potential to revolutionize municipal law by providing an automated, systematic approach to ensuring consistency and completeness. Traditional legal drafting and review processes rely heavily on human oversight and are consequently susceptible to errors, especially in laws that have evolved over time through multiple amendments and contributors. By leveraging an LLM-based tool to convert legal documents into attributed knowledge graphs, municipalities can proactively identify gaps, redundancies, and contradictions before laws are enacted or enforced. This proactive approach serves to minimize ambiguity, strengthen legal clarity, and enhance the efficiency of legal review processes.

A knowledge graph-based representation of municipal laws enables a structured, machine-readable format that facilitates logical analysis. Unlike traditional text-based legal review, which requires extensive manual effort to trace dependencies and resolve conflicts, a knowledge graph explicitly maps relationships between legal provisions, definitions, and enforcement mechanisms. This structure allows municipal lawyers to assess the interconnectivity of legal clauses and verify their consistency against established legal principles. Furthermore, an attributed knowledge graph can highlight areas where laws are incomplete or misaligned with overarching governance policies, enabling timely revisions that improve legal coherence.

Beyond improving legal clarity, the ability to create consistent and complete municipal laws has direct financial implications. Inconsistent or incomplete regulations can lead to disputes over zoning, taxation, and permitting, often resulting in costly litigation or lost revenue from unenforceable provisions. By employing an LLM-driven tool to detect and resolve these issues at the drafting stage, municipalities can reduce legal ambiguities that might otherwise be exploited. This strengthens fiscal sustainability by preventing revenue leakage and ensuring that all applicable fees, fines, and taxes are properly assessed and collected.

The integration of LLM-based tools into municipal lawmaking represents a transformative step toward modernizing local governance. As artificial intelligence continues to advance, municipalities that adopt such technologies will gain a significant advantage in maintaining legally sound and financially sustainable frameworks. Future research could extend this approach beyond municipal laws to other domains of legal and regulatory governance, demonstrating the broader impact of AI-driven knowledge representation on ensuring legal accuracy and reducing administrative burdens.

\section{Research Objectives}
The primary objective of this research is to develop a tool capable of automatically processing documents of any size into a coherent set of entities within a knowledge graph. This tool will leverage advanced techniques to analyze document content, identify potential entities, and provide access to the resulting knowledge graph.\index{Research Objectives}

The created knowledge graph will then be analyzed to determine its suitability for checking the source document for inconsistencies and incompleteness. This evaluation will involve introducing targeted issues into the source documents and subsequently demonstrating the ease with which these issues can be observed and identified within the knowledge graph representation.

\section{Research Questions}
To achieve the research objectives, the following research questions will be addressed.\index{Research Questions}

\textbf{RQ1:} Can an LLM be used to convert a large document into a knowledge graph?\par

\textbf{RQ2:} Can an LLM be used to process multiple knowledge graphs into a typed cluster of knowledge graphs\index{Knowledge Graphs!typed cluster}.\par

\textbf{RQ3:} Can a typed cluster of knowledge graphs be used to check the source document for consistency and completeness?\par
\section{Research Hypotheses}
This research will test the following hypotheses.\index{Research Hypotheses}

\textbf{H1:} An LLM can be used to convert a large document into a knowledge graph.\par

\textbf{H2:} An LLM can be used to process multiple knowledge graphs into a typed cluster of knowledge graphs.\par

\textbf{H3:} A typed cluster of knowledge graphs can be used to check the source document for consistency and completeness.\par

\section{Research Scope and Limitations}
The subsequent sections outline the scope and limitations of this study. This research employs Pennsylvania township laws as a case study to develop and evaluate an automated tool for analyzing legal documents. These publicly accessible laws, having undergone extensive manual reviews, provide a rigorous benchmark for assessing the proposed methodology. The primary focus is the construction of Knowledge Graphs that faithfully represent document structure and content, thereby laying the groundwork for future work in verifying legal consistency and completeness. This study acknowledges several inherent limitations, including computational constraints, challenges associated with specific document formats, and a primary emphasis on textual analysis, all of which underscore the need for continued research.
\subsection{Research Scope}

This study focuses on the use of Pennsylvania township laws as a case study for developing and testing an automated tool designed to analyze legal documents. These laws, publicly available in PDF and Word formats, were selected for their complexity, length, and history of multiple authorships. Having undergone rigorous manual reviews, they serve as an ideal benchmark for evaluating the effectiveness of the proposed approach. Although the primary application is in the legal domain, the methodology is designed to be adaptable for broader use across various document types.\index{Scope}

The core of this research centers on constructing Knowledge Graphs that accurately represent the structure and content of the documents under review. These graphs will serve as a foundation for future work in verifying legal consistency and completeness. While this study will assess the suitability of the generated Knowledge Graphs for such tasks, the actual implementation of automated consistency and completeness checks is deferred to future research. This approach ensures a focused and systematic exploration of Knowledge Graph generation while establishing a foundation for subsequent advancements in automated legal analysis.

\subsection{Research Limitations}
This study has several potential limitations. Computational constraints\index{Limitations!computational constraints} may affect the efficiency and scalability of processing large and complex legal documents. Challenges may also arise in handling specific document formats and linguistic intricacies, particularly in ensuring accurate interpretation and structuring of legal text. Furthermore, while this research focuses on leveraging existing LLMs such as Gemini\index{Large Language Models (LLMs)!Gemini} and ChatGPT\index{Large Language Models (LLMs)!ChatGPT}, it does not involve developing specialized models tailored for knowledge graph construction. Such specialization could be an avenue for future work to reduce computational costs and energy consumption.\index{Limitations}

This research does not perform direct testing for consistency and completeness. Instead, it utilizes Pennsylvania township laws that are publicly available and have already undergone such validation, treating them as a gold standard. Future studies should explore the applicability of this approach to a broader range of legal and non-legal documents where ground-truth validation is not pre-existing.

For document handling, this research primarily uses Word documents to facilitate modifications during testing. Although the methodology is expected to be compatible with PDFs, further research is needed to confirm seamless integration and processing across different file formats.

Finally, this study is limited to textual analysis. Future work could expand upon this research by incorporating non-textual elements such as tables, formulas, images, and diagrams to achieve a more holistic document comprehension and analysis.

\section{Praxis Organization}

The remainder of this research is organized into several key chapters. Chapter 2 provides a comprehensive review of the relevant literature, focusing on the creation of knowledge graphs from documents by LLMs, the processing of multiple knowledge graphs by LLMs, and the utility of knowledge graphs in ensuring document consistency and completeness. This review also covers the process of creating and maintaining local laws in Pennsylvania. Chapter 3 delves into the statistical and machine learning methodologies employed, detailing the processes of data pre-processing, model selection, training, and evaluation. Chapter 4 presents and analyzes the results, addressing each research question and hypothesis while evaluating the performance of the proposed methodology. Finally, Chapter 5 concludes the investigation with a discussion of the key findings, contributions, and recommendations for practical applications, as well as potential avenues for future research.
