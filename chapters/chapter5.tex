\chapter{Discussion and Conclusions}
% Due by November 1, 2025 Includes entire Praxis
% 3 to 5 pages
\section{Conclusion}

Failures - it takes too long to run, it will take many more cycles to form a rich semantic hierarchy, maybe hundreds or thousands of cycles, the KG did not have entity properties even though the LLM was asked to create them.

When I started this I had ample evidence that this was needed. I had specific evidence from problems that Easttown township faced due to inconsistencies in laws. I knew from my experience that teams had issues creating large documents that were complete and consistent. Late changes were often not applied accross a document or set of documents.

I was surprised by how little research was going on in this area. there was a lot of research on small documents. There was a lot of research on summarizing oducuments. But little to none on large documents. Large documents are where the power of computers could come into play.

I thought and still belive that my approach of layering the processing and working on small randomply selected sections can allow for the processing of large documents. I underestimated how hard this would be. Even LangExtract from Google has toruble with larger documents.

The approach did show that it could work. It needs custom LLMs defiened for the specific needs. It needs

\section{Contribution to the Body of Knowledge}


\section{Recommendations for Future Research}

Run the processes asynchronously
Create specific LLM for each task
Build the Query Processing
Build the response processing agent
Build the ability to validate documents
Prepossessing the copra to identify initial Node Labels and Relationship Types
DO not clear the LTM for each run, processing the same document multiple times just serves as reinforcement
Try a very large number of cycles - 1,000 or more
Make all parts run asynchronously with blocking
For each entity you find in the source, attach it to an entity in LTM, start with Thing. How do we do this efficiently in a bounded number of steps? Maybe start at the root, Thing, and head up the tree picking the most likely for up to n levels.
Stop refinement that flips back and forth.
Create a set of documents and associated ground truth knowledge graph. The documents should be about 3,000 pages. There should be variations with specific introduced errors. This would support further research on large documents.
