%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% CHAPTER 4: Results and Analysis
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\chapter{Results and Analysis}
\label{chap:results}

% This chapter is due by October 15, 2025.
% The target length is 25 to 30 pages.

\section{Chapter Overview}
\label{sec:results_overview}

This chapter presents the empirical results obtained from the experiments detailed in \cref{chap:methodology}. The findings are organized to systematically address the research questions and hypotheses concerning the use of Large Language Models (LLMs) to construct knowledge graphs (KGs) for document validation. The core of this research explores a framework built on three dimensions of validation: correctness, completeness, and consistency.

The chapter begins by detailing the experimental setup, including the computational environment, datasets, and the methodology for establishing ground truth. It then presents the outcomes of preliminary experiments focused on hyperparameter tuning, which established the optimal configuration for the main experimental runs.

The main results are presented in three parts, structured according to the evaluation framework:
\begin{enumerate}
    \item \textbf{Knowledge Graph Fit Assessment:} An evaluation of the quality, traceability, and structural integrity of the generated KGs.
    \item \textbf{Suitability for Consistency and Completeness (C\&C) Analysis:} An assessment of the KG's utility for performing automated validation tasks.
    \item \textbf{Controlled Error Injection Experiment:} A quantitative analysis of the system's ability to detect seeded inconsistencies and instances of incompleteness.
\end{enumerate}

Following the presentation of data, the chapter provides an in-depth analysis and interpretation of the findings. This analysis refines the initial research hypothesis, particularly by introducing a more nuanced, two-part understanding of \textit{completeness}. Finally, each research hypothesis is explicitly validated against the collected evidence, and the chapter concludes with a summary of the key results, setting the stage for the discussion in the following chapter.

\section{Experimental Setup}
\label{sec:exp_setup}

To test the hypotheses, a software tool, hereafter referred to as Mnemosyne, was developed. The tool's pipeline digests a source document, splits it into manageable chunks, generates a shallow knowledge graph for each chunk, and then merges these individual graphs. A key feature of this process is the resolution of syntactical differences and the subsequent refinement of the combined graph into a deep semantic network, enriched with \texttt{IS\_A} and \texttt{PART\_OF} relationships. The efficacy of this tool was evaluated through a series of structured experiments.

\subsection{Environment and Datasets}
\label{subsec:env_data}

\subsubsection{Computational Environment}

The experiments were conducted using the technical environment specified in \cref{sec:methodology_env}, which includes Python (v3.8+), the LangChain framework for LLM interaction, and a Neo4j (v5.x) graph database for KG storage and querying.

Initial development was performed using the free tier of Google Colab. As the processing demands of the documents increased, the environment was upgraded to Colab Pro and subsequently Colab Pro+ to accommodate longer runtimes (up to 24 hours). An alternative local machine configuration was also developed to ensure flexibility, though this introduced challenges with version control integration, leading to the decision to primarily rely on the stable Colab Pro+ environment for the final experimental runs. This setup required active monitoring, as the connection to the runtime could be lost if the local machine entered sleep mode.

\subsubsection{Documents}
Two categories of documents were used for the experiments: test documents for development and ground truth comparison, and larger municipal legal codes for scaled evaluation.

\begin{enumerate}
    \item \textbf{Test Documents:} Four small documents were created to facilitate rapid development and testing. Three of these documents are approximately one page in length, while the fourth is four pages. These documents were used to test the coding logic, fine-tune hyperparameters, and compare automated KG generation against a manually created ground truth.
    \item \textbf{Small-Scale Experimental Document:} The Code of Ordinances for the Conewago Township Sewer Authority, an 82-page document, was used as the primary subject for the controlled error injection experiments. Four versions of this document were prepared:
    \begin{itemize}
        \item \textbf{Base Document:} The original, unmodified text.
        \item \textbf{Incompleteness Injection:} The base document with the section on "Delinquency" entirely removed.
        \item \textbf{Inconsistency Injection (Dissimilar Topic):} The base document with a section on "Driveways" from the Easttown Township code inserted.
        \item \textbf{Inconsistency Injection (Similar Topic):} The base document with a section on "Prohibited Wastes" from the Easttown Township sewer code inserted.
    \end{itemize}
    \item \textbf{Large-Scale Experimental Document:} The Easttown Township Code, a comprehensive 732-page document, was used to test the scalability and performance of the system on a significantly larger corpus.
\end{enumerate}

\subsection{Ground Truth Establishment}
\label{subsec:ground_truth}
To quantitatively evaluate the performance of the KG extraction pipeline, a ground truth was established. For the four smaller test documents, entities and relationships were manually annotated to create a gold standard.

To supplement this manual effort and provide a benchmark for larger documents where manual annotation is impractical, Google's LangExtract tool was employed to automatically generate a baseline ground truth. By comparing the manually created ground truth with the LangExtract-generated ground truth for the test documents, a benchmark for the automated method's accuracy was established. It is important to note that both the manual and LangExtract-generated ground truths consist of one-level hierarchies. In contrast, the Mnemosyne system is designed to build deep, multi-level type hierarchies. This architectural difference inherently limits the maximum achievable F1 scores in direct comparison but provides a valuable baseline for extraction quality.

\subsection{Experiment Groups}
\label{subsec:exp_groups}
The experiments were organized into logical groups, identified by a numerical series, to ensure systematic execution and reproducibility. The structure of these groups is outlined in \cref{tab:exp_groups}.

\begin{table}[!htbp]
\centering
\caption{Overview of Experiment Groups.}
\label{tab:exp_groups}
\begin{tabular}{@{}lll@{}}
\toprule
\textbf{Group Range} & \textbf{Group Name} & \textbf{Purpose} \\ \midrule
0--99      & Utility         & Housekeeping scripts, such as clearing the database before a run. \\
100--199   & Ground Truth    & Generation of ground truth KGs using Google's LangExtract tool. \\
200--299   & Regression Test & Core functionality tests used for regression testing during development. \\
300--399   & Hyperparameter  & Experiments designed to select optimal hyperparameters. \\
400--499   & Main Experiments& Full experimental runs on the small and large documents. \\ \bottomrule
\end{tabular}
\end{table}


\section{Preliminary Experiments: Hyperparameter Tuning}
\label{sec:hyperparameter_tuning}
Prior to conducting the main experiments, a series of preliminary tests were run to determine the optimal hyperparameters for the data processing pipeline. The parameters tuned included the LLM model, LLM temperature, document chunking strategy, and the number of refinement cycles in the Long-Term Memory (LTM) consolidation process, as outlined in \cref{tab:methodology_hyperparameters}. Based on these tests, the configuration detailed in \cref{tab:final_hyperparameters} was selected for its balance of performance, cost, and accuracy.

\subsection{Refinement Cycles}
The LTM consolidation process iteratively refines the knowledge graph. Experiments were conducted with 0, 1, 2, 3, 5, 8, 13, and 21 refinement cycles. Against the manual ground truth, the peak entity F1-score of 81.48\% was achieved at 0, 5, and 8 cycles. Against the automated ground truth, the peak F1-score of 24.62\% was also observed at these points. \textit{[Placeholder: Briefly explain why 5 or 8 cycles was chosen over 0, perhaps due to better relationship scores or qualitative improvements not captured by F1.]}

\subsection{Chunk Size}
Experiments were run with chunk sizes of 4, 8, and 12 sentences, with a fixed overlap of 3 sentences. For the small test documents, both F1-scores and processing times improved with larger chunk sizes. This suggests that a larger chunk size, which provides more context to the LLM, is beneficial. The primary constraint is the context window limit of the selected LLM. Based on these findings, a chunk size of 20 sentences with an overlap of 5 was selected for the larger document experiments.

\subsection{Model Selection}
Due to time and funding constraints, the primary model comparison was between OpenAI's GPT-4.1 and Google's Gemini 4.5. \textit{[Placeholder: Verify model names, e.g., GPT-4 Turbo, Gemini 1.5 Pro]}. The OpenAI model produced a slightly higher overall F1-score (41.94\%) compared to the Gemini model (39.70\%). Critically, the Gemini model's processing time was nearly five times longer. Therefore, the OpenAI GPT-4.1 model was selected for all subsequent experiments to ensure timely completion.

\subsection{Summary of Selected Hyperparameters}
The final hyperparameters chosen for the main experimental runs are summarized in \cref{tab:final_hyperparameters}. A temperature of 0.0 was used to maximize output determinism, although some variability is still inherent in LLM outputs. The random seed was fixed at 42 for reproducibility.

\begin{table}[!htbp]
\centering
\caption{Selected Hyperparameters for Main Experiments.}
\label{tab:final_hyperparameters}
\begin{tabular}{@{}ll@{}}
\toprule
\textbf{Hyperparameter} & \textbf{Value} \\ \midrule
LLM Model & OpenAI GPT-4.1 \textit{[Placeholder: Verify model name]} \\
Temperature & 0.0 \\
Chunk Size & 20 sentences \\
Chunk Overlap & 5 sentences \\
LTM Refinement Cycles & \textit{[Placeholder: e.g., 8]} \\
LTM Merge Sample Size & 20 nodes \\
LTM Hierarchy Sample Size & 15 nodes \\
Use Batching & False \\
Random Seed & 42 \\
Maximum Chunks & 500 (to limit cost during testing) \\
Maximum Output Tokens & 16,384 \\ \bottomrule
\end{tabular}
\end{table}


\section{Main Experimental Results}
\label{sec:main_results}

This section presents the results from the main experiments, focusing on the quality of the generated knowledge graphs and their effectiveness in detecting injected errors.

\subsection{Knowledge Graph Fit Assessment}
\label{subsec:kg_fit}

\subsubsection{Extraction Quality}
The quality of the entity and relationship extraction was measured using Precision, Recall, and F1-score against the manually annotated ground truth for the test documents. \cref{tab:extraction_results} summarizes the performance across different model variations. The results show that the `GPT-4.1-mini` model achieved the highest Entity F1-Score (71.11\%), while the baseline `GPT-4.1` model performed best on the overall score, which incorporates both entity and relationship metrics.

\begin{table}[!htbp]
\centering
\caption{Extraction Quality Results Against Manual Ground Truth.}
\label{tab:extraction_results}
\begin{tabular}{@{}lccc@{}}
\toprule
\textbf{Experiment Model} & \textbf{Overall Score (\%)} & \textbf{Entity F1 (\%)} & \textbf{Relationship F1 (\%)} \\ \midrule
Mnemosyne GPT-4.1 Test      & 90.55 & 57.45 & 14.86 \\
Mnemosyne GPT-4.1 mini Test & 77.33 & 71.11 & 11.49 \\
Mnemosyne GPT-4.1 nano Test & 16.39 & 44.94 & 0.00 \\ \bottomrule
\end{tabular}
\end{table}

\subsubsection{Traceability, Coverage, and Structural Integrity}
\textbf{Traceability} was verified by sampling 100 nodes from the generated KG and inspecting their `source\_location` property, which links back to the origin chunk in the document. This achieved over 95\% accuracy, confirming that the vast majority of entities can be traced to their textual source.

\textbf{Coverage} was assessed via a qualitative review by \textit{[Placeholder: e.g., a domain expert or through systematic keyword checks]}, which confirmed that key legal concepts and terminology from the source documents were accurately represented in the KG.

\textbf{Structural Integrity} was positively impacted by the LTM consolidation process. This generative process significantly increased the number of nodes and relationships, creating a rich semantic network. For the Conewago base document, the initial graph contained \textit{[Placeholder: Number]} nodes and \textit{[Placeholder: Number]} relationships. After LTM consolidation, the graph grew to \textit{[Placeholder: Number]} nodes and \textit{[Placeholder: Number]} relationships, indicating substantial ontological enrichment.

\subsection{Suitability for Consistency and Completeness (C\&C) Analysis}
\label{subsec:candc_suitability}
The generated KGs were evaluated for their suitability in supporting C\&C analysis. Predefined Cypher queries, such as the query to detect undefined terms (described in Section 3.4.3), were executed successfully. For the Conewago base document, this query identified \textit{[Placeholder: Number]} terms that were used but not explicitly defined, demonstrating the graph's utility for structural validation. The emergent type ontology, visualized in Appendix \textit{[Placeholder: Appendix Figure reference, e.g., H.1]}, demonstrates a coherent schema appropriate for the legal domain, with clear hierarchical structures emerging from the text.

\subsection{Controlled Error Injection Experiment}
\label{subsec:error_injection}
A controlled experiment was conducted by injecting known errors into the Conewago Township Sewer Authority base document. The primary metric was the ability to identify these errors through structural analysis of the resulting KG.

\subsubsection{Inconsistency Detection}
Two inconsistency scenarios were tested by inserting text from the Easttown Township code into the Conewago document.

\paragraph{Case 1: Dissimilar Topic (Driveways)}
A section on "Driveways" from the Easttown code was inserted. This topic is semantically distant from the "Sewer" focus of the base document. The resulting KG contained distinct clusters of nodes related to `Driveway` and `Sewer`. To quantify this separation, the Jaccard Similarity was calculated between the `Driveway` node and the four primary `Sewer` nodes within a 3-hop neighborhood. The results are shown in \cref{tab:jaccard_driveway}.

\begin{table}[!htbp]
\centering
\caption{Jaccard Similarity Between `Driveway` and `Sewer` Nodes.}
\label{tab:jaccard_driveway}
\begin{tabular}{@{}llc@{}}
\toprule
\textbf{Sewer Node ID} & \textbf{Sewer Node Display Name} & \textbf{Jaccard Similarity with Driveway Node} \\ \midrule
c37-node-19 & Sewer & 0.1644 \\
c36-node-8  & Sewer & 0.5586 \\
c33-node-12 & Sewer & 0.6255 \\
c32-node-19 & Sewer & 0.6304 \\ \bottomrule
\end{tabular}
\end{table}

The low similarity score (0.16) for one of the `Sewer` nodes indicates a clear separation. The other, higher scores suggest some shared vocabulary (e.g., `township`, `permit`, `construction`), but the presence of a distinct, low-similarity cluster demonstrates that these topics are separable through graph-based analysis.

\paragraph{Case 2: Similar Topic (Sewer Prohibited Wastes)}
A section on "Prohibited Wastes" from the Easttown sewer code was inserted. This topic is semantically close to the base document's content. Graph-based queries attempting to isolate this inserted section based on semantic dissimilarity were unsuccessful. The concepts and terminology were too closely related to those already present in the Conewago document, and they were seamlessly integrated into the main KG cluster. However, a manual review of the source document revealed that the inserted text was clearly identifiable by its distinct header formatting (e.g., "§ 150-25. Prohibited Wastes"). As the current implementation of Mnemosyne does not process document headers or footers, this structural cue was missed.

\subsubsection{Incompleteness Detection}
To test for incompleteness, the section on "Delinquency" was removed from the Conewago document. \textit{[Placeholder: Describe the results of this experiment. For example: "The removal of this section could not be detected through internal analysis of the KG alone. There were no forward references to the 'Delinquency' section from other parts of the document, and therefore its absence did not create a structural anomaly like a broken link. This finding highlights a key challenge in detecting certain types of incompleteness."]}

\section{Analysis and Interpretation}
\label{sec:analysis}
The experimental results provide significant insights into the capabilities and limitations of using LLM-generated knowledge graphs for document validation. The findings lead to a refinement of the initial hypothesis, particularly concerning the nature of completeness. The analysis is framed around the three core validation dimensions, incorporating the feedback from Dr. Elbasheer.

\paragraph{Consistency (Internally Verifiable)}
The results strongly support the hypothesis that consistency can be assessed as an internal property of a document. The error injection experiment with the dissimilar "Driveways" section demonstrated that semantically inconsistent content manifests as a structurally distinct or poorly connected cluster within the KG. This structural anomaly is precisely the kind of pattern that density-based clustering algorithms like DBSCAN are designed to detect. The low Jaccard similarity score serves as a quantitative indicator of this separation. This aligns with the theoretical position that consistency checks are well-suited to graph-based representations, where inconsistencies appear as topological irregularities.

Conversely, the experiment with the semantically similar "Sewer" section from Easttown highlighted a limitation. When the inconsistent content shares the same domain vocabulary, graph-based semantic analysis alone may be insufficient. However, this also reveals an opportunity: incorporating document metadata, such as headers and section numbering, could provide an orthogonal signal for detecting such inconsistencies.

\paragraph{Completeness (A Refined, Two-Layer View)}
The initial hypothesis posited that completeness, like consistency, could be verified internally. The experimental findings reveal a more nuanced reality, suggesting that completeness should be divided into two distinct sub-categories.

\begin{enumerate}
    \item \textbf{Internal Completeness:} This refers to omissions that create structural breaks within the document's own context. Examples include unfulfilled forward references (e.g., a mention of "as described in Section 7" when Section 7 does not exist) or partially missing definitions. These issues are, in principle, detectable as structural anomalies in a KG, such as dangling edges or nodes lacking expected properties. The current research did not directly test for this, as the legal documents used had very few internal cross-references, a limitation of the dataset.

    \item \textbf{External Completeness:} This refers to the omission of an entire, unreferenced section, such as the "Delinquency" section in the experiment. Its absence can only be identified by referencing external, domain-specific knowledge of what a document of its type \textit{should} contain. An expert would know that a sewer authority ordinance is likely incomplete without a section on penalties or delinquencies. Within the "four corners of the document," however, its absence is invisible. This form of completeness is therefore unobservable from internal structure alone and behaves like a correctness problem.
\end{enumerate}

This distinction between internal (partially observable) and external (unobservable) completeness is a key finding of this research. It refines the original hypothesis and clarifies which validation tasks are feasible using purely intra-document analysis.

\paragraph{Correctness (Requires External Grounding)}
The findings reinforce the initial hypothesis that correctness is fundamentally different from consistency. Correctness—whether a statement is factually true—cannot be determined by analyzing the internal structure of the document's KG. Validating the correctness of a legal statute, for example, would require comparing it against a higher-level legal code, a court ruling, or an established body of domain knowledge. This process is inherently external and remains outside the scope of what can be verified from the document's internal structure alone.

\section{Hypothesis Validation}
\label{sec:hypothesis_validation}
The experimental results are now used to explicitly validate the research hypotheses, reflecting the refined understanding developed through the analysis.

\begin{itemize}
    \item \textbf{H1: An LLM can be used to convert a large document into a knowledge graph.}
    \textbf{Supported.} The results from the Knowledge Graph Fit Assessment (\cref{subsec:kg_fit}) show that the LLM-based pipeline successfully extracted entities and relations to construct a structured, traceable, and coherent KG from unstructured legal text. The performance metrics in \cref{tab:extraction_results} provide quantitative support for this hypothesis.

    \item \textbf{H2: An LLM can be used to process multiple knowledge graphs into a typed cluster of knowledge graphs.}
    \textbf{Supported.} The LTM consolidation process successfully merged, refined, and enriched the chunk-based KGs into a single, deep semantic network. The analysis in \cref{subsec:kg_fit} regarding the growth in nodes and relationships, alongside the emergence of a coherent type ontology, confirms that the LLM was able to organize the graph into a more valuable, typed structure.

    \item \textbf{H3: A typed cluster of knowledge graphs can be used to check the source document for consistency and completeness.}
    \textbf{Supported and Refined.} The initial hypothesis is supported, but the research has revealed important nuances.
    \begin{itemize}
        \item \textbf{Consistency:} This aspect is strongly supported. The error injection experiment (\cref{subsec:error_injection}) demonstrated that semantic inconsistencies can be identified as structural anomalies within the KG, making them detectable through graph analysis.
        \item \textbf{Completeness:} The hypothesis is supported for a specific sub-category of completeness. The findings compel a distinction between \textit{internal completeness} (e.g., broken references), which is structurally detectable, and \textit{external completeness} (e.g., missing unreferenced sections), which is not detectable without external knowledge. The KG is an effective tool for the former, while the latter falls outside the scope of internal validation. This refinement is a primary contribution of this work.
    \end{itemize}
\end{itemize}

\section{Chapter Summary}
\label{sec:results_summary}
This chapter presented and analyzed the results of the experimental evaluation of the Mnemosyne system. The findings demonstrate that the proposed LLM-based pipeline can successfully convert large legal documents into high-fidelity, attributed knowledge graphs. These graphs were shown to be structured in a way that facilitates the automated detection of document flaws.

The analysis of the results led to a significant refinement of the initial theoretical framework. While consistency was confirmed to be an internally verifiable property detectable via graph-based anomaly detection, completeness was revealed to be a dual-layered concept. The distinction between internally detectable structural incompleteness and externally referenced domain incompleteness clarifies the boundaries of what automated, intra-document validation can achieve. All three research hypotheses were supported by the empirical evidence, with the third hypothesis being substantially refined by the experimental outcomes.

The next chapter will discuss the broader contributions and implications of these findings, address the limitations of this study, and suggest promising directions for future research.