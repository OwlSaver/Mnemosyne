\chapter{Discussion and Conclusions}
% Due by November 1, 2025 Includes entire Praxis
% 3 to 5 pages
\section{Conclusion}

Failures - it takes too long to run, it will take many more cycles to form a rich semantic hierarchy, maybe hundreds or thousands of cycles, the KG did not have entity properties even though the LLM was asked to create them.

Originally my Knowledge Graph processing was all based on using the LLMto improve things. But as I developed this, interactions within a batch become problematic due to dependancies. So, I splt them into one item processes. This resulted in tremendous slow down.

I have started adding some deterministic processes. This should speed things up.

When I started this I had ample evidence that this was needed. I had specific evidence from problems that Easttown township faced due to inconsistencies in laws. I knew from my experience that teams had issues creating large documents that were complete and consistent. Late changes were often not applied accross a document or set of documents.

I was surprised by how little research was going on in this area. there was a lot of research on small documents. There was a lot of research on summarizing oducuments. But little to none on large documents. Large documents are where the power of computers could come into play.

I thought and still belive that my approach of layering the processing and working on small randomply selected sections can allow for the processing of large documents. I underestimated how hard this would be. Even LangExtract from Google has toruble with larger documents.

The approach did show that it could work. It needs custom LLMs defiened for the specific needs. It needs

\section{Contribution to the Body of Knowledge}
I have successfully developed a system that can take a large document, divide it into manageable chunks, and incrementally build a knowledge graph. This process includes creating a semantic layer focused on "IS A" (type) and "PART OF" relationships, which is improved through successive refinements.

My literature review revealed that while there is a recognized need for handling large documents, existing approaches typically:

Build shallow, single-layer knowledge graphs (i.e., generating (head, relation, tail) triples directly).

Perform little to no refinement of the graph after its initial construction.

My work builds upon this by incorporating several key advancements:

Constructing a foundational knowledge graph.

Developing a separate "IS A" type layer on top of the foundational graph.

Implementing multiple refinement steps to improve the "IS A" hierarchy.

Establishing "PART OF" relationships within the graph.

Merging nodes based on both syntactic and semantic similarity.

\section{Recommendations for Future Research}

Run the processes asynchronously
Create specific LLM for each task
Build the Query Processing
Build the response processing agent
Build the ability to validate documents
Prepossessing the copra to identify initial Node Labels and Relationship Types
DO not clear the LTM for each run, processing the same document multiple times just serves as reinforcement
Try a very large number of cycles - 1,000 or more
Make all parts run asynchronously with blocking
For each entity you find in the source, attach it to an entity in LTM, start with Thing. How do we do this efficiently in a bounded number of steps? Maybe start at the root, Thing, and head up the tree picking the most likely for up to n levels.
Stop refinement that flips back and forth.
Create a set of documents and associated ground truth knowledge graph. The documents should be about 3,000 pages. There should be variations with specific introduced errors. They could be in specific domains such as law, medicine, or engineering. This would support further research on large documents.
