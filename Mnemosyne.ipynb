{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/OwlSaver/Mnemosyne/blob/main/Mnemosyne.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cAQefra1ebNa"
      },
      "source": [
        "# Mnemosyne\n",
        "\n",
        "This notebook, titled Mnemosyne, is a comprehensive system designed to transform unstructured text from documents into structured knowledge graphs. The primary goal is to automate the extraction of entities and their relationships, creating a semantic network that can be analyzed for consistency, completeness, and deeper insights. The process begins by breaking down a source document into manageable chunks. Each chunk is then processed by a Large Language Model (LLM), such as Google's Gemini or OpenAI's GPT, which has been prompted to identify and categorize named entities and the relationships between them according to a predefined data model.\n",
        "\n",
        "The extracted information is structured as a knowledge graph and ingested into a Neo4j graph database. This initial graph, referred to as Short-Term Memory (STM), is then consolidated into a Long-Term Memory (LTM). The consolidation phase is a critical step where the system refines the knowledge graph by merging duplicate entities, inferring hierarchical relationships to build a type ontology, and ensuring data integrity. The entire process is managed through a series of configurable experiments, allowing for detailed control over parameters like the LLM used, document chunk size, and the number of refinement cycles. The system is designed to be modular and extensible, with robust error handling, logging, and a suite of unit tests to ensure reliability. The ultimate aim of Mnemosyne is to provide a powerful tool for knowledge management and analysis, enabling users to uncover connections and insights that would be difficult to discern from the original text alone.\n",
        "\n",
        "Mnemosyne: In Greek mythology, Mnemosyne is the goddess of memory. This is a meant to evoke the system's STM/LTM architecture."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIgfXlAkjjje"
      },
      "source": [
        "# Project Management\n",
        "This section serves as the project management hub for the Mnemosyne initiative. It begins with the high-level goals, outlining the core objectives of the system. This is followed by a detailed task board that tracks development progress across various categories, including API integration, bug fixes, documentation, and new feature implementation. The section concludes with a summary of the key risks, ongoing issues, and architectural decisions made throughout the project's lifecycle, providing important context for the system's design and evolution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OQepvAQBSm3x"
      },
      "source": [
        "##Goals for Mnemosyne\n",
        "**Main Goal:** To design, build, and evaluate a novel, automated pipeline that transforms large, unstructured legal documents into a semantically coherent knowledge graph, and to validate this graph's suitability for facilitating the analysis of the document's internal consistency and completeness.\n",
        "\n",
        "This primary goal is supported by the following key objectives:\n",
        "\n",
        "1. **Develop a Scalable System for Knowledge Extraction from Large Documents.**\n",
        "This involves creating a robust pipeline that overcomes the inherent context-window limitations of Large Language Models (LLMs). The system will systematically segment a source document, use a prompted LLM to perform named entity and relation extraction on each segment, and aggregate the resulting graph fragments into a unified, persistent knowledge graph in a graph database (e.g., Neo4j).\n",
        "\n",
        "2. **Implement a Process for Automated Knowledge Graph Refinement and Semantic Consolidation.**\n",
        "The initial, aggregated graph will be raw. This objective is to develop and implement a \"Long-Term Memory\" consolidation process that automatically refines the graph's structure and meaning. This includes algorithms for key tasks such as:\n",
        "\n",
        "  - Entity Resolution: Identifying and merging duplicate nodes that refer to the same entity.\n",
        "\n",
        "  - Ontological Enrichment: Inferring and creating a hierarchical type system (e.g., an IS_A hierarchy) to ensure the graph is semantically consistent and logically organized.\n",
        "\n",
        "  - Data Integrity: Ensuring all nodes and relationships conform to a defined data model.\n",
        "\n",
        "3. **Demonstrate the Analytical Utility of the Knowledge Graph for Document Auditing.**\n",
        "The final and most critical objective is to prove that the resulting knowledge graph is a valuable tool for analysis. This will be achieved by using the graph to uncover information that is difficult to discern from the source text alone. Specifically, this involves demonstrating that structured queries (e.g., in Cypher) can be executed against the graph to effectively identify potential issues, such as:\n",
        "\n",
        "  - Concepts or terms that are used but never formally defined.\n",
        "\n",
        "  - Relationships between distant or disparate parts of the document.\n",
        "\n",
        "  - Structural inconsistencies or logical gaps in the source material.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mX1SeZyzKR11"
      },
      "source": [
        "## Knowledge Graph Pipeline\n",
        "This project transforms unstructured text into a structured knowledge graph in five distinct stages:\n",
        "\n",
        "1. üìÑ Document Decomposition: The process begins by taking a source document and breaking it down into smaller, manageable text segments called \"chunks.\" This is a crucial step to manage the context limitations of Large Language Models (LLMs).\n",
        "\n",
        "2. ‚ú® Short-Term Memory (STM) Creation: Each text chunk is individually processed by an LLM. The model extracts named entities and their relationships, converting each chunk into a small, self-contained knowledge graph fragment formatted in JSON. This collection of temporary graphs constitutes the Short-Term Memory.\n",
        "\n",
        "3. üíæ Long-Term Memory (LTM) Ingestion: All individual JSON graph fragments from the STM are loaded into a Neo4j graph database. In this stage, duplicate entities from different chunks are merged to create a single, unified knowledge graph for the entire document, which is now considered the Long-Term Memory.\n",
        "\n",
        "4. üîß Semantic Refinement & Consolidation: The raw knowledge graph in the LTM undergoes an automated refinement process. This critical stage enhances the graph's quality by inferring a type hierarchy (ontology), merging semantically similar entities, and ensuring the overall structure is consistent and logical.\n",
        "\n",
        "5. üîç Analytical Validation: The final, refined knowledge graph is used to demonstrate its analytical value. By running queries against the graph, the system can uncover insights that are not obvious from the original text, such as identifying concepts that are used but never defined or discovering relationships between distant parts of the document."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMgD9spdu3c3"
      },
      "source": [
        "## Development Tasks\n",
        "The tables below provide a comprehensive overview of 130 Mnemosyne project tasks, grouped by status. This task board is used to track priorities and manage the development workflow.\n",
        "\n",
        "---\n",
        "### To Do üìù\n",
        "This section lists all planned tasks, prioritized for future development.\n",
        "\n",
        "#### Foundational Refactoring\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "\n",
        "None currently identified.\n",
        "\n",
        "\n",
        "#### Core Feature Development\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 5 | To Do | Measurement | 13 | **Develop Anomaly Detection Method** | Research and implement a graph-based method to automatically identify anomalous patterns, such as outlier nodes or disconnected subgraphs, to flag potential data quality issues. |\n",
        "| 7 | To Do | Measurement | 13 | **Define Consistency & Completeness Rules** | Formalize a set of structural and logical validation rules (e.g., an entity cannot have conflicting types) and develop Cypher queries to audit the graph for C&C gaps. |\n",
        "| 100 | To Do | Core Logic | 19 | **Refactor Ontology Hierarchy Construction** | Replace the single-prompt hierarchy creation with an iterative process. The system will loop through each `Type` node and use a focused LLM call to assign its single most appropriate parent from the list of existing types. |\n",
        "| 101 | To Do | Core Logic | 19 | **Implement Focused Ontology Refinement** | Create a new refinement module that analyzes a small sample of `Type` nodes per cycle. For each node, it will use separate, targeted LLM prompts to evaluate and suggest improvements for the node's name, the cohesion of its children, and potential child-to-parent merges. |\n",
        "| 102 | To Do | Experiment | 19 | **Add Separate Ontology Refinement Cycle** | Create a distinct consolidation loop for the focused ontology refinement tasks. Add a new hyperparameter, `num_ontology_refinement_cycles`, to control how many times this resource-intensive loop runs, independent of the main instance-level refinement. |\n",
        "| 120 | To Do | Improvement | 20 | Implement Relationship Aggregation | Refactor the relationship creation logic to prevent duplicates. If a relationship between two nodes already exists, increment a strength property on that relationship instead of creating a new one. This will transform relationship frequency into a measure of confidence. |\n",
        "| 121 | To Do | Improvement | 20 | Calculate Graph Refinement Score | Develop a scoring metric to quantify the degree of LTM consolidation. The score should be a function of the reduction in node count (from merging) and the average refinementCount across all nodes, providing a measurable indicator of graph maturity. |\n",
        "| 128 | To Do | Improvement | 20 | Implement Dead-End Node Analysis | Create a process to identify \"dead-end\" nodes‚Äîentities with very few or no relationships. These often represent incomplete or low-value data points and could be flagged for review or deprioritized in subsequent refinement cycles. |\n",
        "\n",
        "\n",
        "#### Diagram Generation\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 127 | To Do | Improvement | 20 | Generate Type Ontology Graph | Create a utility to generate a pyvis graph of just the :Type nodes and their :IS_A relationships. This will provide a clear, visual representation of the inferred ontology for each experiment. |\n",
        "\n",
        "#### Research and Exploration\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 14 | To Do | Research | 13 | **Research Neo4j Schema Design** | Investigate and document Neo4j schema design best practices, specifically comparing the use of node Labels vs. a `type` property for modeling entity categories. |\n",
        "| 122 | To Do | Research | 20 | Look into LangExtract | Does it complement my work? Can I use it? Are thre papers on it? |\n",
        "| 129 | To Do | Research | 20 | Investigate LangChain for Integration | Research the LangChain library to determine if its functionalities, particularly its document loaders, text splitters, or agentic workflows, could be integrated to streamline or enhance the Mnemosyne pipeline. The goal is to identify potential for code simplification or feature enhancement. |\n",
        "| 130 | To Do | Research | 20 | Explore Graph-Based Text Generation | Investigate techniques for using the refined knowledge graph to generate coherent, human-readable summaries or answer questions about the source document. This would validate the graph's analytical value by transforming its structured data back into natural language. |\n",
        "\n",
        "#### Bug Fixes\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "\n",
        "None currently identified.\n",
        "\n",
        "#### Testing and Validation\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 38 | To Do | Testing | 13 | **Add Relationship Persistence Test** | Develop an automated test to confirm that arbitrary relationships generated by the LLM are correctly parsed and persisted in the knowledge graph. |\n",
        "| 39 | To Do | Testing | 13 | **Add IS_A Hierarchy Validation Test** | Create specific tests to validate the `IS_A` hierarchy construction and consolidation logic, ensuring the type ontology is built as expected. |\n",
        "| 40 | To Do | Testing | 13 | **Add End-to-End Consolidation Test** | Implement an integration test to verify the correctness of the entire LTM consolidation process, including merging, typing, and relationship cleanup. |\n",
        "\n",
        "#### Documentation\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 35 | To Do | Documentation | 13 | **Add Docstrings and Inline Comments** | Improve code maintainability by adding Python docstrings to all functions/classes and inline comments to clarify complex logic. |\n",
        "| 37 | To Do | Documentation | 13 | **Create System Architecture Diagram** | Develop a UML class diagram to provide a high-level visual overview of the system's architecture, key classes, and their relationships. |\n",
        "\n",
        "---\n",
        "### Backlog\n",
        "This section lists tasks that are under consideration but are not currently scheduled for development.\n",
        "\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 2 | To Do | Performance | 13 | **Profile and Optimize Pipeline** | Conduct performance profiling to identify and optimize computational bottlenecks in the data processing pipeline. |\n",
        "| 45 | To Do | Experiment | 13 | **Run Baseline Performance Experiment** | Configure and run a baseline experiment using a full, unmodified source document to serve as the 'golden standard' for comparison. |\n",
        "| 52 | To Do | Experiment | 13 | **Analyze Document Chunk Size Impact** | Run experiments varying the `doc_chunk_size` parameter to analyze its impact on graph quality, node/relationship counts, and cost. |\n",
        "| 8 | To Do | Experiment | 13 | **Analyze Error Robustness** | Design and execute an experiment where known errors are injected into source documents to measure the system's robustness. |\n",
        "| 46 | To Do | Experiment | 13 | **Test Inconsistent Information Handling** | Configure an experiment that appends a conceptually inconsistent section from an unrelated document to test contradiction handling. |\n",
        "| 47 | To Do | Experiment | 13 | **Test Thematically Related Document Merging** | Configure an experiment that processes a thematically related but distinct document to test the system's ability to merge shared concepts. |\n",
        "\n",
        "---\n",
        "### Might Do ü§î\n",
        "This section lists tasks that are speculative and may or may not be implemented in the future.\n",
        "\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 3 | Might Do | API Integration | 13 | **Integrate Llama-Family LLM APIs** | Implement a client within LLMService to support Llama-based models for comparative analysis. |\n",
        "| 4 | Might Do | API Integration | 13 | **Integrate Deepseek LLM API** | Implement a client within LLMService for Deepseek models to broaden the suite of available LLMs. |\n",
        "| 15 | Might Do | Feature | 13 | **Store Prompts in Short-Term Memory** | Modify MemoryManager to store source chunks and prompts alongside KG fragments for advanced debugging. |\n",
        "| 16 | Might Do | Feature | 13 | **Implement Multi-LLM Ensemble Method** | Develop a module to query multiple LLMs and synthesize their responses to improve accuracy. |\n",
        "| 98 | Might Do | Refactoring | 18 | **Implement Dynamic Consolidation Loop** | Modify the LTM consolidation process to run until the graph stabilizes (i.e., no more changes are made in a full cycle) instead of using a fixed number of iterations. This would be good for a production version. For testing, I need to control the cycles. |\n",
        "| 41 | Might Do | Bug Fix | 13 | **Fix Incorrect Casting of Numerical Strings** | Correct the bug where identifiers with numbers (e.g., 'Section 123') are incorrectly cast as integers, causing data loss. Ensure they are always preserved as strings. This only impacts data in the CSV files. I will only fix it if that becomes and issue. |\n",
        "\n",
        "---\n",
        "### Canceled ‚ùå\n",
        "This section lists tasks that have been canceled, with brief explanations for historical context.\n",
        "\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 73 | Canceled | Bug Fix | 14 | **Investigate IS_A Relationship Direction** | Task to verify the direction of IS_A relationships. Canceled as the issue was not reproducible. |\n",
        "| 60 | Canceled | Configuration | 13 | **Reconfigure for Local Neo4j Instance** | Task to connect to a local Neo4j database. Canceled due to dependency on the infeasible multi-database task (ID 61). |\n",
        "| 61 | Canceled | Feature | 13 | **Isolate Experiments in Separate Databases** | Task to use separate Neo4j databases. Canceled as the Community Edition does not support multiple active databases. |\n",
        "| 63 | Canceled | Feature | 14 | **Implement Experiment Data Deletion Utility** | Create a function to delete data for a specific experiment. Canceled due to implementation complexity exceeding its utility. |\n",
        "| 53 | Canceled | Refactoring | 13 | **Replace Semantic STM IDs with GUIDs** | Refactor ingestion to use GUIDs. Canceled as the current deterministic ID system provides sufficient performance. |\n",
        "| 59 | Canceled | Software | 13 | **Install Neo4j Locally for Multi-DB Support** | Install a local Neo4j instance. Canceled as the Community Edition does not support this feature. |\n",
        "\n",
        "---\n",
        "### Done ‚úÖ\n",
        "This section lists all completed tasks, providing a record of the project's progress.\n",
        "\n",
        "| ID | Status | Category | Version | Task | Details/Description |\n",
        "|:---|:---|:---|:---:|:---|:---|\n",
        "| 19 | Done | API Integration | 13 | Leverage Native LLM JSON Output | Modified API clients to use the native JSON output modes (response_format for OpenAI, response_mime_type for Gemini), improving parsing reliability and reducing errors. |\n",
        "| 22 | Done | API Integration | 13 | Implement Multi-Provider LLM Connections | Implemented clients for both Google Gemini and OpenAI GPT models within the LLMService, allowing for flexible provider selection in experiments. |\n",
        "| 54 | Done | Bug Fix | 13 | Unify IS_A Relationship Type | Resolved an inconsistency where both IS_A and ISA relationship types were being created. The normalization function was updated to enforce IS_A as the standard. |\n",
        "| 67 | Done | Bug Fix | 14 | Fix Random Node Selection Query | Corrected a Cypher query bug that caused the random entity selection logic to return only a single node, regardless of the requested sample size. The LIMIT clause is now correctly applied. |\n",
        "| 69 | Done | Bug Fix | 16 | Fix Incomplete Node Export for Experiments | Corrected the export logic to ensure the KnowledgeGraphNodes.csv file includes all nodes associated with an experiment run, not just a single node. |\n",
        "| 74 | Done | Bug Fix | 14 | Consolidate and Refine Master Prompt | Updated and consolidated the master instruction prompt to reflect all recent logic changes and architectural improvements, ensuring clarity and consistency. |\n",
        "| 79 | Done | Bug Fix | 14 | Ensure Source Nodes Have a Type | Addressed a bug where Source nodes were created without a Type node. The ingestion process now ensures they are correctly typed for graph consistency. |\n",
        "| 95 | Done | Refactoring | 18 | **Create a Mnemosyne folder** | Create the folder at the same level as the existing Praxis folder. Create an Input and Output folder under it. Copy the input files from the Praxis folder to the new Mnemosyne folder. |\n",
        "| 24 | Done | Core Logic | 13 | Implement Hierarchical Relationship Inference | Deployed logic that analyzes entities to infer and create hierarchical IS_A relationships, transforming the flat graph into a more structured ontology. |\n",
        "| 26 | Done | Core Logic | 13 | Implement Entity Source Tracking | Enhanced data traceability by creating Source nodes and linking them to each extracted entity via a FROM relationship, showing its document origin. |\n",
        "| 27 | Done | Core Logic | 13 | Implement Entity Grouping by Type | Deployed a consolidation algorithm that groups and merges nodes based on their assigned entity type, reducing redundancy in the Long-Term Memory (LTM). |\n",
        "| 28 | Done | Core Logic | 13 | Implement Entity Merging by Similarity | Deployed a core consolidation algorithm that identifies and merges semantically similar entities, creating a more coherent knowledge graph by reducing duplication. |\n",
        "| 34 | Done | Data Integrity | 13 | Implement Deterministic Node IDs | Established a deterministic method for generating consistent and reproducible pseudo-GUIDs for all nodes across experimental runs. |\n",
        "| 42 | Done | Data Integrity | 16 | Enforce name Property for All Nodes | Implemented a validation step during ingestion to ensure every node in the graph has a non-null name property, improving data consistency. |\n",
        "| 68 | Done | Documentation | 14 | Add Linked Table of Contents to Notebook | Improved notebook navigation by structuring it with Level 1 headers for major sections and adding a linked Table of Contents at the top. |\n",
        "| 20 | Done | Error Handling | 13 | Implement Fail-Fast on Critical Errors | Improved experiment robustness by implementing a mechanism to automatically halt an experiment and flag it as 'failed' if a critical, unrecoverable error occurs. |\n",
        "| 30 | Done | Error Handling | 13 | Implement Global Exception Handling | Increased application stability by wrapping all major I/O and processing operations in try...except blocks for graceful error handling and logging. |\n",
        "| 21 | Done | Experiment | 13 | Parameterize Experiment Selection and Refinement | Refactored experiment configuration to centralize key variables (e.g., sample sizes, iteration counts) into experiment definitions for easier control. |\n",
        "| 23 | Done | Experiment | 13 | Implement Batch Experiment Execution | Implemented a flexible data structure that allows for the definition and automated execution of batch experiments with varying hyperparameter combinations. |\n",
        "| 44 | Done | Feature | 13 | Implement Granular Pipeline Stage Controls | Implemented modular pipeline controls, allowing experiments to run specific stages (e.g., 'STM only', 'full consolidation') for targeted analysis. |\n",
        "| 57 | Done | Feature | 13 | Add enabled Flag to Experiments | Added an enabled flag to the experiment definition structure, allowing individual experiments to be toggled on or off without deleting their configuration. |\n",
        "| 58 | Done | Feature | 13 | Add Token and Threshold Hyperparameters | Expanded experiment configurability by adding MAX_OUTPUT_TOKENS and STM_FULLNESS_THRESHOLD as new hyperparameters. |\n",
        "| 62 | Done | Feature | 14 | Isolate Experiment Data with Neo4j Labels | Enhanced the data model to tag all nodes and relationships with dynamic labels for the experiment_id and run_timestamp, allowing multiple runs to coexist in one database. |\n",
        "| 64 | Done | Feature | 14 | Log LLM Response Size | Modified the LLMService to log the character count of the raw JSON response from the language model, providing insight into the size of generated KG fragments. |\n",
        "| 65 | Done | Feature | 14 | Log LLM API Call Duration | Instrumented the LLMService to measure and log the precise execution time for each API call, formatted as HH:MM:SS.ms, to analyze performance. |\n",
        "| 66 | Done | Feature | 16 | Scope Graph Consolidation per Experiment | Refactored all LTM consolidation Cypher queries to operate exclusively on nodes tagged with the current experiment's ID and timestamp, preventing cross-contamination between runs. |\n",
        "| 72 | Done | Feature | 14 | Add In-Line Neo4j Graph Visualization | Implemented a feature to render the Neo4j knowledge graph directly within a notebook cell, providing immediate visual feedback for analysis and debugging. |\n",
        "| 78 | Done | Feature | 15 | Generate Relationship Export File | Added a feature to generate a KnowledgeGraphRelationships.csv file, containing one row per relationship for the current experiment run. |\n",
        "| 17 | Done | Logging | 13 | Log Node/Relationship Counts Pre- and Post-Consolidation | Added logging to report node and relationship counts before and after the LTM consolidation stage to quantitatively measure its impact. |\n",
        "| 91 | Done | Logging | 18 | Add Detailed Experiment Statistics | Expanded experiment logging to include detailed statistics such as total entities/relationships discovered, counts from the ground truth (ER) document, and matching scores. |\n",
        "| 29 | Done | Performance | 13 | Offload Merging Logic to Neo4j via Cypher | Achieved a significant performance improvement by migrating all graph merging and consolidation logic from Python to optimized Cypher queries executed by the database. |\n",
        "| 1 | Done | Refactoring | 16 | Add All Hyperparameters to Results File | Modified the output script to automatically include all experiment hyperparameters as columns in the final results CSV file, enhancing reproducibility. Will require manual updates for new parameters. |\n",
        "| 25 | Done | Refactoring | 13 | Migrate LTM to Native Neo4j Operations | Completed a major architectural refactor to eliminate the in-memory graph representation. All LTM operations now occur directly within Neo4j for improved scalability. |\n",
        "| 31 | Done | Refactoring | 13 | Standardize on Python logging Module | Modernized application logging by replacing all print() statements with Python's standard logging module for structured and configurable output. |\n",
        "| 33 | Done | Refactoring | 13 | Centralize Hyperparameter Definitions | Centralized all hyperparameters into a single experiment_definitions configuration structure to simplify management and ensure run-to-run consistency. |\n",
        "| 49 | Done | Refactoring | 13 | Add clear_database Hyperparameter | Introduced a clear_database boolean hyperparameter to allow experimental runs to either start with a fresh database or build upon existing data. |\n",
        "| 70 | Done | Refactoring | 14 | Encapsulate Logic in Experiment Class | Refactored the main processing logic into a dedicated Experiment class to encapsulate the setup, execution, and result collection for a single run. |\n",
        "| 71 | Done | Refactoring | 14 | Expose Core Mind Class Methods | Exposed the doc_to_stm and stm_to_ltm methods in the Mind class, allowing the Experiment class to orchestrate the workflow more cleanly. |\n",
        "| 75 | Done | Refactoring | 14 | Simplify Node ID Schema | Simplified the node ID schema to a clean, sequential format (e.g., C1-Node23), removing a separate GUID conversion step and streamlining ingestion. |\n",
        "| 76 | Done | Refactoring | 16 | Consolidate Duplicate Entities Across Chunks | Enhanced LTM ingestion to merge nodes that share the same name and type, creating a unified entity while preserving all FROM relationships to track its origins. |\n",
        "| 77 | Done | Refactoring | 16 | Ensure Complete Node Export | Corrected the export logic for the KnowledgeGraphNodes.csv file to ensure it captures all nodes from the specified experiment run, providing a complete record. |\n",
        "| 80 | Done | Refactoring | 15 | Branch Codebase to Version 15 | Branched the codebase to Version 15 to isolate significant architectural changes, ensuring a stable rollback point before implementation. |\n",
        "| 81 | Done | Refactoring | 15 | Move Source Node Creation to Python | Refactored the pipeline to create Source nodes and relationships in Python code rather than relying on the LLM, improving control and simplifying the prompt. |\n",
        "| 82 | Done | Refactoring | 15 | Support Utility-Only Experiments | Modified the experiment runner to support \"utility\" experiments (e.g., a run that only clears the database) for flexible database management. |\n",
        "| 83 | Done | Refactoring | 16 | Align LTM Logic with Mnemosyne Updates | Updated the LTM consolidation process, including entity merging and hierarchy construction, to align with the refined logic and data models in the latest project version. |\n",
        "| 84 | Done | Refactoring | 16 | Improve Readability of Console Output | Improved the formatting and clarity of console log output to make it easier to monitor experiment progress and debug issues. |\n",
        "| 85 | Done | Refactoring | 17 | Clarify Node Merging Logs | Updated logging during node merges to display human-readable entity names instead of internal node IDs, improving debuggability. |\n",
        "| 86 | Done | Refactoring | 17 | Apply Standard Number Formatting | Implemented thousands-separator formatting for numerical outputs (e.g., character counts) in logs to improve readability. |\n",
        "| 87 | Done | Refactoring | 17 | Log KG Comparison Scores to Results | Added columns for EntityMatchScore, RelationshipMatchScore, and OverallScore to the main experiment results table. |\n",
        "| 89 | Done | Refactoring | 17 | Prioritize Global Nodes in Merges | Refactor the merge logic to ensure that when merging nodes with 'Thing' or 'Source', the system preserves the global/canonical version of that node. |\n",
        "| 90 | Done | Refactoring | 17 | Expand 'PART_OF' Relationship Inference | Refactor the 'PART_OF' relationship discovery process. Instead of sampling random entities, the system should systematically analyze the graph to infer and create these relationships. |\n",
        "| 92 | Done | Refactoring | 18 | Add Merge Count to Nodes | Implemented a mergeCount property on nodes. It initializes to 0 and is incremented (count1 + count2 + 1) during merges to track entity consolidation strength. |\n",
        "| 93 | Done | Refactoring | 18 | Add Duplicate Name Count to Results | Added a new metric to the experiment results that counts the total number of entities sharing the same name within the generated graph, providing a measure of ambiguity. |\n",
        "| 94 | Done | Refactoring | 18 | Standardize Entity Name Casing | Refactored entity handling to use two properties: name (lowercase for backend matching) and displayName (Title Case for UI). This ensures consistent merging while preserving readability. |\n",
        "| 18 | Done | Resilience | 13 | Implement Retry Logic for Token Limit Errors | Implemented an automatic retry mechanism for API calls that fail due to token limits, which re-attempts the call with an increased token allocation. |\n",
        "| 32 | Done | Testing | 13 | Create Unit Tests for Document Class | Developed a unittest suite for the Document class to validate its core text processing and chunking logic. |\n",
        "| 55 | Done | Testing | 13 | Create Standardized Test Documents and Golden KGs | Developed a set of standardized test documents and their corresponding 'golden' (ideal) knowledge graphs to serve as a baseline for automated validation and regression testing. |\n",
        "| 56 | Done | Testing | 13 | Automate KG Validation Against Golden Standards | Create a testing framework that automatically runs experiments using the test documents, compares the resulting KGs against the 'golden' KGs, and reports accuracy scores. |\n",
        "| 95 | Done | Refactoring | 18 | Standardize Project File Structure | Implement a single global variable for the root project path and define all other paths relative to it. Ensure all I/O operations respect the new `Input/` and `Output/` directory structure. |\n",
        "| 96 | Done | Refactoring | 18 | Refactor GraphDBManager | Decompose the monolithic `GraphDBManager` into smaller, role-focused classes (`GraphDBWriter`, `GraphDBRefiner`, `GraphDBReader`) to improve maintainability and adhere to the Single Responsibility Principle. |\n",
        "| 97 | Done | Refactoring | 18 | Centralize Utility Functions | Move static helper methods (e.g., `_to_pascal_case`) from core classes into a dedicated `Utils` class to improve code organization and reusability. |\n",
        "| 103 | Done | Refactoring | 18 | Refactor Mind | Decompose Mind into smaller classes to improve maintainability and adhere to the Single Responsibility Principle. |\n",
        "| 104 | Done | Refactoring | 18 | Refactor LLM Merge | The LLM merge was generating multiple errors. It was numbering results event though not asked to. It was merging the same entity to itself. Sometimes it would<br> generate an infinite result which only ended when it exceeded the max output tokens. By restructuring the Prompt, these issues were redced or<br> removed. The code was also updated to gracefully handle errors if they occured. |\n",
        "| 105 | Done | Core Logic | 19 | Add Refinement Tracking Properties to Nodes | Modify the data model and ingestion process to add two new properties to all nodes: refinementCount (initialized to 0) and lastRefined (initialized to the creation timestamp). |\n",
        "| 106 | Done | Refactoring | 19 | Update Refinement Methods to Track Changes | Instrument all graph refinement methods (e.g., merging, re-typing) in the GraphDBRefiner class to increment the refinementCount and update the lastRefined timestamp on any node that is modified. |\n",
        "| 107 | Done | Core Logic | 19 | Improve Randon Selection | Replace the current random node selection query in GraphDBReader with a new Cypher query that calculates a priority score. The score should be weighted to favor nodes with a lower refinementCount and an older lastRefined timestamp. The selection probability should follow an exponential decay pattern, ensuring that newer, less-refined nodes are highly likely to be selected, while older, more-refined nodes become progressively less likely but never have a zero chance of being selected. |\n",
        "| 111 | Done | Bug Fix | 19 | JSON Schema Prompts | Create JSON Schema for all prompts. It can be inline with the prompt. This will make it more likely that the LLM produces the right output. |\n",
        "| 112 | Done | Refactoring | 19 | Move prompt text | Move the prompt text from     def _organize_ontology_hierarchy(self): to the Mind Configuration class, add a JSON Schema to it, and substitute the values. |\n",
        "| 110 | Done | Bug Fix | 19 | Ontology not working | This error appears randomly. Invetigate it and fix it.<br>INFO -     ‚ñ∂Ô∏è START: Organizing ontology hierarchy...<br>/usr/local/lib/python3.11/dist-packages/neo4j/_sync/work/result.py:625: User Warning: Expected a result with a single record, but found multiple.<br>warn(<br>INFO -       Created 14 hierarchical relationships.<br>INFO -     ‚úÖ END: Organizing ontology hierarchy.<Br>SOLUTION: Change the code in _organize_ontology_hierarchy to use IDs rather than names for the relationships. |\n",
        "| 113 | Done | Bug Fix | 19 | Only one batch | The following error occurs because the pair was processed in the first batch and is not aviable for the second batch.<br>INFO -     ‚ñ∂Ô∏è START: Merging similar instances...<br>INFO -       Processing batch 1/2 for merging...<br>INFO -       Merged 'Nexus Platform(c1-node-9)' into 'Nexus Platform(c3-node-1)'.<br>INFO -       Merged 'Dr. Aris Thorne(c4-node-17)' into 'Dr. Aris Thorne(c3-node-17)'.<br>INFO -       Merged 'Ben Carter(c4-node-23)' into 'Ben Carter(c2-node-10)'.<br>INFO -       Merged 'Innovatech(c4-node-5)' into 'Innovatech Solutions Inc.(c3-node-20)'.<br>INFO -       Merged 'Global Logistics Corp.(c3-node-25)' into 'Global Logistics Corp.(c3-node-25)'.<br>INFO -       Merged 5 pairs of instances in this batch.<br>INFO -       Processing batch 2/2 for merging...<br>INFO -       Merged 'Dr. Lena Petrova(c4-node-21)' into 'Dr. Lena Petrova(c1-node-13)'.<br>INFO -       Merged 'Cybernetics Corp(c1-node-14)' into 'Cybernetics Corp(c1-node-14)'.<br>INFO -       Skipping merge for pair ['c4-node-5', 'c3-node-20']: one or both nodes not found in the database.<br>INFO -       Merged 'Nexus Analytics(c4-node-9)' into 'Nexus Analytics(c3-node-8)'.<br>INFO -       Merged 'Dr. Aris Thorne(c4-node-17)' into 'Dr. Aris Thorne(c4-node-17)'.<br>INFO -       Merged 4 pairs of instances in this batch.<br>INFO -     ‚úÖ END: Merging similar instances. |\n",
        "| 114 | Done | Refactoring | 19 | Restructre the output directory | Now that there are categories, restructure it so that it is Run / Category / Experiment. Add a file to capture the Category data. Add a summary file. |\n",
        "| 115 | Done | Refactoring | 19 | Add category and experiment ids | Add ids to the definitions of groups and experiments. Use these ids for file names. For groups use GRP001, GRP002, etc. For experiments in group 1 use GRP001EXP001, GRP001EXP002, etc. |\n",
        "| 116 | Done | Refactoring | 19 | Have create_part_of_relationship use IDs | Peform comparisons using names but update the database using IDs. |\n",
        "| 117 | Done | Refactoring | 19 | Move random selection to Python | Neo4j does not support seeding its random function. It may be less efficent. But the Python randome fnction does support seeding. This will allow for repratable experiments. |\n",
        "| 108 | Done | Bug Fix | 19 | Fix error in Instance Types | This error appears randomly. Investigate it and fix it.<br> INFO -     ‚ñ∂Ô∏è START: Correcting instance types...<br>INFO -       Error during instance type correction: Replacement index 0 out of range for positional args tuple<br>INFO -       -> Reclassified 0 instances.<br>INFO -       Finished: Correcting instance types.|\n",
        "| 118 | Done | Refactoring | 19 | Merge Sequence |  When one merge operation removes a node that a subsequent operation in the same batch needs to reference, an error occurs. The solution is to track these merges as they happen within the merge_entities method. By keeping a local \"redirect\" map, the code can find the new target for any node that has already been merged away, ensuring all operations in the batch can be completed successfully. |\n",
        "| 109 | Done | Bug Fix | 20 | Part Of not working | The part of processing never does anything. Look into why. It may be best to break it down. Ask if the enetity is part of something in one step. Ask if the<br> entity has parts in another step. Do this to a small number of entities. |\n",
        "| 119 | Done | Refactoring | 20 | Magic Values | Look through the code for magic values. Put them as class constants. Use the class constants. |\n",
        "| 123 | Done | Refactoring | 20 | File Definitions | Abstract away the definition of file locations and names for the experiments. |\n",
        "| 99 | Done | Refactoring | 18 | Centralize \\\"Magic Strings\\\" | Move hardcoded string literals like `\"DoNotChange\"` to a central configuration in the `MindConfig` class to improve maintainability. Was done in task 119. |\n",
        "| 43 | Done | Scalability | 13 | Develop Vocabulary Management Strategy | Research and prototype a more scalable solution for managing the vocabulary file to prevent it from becoming a performance bottleneck as the graph grows. Addressed through task 114. |\n",
        "| 124 | Done | Improvement | 20 | Generate Input Document Word Cloud | Implement a feature that, for each source document processed in an experiment, generates and saves a word cloud visualization to a designated diagrams directory. This will provide a quick visual summary of the source text's key terms. |\n",
        "| 125 | Done | Improvement | 20 | Generate Extracted Entity Word Cloud | Create a post-processing step that generates a word cloud from the displayName of all extracted Instance nodes for an experiment. This will visualize the most prominent entities identified by the system. |\n",
        "| 126 | Done | Improvement | 20 | Generate Relationship Type Word Cloud | Implement a feature to generate a word cloud from the types of all relationships created in an experiment. This will offer a high-level view of the most common semantic connections discovered in the text. |\n",
        "| 131 | Done | Improvement | 21 | Validate and Consolidate Raw LLM Output | Implemented a pre-processing step that validates and consolidates raw JSON output from the LLM before ingestion into Short-Term Memory (STM). The logic first enforces data integrity by deleting invalid :IS_A relationships that point from a :Type to an :Instance node. It then deduplicates duplicate :Type nodes within the same output, merging all identically named types into a single canonical node and redirecting all relevant relationships. This ensures a cleaner, more consistent graph fragment enters the pipeline. |\n",
        "| 132 | Done | Improvement | 21 | Implement Creation Provenance Tracking | Added a creationStage property to all nodes and relationships to track their point of origin within the pipeline. This enhances traceability and simplifies debugging. The stages are defined as constants in the GraphSchema class and are assigned as follows: <br>‚Ä¢ SOURCE_EXTRACTION: Assigned to all initial nodes and relationships extracted directly from the source text by the LLM. <br>‚Ä¢ INGESTION: Assigned to framework-generated elements during the STM-to-LTM process, such as the global :Source and :Thing nodes. <br>‚Ä¢ CONSOLIDATION: Assigned to any new nodes or relationships created during the LTM refinement and consolidation phase (e.g., new IS_A or PART_OF relationships). |\n",
        "| 133 | Done | Improvement | 21 | Move code to GitHub | Store the code in GitHub. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvB9Cc8gJiMO"
      },
      "source": [
        "## Risks, Issues, and Decisions\n",
        "This section provides a consolidated log of the key risks, ongoing issues, and architectural decisions that have shaped the project. It serves as a transparent record of the challenges faced and the foundational rules established to guide development.\n",
        "\n",
        "### Overview\n",
        "The primary challenges currently revolve around performance, cost, and the inherent unpredictability of Large Language Models (LLMs). Document processing is time-consuming, and the cost of API calls requires careful management of experimental runs. We are actively addressing issues related to LLM behavior, such as tracing errors back to prompt changes, inconsistent initial entity typing, and model version compatibility. Furthermore, ensuring logical consistency during graph consolidation, where entities might be re-typed, is an ongoing focus.\n",
        "\n",
        "To counterbalance these challenges, a set of firm architectural decisions provides stability. These decisions establish a strict and consistent graph data model, including clear naming conventions for all graph elements (nodes, properties, relationships). We have formalized the rules for creating the type hierarchy, labeling instance nodes, and, most importantly, modeling entity provenance. By linking every entity back to its specific origin in the source document, we ensure full traceability and a scalable, queryable data structure. While initial environmental risks have been resolved, these core issues and foundational decisions continue to guide the project's trajectory.\n",
        "\n",
        "| ID | Status | Type | Title | Description / Resolution / Mitigation |\n",
        "|:---|:---|:---|:---|:---|\n",
        "| 1 | Resolved | Risk | Unstable Colab Environment | Resolution: Proactively restart the Colab session before each run to ensure a clean and stable environment. |\n",
        "| 2 | Resolved | Issue | Intermittent Neo4j Shutdowns | Mitigation: Acknowledge that the service can shut down unexpectedly and perform daily checks to ensure it is running before starting work. |\n",
        "| 3 | Ongoing | Issue | Suboptimal Performance | Description: Processing even small documents is time-consuming (approx. 20 minutes), which slows down iterative experiments and increases the risk of intermittent failures. |\n",
        "| 4 | Ongoing | Issue | LLM Prompt Brittleness | Description: Minor changes to LLM prompts can introduce unexpected bugs that are difficult to trace, creating a dependency risk. |\n",
        "| 5 | Ongoing | Issue | LLM API Costs | Description: API costs (e.g., $0.50 per run with OpenAI) can become prohibitive during large-scale experiments. Mitigation: Limit initial experiments to Gemini and OpenAI to manage costs. |\n",
        "| 6 | Resolved | Issue | Lack of Dynamic LLM Access for IDs | Description: The LLM cannot generate truly unique identifiers (GUIDs) in real-time. Resolution: Implemented a deterministic ID schema based on the document, chunk, and a sequential number. |\n",
        "| 7 | Decided | Decision | Node Naming Convention | All nodes must have a name property (lowercase, for merging) and a displayName property (Title Case, for visualization). |\n",
        "| 8 | Decided | Decision | Type Node Definition | Nodes defining a type must have the :Type label. The ontology is formed by connecting :Type nodes with [:IS_A] relationships. |\n",
        "| 9 | Decided | Decision | Instance Node Labeling | Every instance node must have a dynamic label that matches the name property of its corresponding :Type node (e.g., a node for a car is labeled :Car). |\n",
        "| 10 | Decided | Decision | Instance-to-Type Connection | Every instance node must have a single outgoing [:IS_A] relationship pointing to its :Type node. |\n",
        "| 11 | Decided | Decision | Hierarchy Root Node | A single root node, (:Type {name: \"Thing\"}), must exist as the ultimate parent in the type hierarchy. It is the only :Type node with no outgoing [:IS_A] relationships. |\n",
        "| 12 | Decided | Decision | Two-Level STM Knowledge Graph | The initial knowledge graph in Short-Term Memory (STM) is limited to a two-level structure (instance and its direct type). A full type hierarchy is built later during LTM consolidation. |\n",
        "| 13 | Ongoing | Issue | Gemini 1.5 Pro Compatibility | Description: Using Gemini 1.5 Pro causes an error due to an excessively large response. Mitigation: Reverted to Gemini 1.5 Pro. Next Step: Analyze the JSON output from 2.5 Pro to<br> identify and resolve the root cause. |\n",
        "| 14 | Decided | Decision | Neo4j Naming Conventions | Enforce standard naming conventions for graph elements: PascalCase for node labels, UPPER_SNAKE_CASE for relationship types, and camelCase for property keys. |\n",
        "| 15 | Decided | Decision | Modeling Entity Provenance | Entity origins will be tracked using a (:Source) node for the document. Each entity will connect to it via a [:FROM {chunk: n}] relationship. This graph-native approach was chosen<br> for its superior query performance and scalability over storing provenance in a node property. |\n",
        "| 16 | Ongoing | Issue | Inconsistent Initial Type Identification | Description: The LLM does not consistently assign a specific type to all extracted entities, often defaulting to \"Thing.\" Mitigation: This will be addressed during the LTM consolidation<br> phase, though improving initial extraction via prompt engineering remains a goal. |\n",
        "| 17 | Ongoing | Issue | Type Classification Instability | Description: During LTM consolidation, a node's inferred type can oscillate between a specific type (e.g., Person) and a generic one (e.g., Thing). Next Step: Investigate logic to prevent<br> a node's type from being generically reassigned once a specific type has been established. |\n",
        "| 18 | Decided | Decision | Project Name | The project has been officially named Mnemosyne to provide a clear and memorable identifier, separating the project's identity from the academic paper (Praxis). |\n",
        "| 19 | Decided | Decision | Version Numbering | To maintain a consistent historical record, the versioning scheme will continue sequentially from the previous project name. The first version of Mnemosyne is v18. |\n",
        "| 20 | Decided | Decision | Case Handling for Entity Names | Entity names will be stored in two properties: name (all-lowercase, for reliable merging and backend logic) and displayName (Title Case, for human-readable visualization). |\n",
        "| 21 | Decided | Decision | Project Directory Structure | A standardized directory structure will be used to organize all project assets. The root folder will be /Mnemosyne/, containing the notebook, an Input/ folder for source documents,<br> and an Output/ folder. The Output/ folder will contain a timestamped sub-directory for each execution run (e.g., RUN_YYYY_MM_DD_HH_MM_SS/), ensuring results<br> are isolated. The results of all experiments for a run will be in combined files. The ExperimentConfig class and experiment definitions have been updated to reflect this structure. |\n",
        "| 22 | Decided | Decision | Iterative Parent Assignment for Type Nodes | The current method of asking the LLM to organize the entire type hierarchy at once is inefficient and fails intermittently. The new approach will be to iterate through<br> each non-root Type node individually. For each Type, the system will query the LLM with a focused prompt asking it to select the single most appropriate<br> parent from the list of all other existing Type nodes. This breaks the large, complex reasoning task into smaller, more reliable steps, preventing timeouts and improving the logical<br> consistency of the resulting ontology. |\n",
        "| 23 | Decided | Decision | Focused Ontology Refinement Cycle | To improve the quality of the ontology, a new, separate refinement process will analyze a small, random sample of Type nodes in each cycle (e.g., 1-3 nodes). For each selected<br> Type node, the system will execute a series of distinct, focused LLM prompts to: <br>1. Evaluate Name Clarity: Ask if the node's name could be improved (e.g., made more specific or standard) and suggest a new name if applicable. <br>2. Assess Cohesion: Given the node and its direct children, ask if all children truly belong. If not, the LLM will identify which children should be moved to a different parent. <br>3. Identify Merge Candidates: Given the node and its children, ask if any child concept is synonymous and should be merged into the parent node. |\n",
        "| 24 | Decided | Decision | Separate Refinement Cycles | The new, focused ontology refinement tasks (Decision #23) are computationally intensive. To manage performance and cost, they will be run in their own distinct<br> consolidation loop, separate from the main instance-level refinement (merging, re-classifying instances, etc.). This will be controlled by a new experiment hyperparameter,<br> num_ontology_refinement_cycles, allowing for independent control over how many times each type of refinement is performed. |\n",
        "| 25 | Decided | Decision | Implement Prioritized Random Sampling for Refinement | Problem: The current refinement process, which relies on purely random node selection, is not consistently converging. Quality metrics fluctuate between cycles,<br> suggesting that refinement efforts are not being focused on the least stable or newest parts of the graph.<br><br>Decision: To improve convergence, the selection logic will be updated to a prioritized random sampling model. This approach will favor nodes that are newer<br> or have undergone fewer refinement cycles, making the process more efficient.<br><br>Implementation Plan:<br>1. Enhance Node Properties: Add two properties to nodes: refinementCount (integer, default 0) and lastRefined (timestamp).<br>2. Update Refinement Logic: When a node is processed during a refinement step (e.g., merged, re-typed), its refinementCount will be incremented and the lastRefined timestamp updated.<br>3. Modify Selection Query: The Cypher query for selecting random entities will be modified to calculate a priority score, weighting nodes with a lower refinementCount<br> and older lastRefined timestamp more heavily. |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zYSNq58AkIrC"
      },
      "source": [
        "# Imports and Setup\n",
        "\n",
        "This section is responsible for preparing the notebook's runtime environment. It contains the installation commands for required Python libraries, the necessary imports, and the setup logic for mounting Google Drive and configuring global logging. Crucially, this section also contains the experiment_definitions list, which serves as the central configuration hub where all experimental runs are defined."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B6l5XfCXsLbP"
      },
      "source": [
        "## Setup and Environment Configuration\n",
        "This cell configures the foundational settings for the notebook. It detects whether the code is running in a Google Colab environment or a local setup and adjusts the root file path accordingly. It also mounts Google Drive if in Colab and sets global logging configurations, ensuring that output is consistent and informative while suppressing excessive noise from third-party libraries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 450,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBTdy4Ta4Y6S",
        "outputId": "9de2d73b-27a8-4c08-86d3-36105828f418"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "Running in Google Colab.\n",
            "Project Root Path set to: /content/drive/MyDrive/Mnemosyne\n"
          ]
        }
      ],
      "source": [
        "# A few imports are needed for the setup. The rest will come next.\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import logging\n",
        "# --- Environment Detection ---\n",
        "# Check if the script is running in a Google Colab environment.\n",
        "IS_COLAB = 'google.colab' in sys.modules\n",
        "\n",
        "# --- Global Configuration based on Environment ---\n",
        "if IS_COLAB:\n",
        "    # Colab-specific setup\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "    # Define the root path for Colab\n",
        "    ROOT_PATH = Path(\"/content/drive/MyDrive/Mnemosyne/\")\n",
        "\n",
        "    # Set the global logging level\n",
        "    logging.basicConfig(level=logging.INFO, force=True)\n",
        "\n",
        "else:\n",
        "    # Local machine setup\n",
        "    # Explicitly define the path to your synced Google Drive data folder\n",
        "    ROOT_PATH = Path(\"H:/My Drive/Mnemosyne\")\n",
        "\n",
        "    # Set the global logging level\n",
        "    # (This might be configured differently in a local IDE, but good for consistency)\n",
        "    logging.basicConfig(level=logging.INFO, force=True, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "# --- Universal Setup ---\n",
        "# Silence verbose logs from third-party libraries, works in both environments\n",
        "logging.getLogger(\"httpx\").setLevel(logging.INFO)\n",
        "logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.WARNING)\n",
        "\n",
        "print(f\"Running in {'Google Colab' if IS_COLAB else 'Local Environment'}.\")\n",
        "print(f\"Project Root Path set to: {ROOT_PATH}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NRtPINlGsG5B"
      },
      "source": [
        "## Imports\n",
        "This cell manages all necessary Python libraries. It begins by conditionally installing packages required for the Colab environment to ensure all dependencies are met. Following the installations, it imports all standard and third-party libraries used throughout the notebook. Finally, it includes a robust secrets management system that seamlessly handles API keys and credentials by loading them from Colab's userdata or a local .env file, depending on the runtime environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 451,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYGILMcJNBGR",
        "outputId": "d711c320-ce65-400b-ae2c-1540c8e55fcc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Colab environment detected. Installing required packages...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Successfully loaded secrets from google.colab.userdata.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Package installation complete.\n"
          ]
        }
      ],
      "source": [
        "# --- Conditional Installation for Colab ---\n",
        "if IS_COLAB:\n",
        "    print(\"Colab environment detected. Installing required packages...\")\n",
        "    # Install the necessary libraries\n",
        "    !pip install python-docx -q\n",
        "    !pip install neo4j -q\n",
        "    !pip install python-dotenv -q\n",
        "    !pip install pylint -q\n",
        "    !apt-get install graphviz -qq > /dev/null\n",
        "    !pip install pyvis -q\n",
        "    !pip install wordcloud -q\n",
        "    !pip install black[jupyter] -q\n",
        "    !pip install langextract[openai] -q\n",
        "    print(\"Package installation complete.\")\n",
        "\n",
        "# --- Universal Imports ---\n",
        "import docx\n",
        "import os\n",
        "import google.generativeai as genai\n",
        "import openai\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from neo4j import GraphDatabase\n",
        "from neo4j.exceptions import ServiceUnavailable\n",
        "import re\n",
        "import textwrap\n",
        "import dotenv # Keep this for local .env file loading\n",
        "from pathlib import Path\n",
        "import random\n",
        "import uuid\n",
        "import unittest\n",
        "from datetime import datetime, timezone\n",
        "import pandas as pd\n",
        "from collections import Counter\n",
        "from pyvis.network import Network\n",
        "from IPython.display import HTML\n",
        "import time\n",
        "from contextlib import contextmanager\n",
        "from wordcloud import WordCloud\n",
        "import matplotlib.pyplot as plt\n",
        "import langextract as lx\n",
        "\n",
        "# --- Handle Secrets Management ---\n",
        "# This block now correctly handles both Colab and local environments.\n",
        "if IS_COLAB:\n",
        "    from google.colab import userdata\n",
        "    secrets = userdata\n",
        "    logging.info(\"Successfully loaded secrets from google.colab.userdata.\")\n",
        "else:\n",
        "    # For local testing, use a class that mimics the userdata interface\n",
        "    # but loads variables from a .env file.\n",
        "    class LocalSecrets:\n",
        "        def __init__(self):\n",
        "            # Load environment variables from .env file in the root path\n",
        "            dotenv.load_dotenv(dotenv_path=ROOT_PATH / '.env')\n",
        "            logging.info(\"Loading secrets from local .env file.\")\n",
        "\n",
        "        def get(self, key):\n",
        "            value = os.getenv(key)\n",
        "            if value is None:\n",
        "                logging.warning(f\"Secret '{key}' not found in .env file.\")\n",
        "            return value\n",
        "\n",
        "    # Instantiate the local secrets manager\n",
        "    secrets = LocalSecrets()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gSOd60cSH_ta"
      },
      "source": [
        "## Logging Utilities"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 452,
      "metadata": {
        "id": "kz_3yOD5IAfM"
      },
      "outputs": [],
      "source": [
        "class IndentedLogger:\n",
        "    \"\"\"A singleton-like class to manage the global indentation level for logging.\"\"\"\n",
        "    level = 0\n",
        "    indent_char = \"  \"\n",
        "\n",
        "class IndentedFormatter(logging.Formatter):\n",
        "    \"\"\"A custom formatter to indent the message, not the log prefix.\"\"\"\n",
        "    def format(self, record):\n",
        "        indentation = IndentedLogger.indent_char * IndentedLogger.level\n",
        "\n",
        "        # Prepend the indentation directly to the message payload\n",
        "        record.msg = f\"{indentation}{record.msg}\"\n",
        "\n",
        "        # Now, let the parent formatter do the rest of the work\n",
        "        return super().format(record)\n",
        "\n",
        "@contextmanager\n",
        "def log_step(description: str):\n",
        "    \"\"\"A context manager to log the start/end of a step and indent nested logs.\"\"\"\n",
        "    logging.info(f\"‚ñ∂Ô∏è START: {description}...\")\n",
        "    IndentedLogger.level += 1\n",
        "    try:\n",
        "        yield\n",
        "    finally:\n",
        "        IndentedLogger.level -= 1\n",
        "        logging.info(f\"‚úÖ END: {description}.\")\n",
        "\n",
        "# --- Apply the new formatter ---\n",
        "def setup_indented_logging():\n",
        "    \"\"\"Removes old handlers and sets up the new indented formatter.\"\"\"\n",
        "    logger = logging.getLogger()\n",
        "    # Clear existing handlers to avoid duplicate messages\n",
        "    for handler in logger.handlers[:]:\n",
        "        logger.removeHandler(handler)\n",
        "\n",
        "    # Add a new handler with our custom formatter\n",
        "    handler = logging.StreamHandler()\n",
        "    # The format string now correctly applies to the output *after* we modify the message\n",
        "    formatter = IndentedFormatter('INFO - %(message)s')\n",
        "    handler.setFormatter(formatter)\n",
        "    logger.addHandler(handler)\n",
        "    logger.setLevel(logging.INFO)\n",
        "\n",
        "# Call the setup function once to configure the logger\n",
        "setup_indented_logging()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Smi6Orbe-7ZK"
      },
      "source": [
        "## Experiment Defintions\n",
        "\n",
        "This cell is the central control panel for the entire project. It contains two key configuration lists:\n",
        "\n",
        "1. **locations:** A registry of all file and directory paths used in the experiments. It assigns a unique location_id and a human-readable name to each path, making it easy to reference input documents, output directories, and ground truth files throughout the notebook.\n",
        "\n",
        "2. **experiment_groups:** The primary data structure that defines all experimental runs. It is a list of dictionaries, where each dictionary represents a group of related experiments. Within each group, you can define multiple individual experiments with specific hyperparameters, such as the LLM provider, model, chunk size, and number of refinement cycles. Flags like run_group and run_experiment allow for selectively enabling or disabling entire groups or specific tests without deleting their configurations."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- DEFINE YOUR FILE AND DIRECTORY LOCATIONS HERE ---\n",
        "\n",
        "# 1. Create a centralized registry of all locations.\n",
        "#    Paths are relative to the ROOT_PATH.\n",
        "locations = [\n",
        "    # Directories\n",
        "    {\"location_id\": \"LOC001\", \"name\": \"input_dir\", \"path\": \"Input/\", \"use\": \"Input Directory\"},\n",
        "    {\"location_id\": \"LOC002\", \"name\": \"output_dir\", \"path\": \"Output/\", \"use\": \"Output Directory\"},\n",
        "    {\"location_id\": \"LOC003\", \"name\": \"diagram_dir\", \"path\": \"figures/appendix_fig/\", \"use\": \"Diagrams Directory\"},\n",
        "\n",
        "    # Test Files\n",
        "    {\"location_id\": \"LOC004\", \"name\": \"ner_test_1_text\", \"path\": \"NER Test 1 - Text.docx\", \"use\": \"Input\"},\n",
        "    {\"location_id\": \"LOC005\", \"name\": \"ner_test_1_man_gt_txt\", \"path\": \"NER Test 1 - MAN - GT - Text.docx\", \"use\": \"Manual\"},\n",
        "    {\"location_id\": \"LOC006\", \"name\": \"ner_test_1_man_gt_json\", \"path\": \"NER Test 1 - MAN - GT - JSON.json\", \"use\": \"Manual\"},\n",
        "    {\"location_id\": \"LOC007\", \"name\": \"ner_test_1_le_gt_json\", \"path\": \"NER Test 1 - LE - GT - JSON.json\", \"use\": \"LangExtract\"},\n",
        "    {\"location_id\": \"LOC008\", \"name\": \"ner_test_2_text\", \"path\": \"NER Test 2 - Text.docx\", \"use\": \"Input\"},\n",
        "    {\"location_id\": \"LOC009\", \"name\": \"ner_test_2_man_gt_txt\", \"path\": \"NER Test 2 - MAN - GT - Text.docx\", \"use\": \"Manual\"},\n",
        "    {\"location_id\": \"LOC010\", \"name\": \"ner_test_2_man_gt_json\", \"path\": \"NER Test 2 - MAN - GT - JSON.json\", \"use\": \"Manual\"},\n",
        "    {\"location_id\": \"LOC011\", \"name\": \"ner_test_2_le_gt_json\", \"path\": \"NER Test 2 - LE - GT - JSON.json\", \"use\": \"LangExtract\"},\n",
        "    {\"location_id\": \"LOC012\", \"name\": \"ner_test_3_text\", \"path\": \"NER Test 3 - Text.docx\", \"use\": \"Input\"},\n",
        "    {\"location_id\": \"LOC013\", \"name\": \"ner_test_3_man_gt_txt\", \"path\": \"NER Test 3 - MAN - GT - Text.docx\", \"use\": \"Manual\"},\n",
        "    {\"location_id\": \"LOC014\", \"name\": \"ner_test_3_man_gt_json\", \"path\": \"NER Test 3 - MAN - GT - JSON.json\", \"use\": \"Manual\"},\n",
        "    {\"location_id\": \"LOC015\", \"name\": \"ner_test_3_le_gt_json\", \"path\": \"NER Test 3 - LE - GT - JSON.json\", \"use\": \"LangExtract\"},\n",
        "    {\"location_id\": \"LOC016\", \"name\": \"ner_test_4_text\", \"path\": \"NER Test 4 - Text.docx\", \"use\": \"Input\"},\n",
        "    {\"location_id\": \"LOC017\", \"name\": \"ner_test_4_man_gt_txt\", \"path\": \"NER Test 4 - MAN - GT - Text.docx\", \"use\": \"Manual\"},\n",
        "    {\"location_id\": \"LOC018\", \"name\": \"ner_test_4_man_gt_json\", \"path\": \"NER Test 4 - MAN - GT - JSON.json\", \"use\": \"Manual\"},\n",
        "    {\"location_id\": \"LOC019\", \"name\": \"ner_test_4_le_gt_json\", \"path\": \"NER Test 4 - LE - GT - JSON.json\", \"use\": \"LangExtract\"},\n",
        "\n",
        "    # Experiment Files\n",
        "    {\"location_id\": \"LOC020\", \"name\": \"exp_base_text\", \"path\": \"CTSA - Base.docx\", \"use\": \"Input\"},\n",
        "    {\"location_id\": \"LOC021\", \"name\": \"exp_base_le_gt_json\", \"path\": \"CTSA - Base - LE - GT - JSON.json\", \"use\": \"LangExtract\"},\n",
        "    {\"location_id\": \"LOC022\", \"name\": \"exp_et_text\", \"path\": \"ET - Base.docx\", \"use\": \"Input\"},\n",
        "    {\"location_id\": \"LOC023\", \"name\": \"exp_et_le_gt_json\", \"path\": \"ET - Base - LE - GT - JSON.json\", \"use\": \"LangExtract\"},\n",
        "]\n",
        "\n",
        "# --- DEFINE EXPERIMENTS HERE ---\n",
        "\n",
        "experiment_groups = [\n",
        "    # Utility Experiments - ID from 000 to 099\n",
        "    {\n",
        "        \"group_id\": \"GRP001\",\n",
        "        \"group_name\": \"Utility: Environment Cleanup\",\n",
        "        \"description\": \"Contains a utility experiment to reset the Neo4j database, ensuring a clean environment before a new run.\",\n",
        "        \"run_group\": True, \"generate_output\": False,\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"Clear Database\",\n",
        "                \"description\": \"Deletes all nodes and relationships from the Neo4j database to provide a clean slate.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": False, \"clear_database\": True,\n",
        "            }\n",
        "        ],\n",
        "    },\n",
        "    # Ground Truth Experiments - ID from 100 to 199\n",
        "    {\n",
        "        \"group_id\": \"GRP101\",\n",
        "        \"group_name\": \"Utility: Generate Ground Truth (Test Files)\",\n",
        "        \"description\": \"Generates the ground truth knowledge graphs for the four standardized test documents using the LangExtract library.\",\n",
        "        \"run_group\": False, \"generate_output\": False, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"Generate GT for Test File 1\",\n",
        "                \"description\": \"Generates the ground truth KG for the 'NER Test 1' document.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": False, \"doc_file_name\": \"ner_test_1_text\", \"er_file_name\": \"ner_test_1_le_gt_json\", \"processing_stage\": \"LE\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\",\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP002\",\n",
        "                \"experiment_name\": \"Generate GT for Test File 2\",\n",
        "                \"description\": \"Generates the ground truth KG for the 'NER Test 2' document.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": False, \"doc_file_name\": \"ner_test_2_text\", \"er_file_name\": \"ner_test_2_le_gt_json\", \"processing_stage\": \"LE\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\",\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP003\",\n",
        "                \"experiment_name\": \"Generate GT for Test File 3\",\n",
        "                \"description\": \"Generates the ground truth KG for the 'NER Test 3' document.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": False, \"doc_file_name\": \"ner_test_3_text\", \"er_file_name\": \"ner_test_3_le_gt_json\", \"processing_stage\": \"LE\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\",\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP004\",\n",
        "                \"experiment_name\": \"Generate GT for Test File 4\",\n",
        "                \"description\": \"Generates the ground truth KG for the 'NER Test 4' document.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": False, \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": \"ner_test_4_le_gt_json\", \"processing_stage\": \"LE\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\",\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"group_id\": \"GRP102\",\n",
        "        \"group_name\": \"Generate Ground Truth for base\",\n",
        "        \"description\": \"This group contains utility experiments designed to prepare the needed ground truth files. It uses the original textfiles and LangExtract.\",\n",
        "        \"run_group\": False, \"generate_output\": False, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"Experiment base ground truth\",\n",
        "                \"description\": \"A utility task that uses LangExtract from Google to generate a ground truth file from the source file.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": False, \"doc_file_name\": \"exp_base_text\", \"er_file_name\": \"exp_base_le_gt_json\", \"processing_stage\": \"LE\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\",\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"group_id\": \"GRP103\",\n",
        "        \"group_name\": \"Generate Ground Truth for Easttown\",\n",
        "        \"description\": \"This group contains utility experiments designed to prepare the needed ground truth files. It uses the original textfiles and LangExtract.\",\n",
        "        \"run_group\": False, \"generate_output\": False, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"Experiment base ground truth\",\n",
        "                \"description\": \"A utility task that uses LangExtract from Google to generate a ground truth file from the source file.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": False, \"doc_file_name\": \"exp_et_text\", \"er_file_name\": \"exp_et_le_gt_json\", \"processing_stage\": \"LE\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\",\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    # Experiments to test processing - ID from 200 to 299\n",
        "    {\n",
        "        \"group_id\": \"GRP201\",\n",
        "        \"group_name\": \"Baseline Model Validation (GPT-4.1)\",\n",
        "        \"description\": \"Establishes baseline performance of the Mnemosyne pipeline using OpenAI's GPT-4.1 model across the suite of standardized test documents.\",\n",
        "        \"run_group\": False, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"GPT-4.1 Baseline - Test File 1\",\n",
        "                \"description\": \"Evaluates the end-to-end pipeline performance on the 'NER Test 1' document.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_1_text\", \"er_file_name\": [\"ner_test_1_man_gt_json\", \"ner_test_1_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP002\",\n",
        "                \"experiment_name\": \"GPT-4.1 Baseline - Test File 2\",\n",
        "                \"description\": \"Evaluates the end-to-end pipeline performance on the 'NER Test 2' document.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_2_text\", \"er_file_name\": [\"ner_test_2_man_gt_json\", \"ner_test_2_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"use_batch_processing\": False,\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"group_id\": \"GRP202\",\n",
        "        \"group_name\": \"Mnemosyne test all test files with GPT 4.1\",\n",
        "        \"description\": \"This group serves as a tool test to validate the core functionality of the Mnemosyne pipeline using the GPT 4.1 series of models. Each experiment processes a small, standardized test document to ensure that data ingestion, entity extraction, and knowledge graph refinement are all working as expected. This provides a quick verification of the system's end-to-end health.\",\n",
        "        \"run_group\": False, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"Mnemosyne GPT 4.1 Test with File 1\",\n",
        "                \"description\": \"A baseline tool test using the flagship GPT 4.1 model. This experiment processes the 'NER Test 1' document to verify the complete data extraction and refinement pipeline, establishing a benchmark for the 4.1 series.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_1_text\", \"er_file_name\": [\"ner_test_1_man_gt_json\", \"ner_test_1_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.1, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP002\",\n",
        "                \"experiment_name\": \"Mnemosyne GPT 4.1 Test with File 2\",\n",
        "                \"description\": \"A baseline tool test using the flagship GPT 4.1 model. This experiment processes the 'NER Test 2' document to verify the complete data extraction and refinement pipeline, establishing a benchmark for the 4.1 series.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_2_text\", \"er_file_name\": [\"ner_test_2_man_gt_json\", \"ner_test_2_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.1, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP003\",\n",
        "                \"experiment_name\": \"Mnemosyne GPT 4.1 Test with File 3\",\n",
        "                \"description\": \"A baseline tool test using the flagship GPT 4.1 model. This experiment processes the 'NER Test 3' document to verify the complete data extraction and refinement pipeline, establishing a benchmark for the 4.1 series.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_3_text\", \"er_file_name\": [\"ner_test_3_man_gt_json\", \"ner_test_3_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.1, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP004\",\n",
        "                \"experiment_name\": \"Mnemosyne GPT 4.1 Test with File 4\",\n",
        "                \"description\": \"A baseline tool test using the flagship GPT 4.1 model. This experiment processes the 'NER Test 4' document to verify the complete data extraction and refinement pipeline, establishing a benchmark for the 4.1 series.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.1, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"use_batch_processing\": False,\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"group_id\": \"GRP203\",\n",
        "        \"group_name\": \"Mnemosyne test with Gemini 2.5\",\n",
        "        \"description\": \"This group serves as a tool test to validate the core functionality of the Mnemosyne pipeline using the Gemini 2.5 series of models. Each experiment processes a small, standardized test document to ensure that data ingestion, entity extraction, and knowledge graph refinement are all working as expected. This provides a quick verification of the system's end-to-end health.\",\n",
        "        \"run_group\": False, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"Mnemosyne Gemini 2.5 Pro Test\",\n",
        "                \"description\": \"A baseline tool test using the flagship Gemini 2.5 pro model. This experiment processes the 'NER Test 1' document to verify the complete data extraction and refinement pipeline, establishing a benchmark for the 4.1 series.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_1_text\", \"er_file_name\": [\"ner_test_1_man_gt_json\", \"ner_test_1_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\", \"temperature\": 0.1, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP002\",\n",
        "                \"experiment_name\": \"Mnemosyne Gemini 2.5 Flash Test\",\n",
        "                \"description\": \"A tool test using the more agile Gemini 2.5 flash model. This experiment processes the 'NER Test 1' document to evaluate the performance and accuracy of a smaller, faster model within the same family.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_1_text\", \"er_file_name\": [\"ner_test_1_man_gt_json\", \"ner_test_1_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-flash\", \"temperature\": 0.1, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"llm_max_output_tokens\": 8192, \"use_batch_processing\": False,\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    {\n",
        "        \"group_id\": \"GRP204\",\n",
        "        \"group_name\": \"Verify that batch processing works with Gemini and GPT.\",\n",
        "        \"description\": \"This group serves as a test to validate that the Mnemosyne implementation of batch processing works with Gemini and GPT.\",\n",
        "        \"run_group\": True, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"Mnemosyne GPT 4.1 Batch Test\",\n",
        "                \"description\": \"A validation of batch processing with GPT 4.1.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_1_text\", \"er_file_name\": [\"ner_test_1_man_gt_json\", \"ner_test_1_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.1, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"llm_max_output_tokens\": 16384, \"use_batch_processing\": True,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP002\",\n",
        "                \"experiment_name\": \"Mnemosyne Gemini 2.5 Pro Batch Test\",\n",
        "                \"description\": \"A validation of batch processing with Gemini 2.5 Pro.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"doc_file_name\": \"ner_test_1_text\", \"er_file_name\": [\"ner_test_1_man_gt_json\", \"ner_test_1_le_gt_json\"], \"processing_stage\": \"D2STM2LTMCON\", \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\", \"temperature\": 0.1, \"doc_chunk_size\": 7, \"doc_chunk_overlap\": 2, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"llm_max_output_tokens\": 16384, \"use_batch_processing\": True,\n",
        "            },\n",
        "        ],\n",
        "    },\n",
        "    # Experiments to test hyper parameters - ID from 300 to 399\n",
        "    {\n",
        "        \"group_id\": \"GRP301\",\n",
        "        \"group_name\": \"Ablation Study: Impact of Refinement Cycles\",\n",
        "        \"description\": \"Quantifies the impact of the LTM consolidation process by varying the number of refinement cycles. This ablation study measures the marginal improvement in graph quality with each additional cycle.\",\n",
        "        \"run_group\": False, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"0 Refinement Cycles (Baseline)\",\n",
        "                \"description\": \"Establishes a zero-refinement baseline to measure the quality of the raw, unconsolidated knowledge graph.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 0, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP002\",\n",
        "                \"experiment_name\": \"1 Refinement Cycle\",\n",
        "                \"description\": \"Measures the graph quality improvement after a single LTM refinement cycle.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 1, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP003\",\n",
        "                \"experiment_name\": \"2 Refinement Cycles\",\n",
        "                \"description\": \"Evaluates the compounding benefits of a second pass of graph consolidation.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 2, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP004\",\n",
        "                \"experiment_name\": \"3 Refinement Cycles\",\n",
        "                \"description\": \"Evaluates the compounding benefits of a second pass of graph consolidation.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP005\",\n",
        "                \"experiment_name\": \"4 Refinement Cycles\",\n",
        "                \"description\": \"Evaluates the compounding benefits of a second pass of graph consolidation.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 4, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"group_id\": \"GRP302\",\n",
        "        \"group_name\": \"Parameter Sensitivity: Document Chunk Size\",\n",
        "        \"description\": \"Investigates the impact of the `doc_chunk_size` hyperparameter on knowledge graph quality. This analysis aims to find the optimal balance between providing sufficient context to the LLM and processing efficiency.\",\n",
        "        \"run_group\": False, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"Chunk Size: 4 Paragraphs\",\n",
        "                \"description\": \"Evaluates pipeline performance with a small chunk size (4 paragraphs) to test behavior with limited context.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 4, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP002\",\n",
        "                \"experiment_name\": \"Chunk Size: 8 Paragraphs (Control)\",\n",
        "                \"description\": \"Tests the pipeline with a medium chunk size (8 paragraphs), serving as the control for this group.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP003\",\n",
        "                \"experiment_name\": \"Chunk Size: 12 Paragraphs\",\n",
        "                \"description\": \"Tests with a large chunk size (12 paragraphs) to determine if more context improves extraction quality.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 12, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"group_id\": \"GRP303\",\n",
        "        \"group_name\": \"Comparative Analysis: OpenAI vs. Gemini Models\",\n",
        "        \"description\": \"Conducts a direct performance and quality comparison between models from OpenAI and Google. All hyperparameters are held constant to isolate the impact of the foundation model.\",\n",
        "        \"run_group\": False, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"OpenAI GPT-4.1 Benchmark\",\n",
        "                \"description\": \"Establishes the performance benchmark for the OpenAI GPT-4.1 model under standard parameters.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP004\",\n",
        "                \"experiment_name\": \"Google Gemini 2.5 Pro Benchmark\",\n",
        "                \"description\": \"Establishes the performance benchmark for the Google Gemini 2.5 Pro model for direct comparison.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"group_id\": \"GRP304\",\n",
        "        \"group_name\": \"Comparative Analysis: OpenAI vs. Gemini Models\",\n",
        "        \"description\": \"Conducts a direct performance and quality comparison between models from OpenAI and Google. All hyperparameters are held constant to isolate the impact of the foundation model.\",\n",
        "        \"run_group\": False, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"OpenAI GPT-4.1 Benchmark\",\n",
        "                \"description\": \"Establishes the performance benchmark for the OpenAI GPT-4.1 model under standard parameters.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "            {\n",
        "                \"experiment_id\": \"EXP004\",\n",
        "                \"experiment_name\": \"Google Gemini 2.5 Pro Benchmark\",\n",
        "                \"description\": \"Establishes the performance benchmark for the Google Gemini 2.5 Pro model for direct comparison.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"ner_test_4_text\", \"er_file_name\": [\"ner_test_4_man_gt_json\", \"ner_test_4_le_gt_json\"], \"llm_provider\": \"Gemini\", \"llm_model\": \"gemini-2.5-pro\", \"temperature\": 0.0, \"doc_chunk_size\": 8, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "    # Experiments - ID from 400 to 499\n",
        "    {\n",
        "        \"group_id\": \"GRP401\",\n",
        "        \"group_name\": \"Primary Use Case: Legal Document Analysis\",\n",
        "        \"description\": \"Applies the optimized Mnemosyne pipeline to the primary target document, the Conewago Township Sewer Authority (CTSA) legal text, to generate a comprehensive knowledge graph.\",\n",
        "        \"run_group\": False, \"generate_output\": True, \"random_seed\": 42, \"input_location\": \"input_dir\", \"output_location\": \"output_dir\", \"diagram_location\": \"diagram_dir\",\n",
        "        \"experiments\": [\n",
        "            {\n",
        "                \"experiment_id\": \"EXP001\",\n",
        "                \"experiment_name\": \"CTSA Document Processing with GPT-4.1\",\n",
        "                \"description\": \"Performs a full run (extraction, ingestion, and consolidation) on the 82-page CTSA document to generate the final knowledge graph for analysis.\",\n",
        "                \"run_experiment\": True, \"run_workflow\": True, \"clear_database\": True, \"processing_stage\": \"D2STM2LTMCON\", \"doc_file_name\": \"exp_base_text\", \"er_file_name\": \"exp_base_le_gt_json\", \"llm_provider\": \"OpenAI\", \"llm_model\": \"gpt-4.1\", \"temperature\": 0.0, \"doc_chunk_size\": 20, \"doc_chunk_overlap\": 3, \"max_chunks_to_process\": 500, \"num_refinement_cycles\": 3, \"ltm_merge_sample_size\": 20, \"ltm_hierarchy_sample_size\": 15, \"max_output_tokens\": 16384, \"stm_fullness_threshold\": 10000, \"use_batch_processing\": False,\n",
        "            },\n",
        "        ]\n",
        "    },\n",
        "]"
      ],
      "metadata": {
        "id": "BWtFPZDfv2go"
      },
      "execution_count": 453,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wDJpQ8m66jKl"
      },
      "source": [
        "## Classes used for exceptions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 454,
      "metadata": {
        "id": "clkDemShJa_t"
      },
      "outputs": [],
      "source": [
        "class GeminiInitializationError(Exception):\n",
        "    \"\"\"Custom exception for errors during Gemini model initialization.\"\"\"\n",
        "    pass\n",
        "\n",
        "class GeminiQueryError(Exception):\n",
        "    \"\"\"Custom exception for errors during a Gemini API query.\"\"\"\n",
        "    pass\n",
        "\n",
        "class GeminiResponseBlockedError(Exception):\n",
        "    \"\"\"Custom exception when a Gemini API response is blocked due to safety settings.\"\"\"\n",
        "    pass\n",
        "\n",
        "class OpenAIInitializationError(Exception):\n",
        "    \"\"\"Custom exception for OpenAI initialization errors.\"\"\"\n",
        "    pass\n",
        "\n",
        "class OpenAIQueryError(Exception):\n",
        "    \"\"\"Custom exception for errors during an OpenAI API query.\"\"\"\n",
        "    pass\n",
        "\n",
        "class ParagraphsTooLong(Exception):\n",
        "    \"\"\"Custom exception for prompts that exceed the maximum length.\"\"\"\n",
        "    pass\n",
        "\n",
        "class JSONParsingError(Exception):\n",
        "    \"\"\"Custom exception for errors during JSON parsing or when JSON is not found.\"\"\"\n",
        "    pass\n",
        "\n",
        "class MaxTokensExceededError(Exception):\n",
        "    \"\"\"Custom exception for when a prompt fails due to exceeding max tokens, even after a retry.\"\"\"\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fq2A7fK-lrZe"
      },
      "source": [
        "# Graph Database Classes\n",
        "This section contains the classes responsible for all communication with the Neo4j database. Following the **Single Responsibility Principle**, the logic is divided into specialized classes that are managed by a central `GraphDB` connection handler.\n",
        "\n",
        "- **`GraphDB`**: The main class that opens, closes, and manages the connection driver.\n",
        "- **`GraphDBWriter`**: Handles all initial data ingestion operations.\n",
        "- **`GraphDBRefiner`**: Contains all methods for consolidating and refining the graph, such as merging nodes and building hierarchies.\n",
        "- **`GraphDBReader`**: Provides all methods for querying and reading data from the graph for analysis and reporting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hwRyiqxy4j8V"
      },
      "source": [
        "## Graph Schema Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 455,
      "metadata": {
        "id": "MCVYYMEA4hzJ"
      },
      "outputs": [],
      "source": [
        "## Graph Schema Class\n",
        "# --- NEW: Centralized Schema Constants ---\n",
        "class GraphSchema:\n",
        "    \"\"\"A single source of truth for graph schema elements like labels, types, and properties.\"\"\"\n",
        "    # Node Labels\n",
        "    NODE_LABEL_INSTANCE = \"Instance\"\n",
        "    NODE_LABEL_TYPE = \"Type\"\n",
        "    NODE_LABEL_SOURCE = \"Source\"\n",
        "    NODE_LABEL_NODE = \"Node\"\n",
        "    NODE_LABEL_DO_NOT_CHANGE = \"DoNotChange\"\n",
        "\n",
        "    # Relationship Types\n",
        "    REL_IS_A = \"IS_A\"\n",
        "    REL_PART_OF = \"PART_OF\"\n",
        "    REL_FROM = \"FROM\"\n",
        "\n",
        "    # Property Keys (CamelCase for consistency)\n",
        "    PROP_ID = \"id\"\n",
        "    PROP_NAME = \"name\"\n",
        "    PROP_DISPLAY_NAME = \"displayName\"\n",
        "    PROP_LABELS = \"labels\"\n",
        "    PROP_PROPERTIES = \"properties\"\n",
        "    PROP_MERGE_COUNT = \"mergeCount\"\n",
        "    PROP_REFINEMENT_COUNT = \"refinementCount\"\n",
        "    PROP_LAST_REFINED = \"lastRefined\"\n",
        "    PROP_SOURCE = \"source\"\n",
        "    PROP_TARGET = \"target\"\n",
        "    PROP_TYPE = \"type\"\n",
        "    PROP_CREATION_STAGE = \"creationStage\" # Task 132: New property key\n",
        "\n",
        "    # JSON Keys from LLM Output\n",
        "    JSON_KEY_NODES = \"nodes\"\n",
        "    JSON_KEY_RELATIONSHIPS = \"relationships\"\n",
        "    JSON_KEY_MERGE_PAIRS = \"merge_pairs\"\n",
        "    JSON_KEY_PART_OF_PAIRS = \"part_of_pairs\"\n",
        "    JSON_KEY_PART_ID = \"part_id\"\n",
        "    JSON_KEY_WHOLE_ID = \"whole_id\"\n",
        "    JSON_KEY_PART_IDS = \"part_ids\"\n",
        "\n",
        "    # Canonical Node Names & IDs\n",
        "    CANONICAL_NAME_THING = \"thing\"\n",
        "    CANONICAL_DISPLAY_NAME_THING = \"Thing\"\n",
        "    CANONICAL_ID_THING = \"thing-type-global\"\n",
        "\n",
        "    CANONICAL_NAME_SOURCE = \"source\"\n",
        "    CANONICAL_DISPLAY_NAME_SOURCE = \"Source\"\n",
        "    CANONICAL_ID_SOURCE = \"source-type-global\"\n",
        "\n",
        "    # --- Task 132: Creation Stage Constants ---\n",
        "    STAGE_SOURCE_EXTRACTION = \"SOURCE_EXTRACTION\"\n",
        "    STAGE_INGESTION = \"INGESTION\"\n",
        "    STAGE_CONSOLIDATION = \"CONSOLIDATION\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKLfBKAzM-PK"
      },
      "source": [
        "## Graph Database Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 456,
      "metadata": {
        "id": "vRqGyyRRmTh9"
      },
      "outputs": [],
      "source": [
        "class GraphDB:\n",
        "    \"\"\"\n",
        "    A container for the shared Neo4j driver and the specialized managers.\n",
        "    This class is responsible for opening and closing the database connection.\n",
        "    \"\"\"\n",
        "    def __init__(self, uri, auth):\n",
        "        \"\"\"Initializes the main GraphDB connection and its specialized managers.\"\"\"\n",
        "        self.driver = None\n",
        "        try:\n",
        "            # Establish the connection driver\n",
        "            self.driver = GraphDatabase.driver(uri, auth=auth)\n",
        "            self.driver.verify_connectivity()\n",
        "            logging.info(\"Successfully connected to Neo4j database.\")\n",
        "\n",
        "            # Instantiate managers with the shared driver\n",
        "            self.reader = GraphDBReader(self.driver)\n",
        "            self.writer = GraphDBWriter(self.driver)\n",
        "            self.refiner = GraphDBRefiner(self.driver)\n",
        "\n",
        "        except (ServiceUnavailable, ValueError) as e:\n",
        "            logging.error(f\"Failed to connect to Neo4j. DB operations will fail. Error: {e}\")\n",
        "            raise # Re-raise the exception to halt execution if connection fails\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred during Neo4j initialization. Error: {e}\")\n",
        "            raise\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Closes the Neo4j driver connection.\"\"\"\n",
        "        if self.driver:\n",
        "            self.driver.close()\n",
        "            logging.info(\"Neo4j connection closed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiuKIp9Wmu-P"
      },
      "source": [
        "## Graph Database Base Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 457,
      "metadata": {
        "id": "_LuiIAEBm0jI"
      },
      "outputs": [],
      "source": [
        "class GraphDBBase:\n",
        "    \"\"\"Base class for Neo4j database interactions, operating on a shared driver.\"\"\"\n",
        "    # --- NEW: Constants for dynamic label prefixes ---\n",
        "    EXP_PREFIX = \"EXP\"\n",
        "    RUN_PREFIX = \"RUN\"\n",
        "\n",
        "    def __init__(self, driver):\n",
        "        self.driver = driver\n",
        "        if not self.driver:\n",
        "            raise ConnectionError(\"GraphDBBase received an invalid Neo4j driver.\")\n",
        "\n",
        "    def _get_scoped_label(self, experiment_id: str, run_timestamp: str) -> str:\n",
        "        \"\"\"Helper to create a sanitized, scoped label string for Cypher queries.\"\"\"\n",
        "        sanitized_exp_id = re.sub(r'[^a-zA-Z0-9_]', '_', experiment_id)\n",
        "        sanitized_run_ts = re.sub(r'[^a-zA-Z0-9_]', '_', run_timestamp.replace(f\"{self.RUN_PREFIX}_\", \"\"))\n",
        "        return f\":`{self.EXP_PREFIX}_{sanitized_exp_id}`:`{self.RUN_PREFIX}_{sanitized_run_ts}`\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LhE8033Bm7TA"
      },
      "source": [
        "## Graph Database Writer Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 458,
      "metadata": {
        "id": "UEoq7IHJm_nI"
      },
      "outputs": [],
      "source": [
        "class GraphDBWriter(GraphDBBase):\n",
        "    \"\"\"Handles all write operations to the Neo4j database.\"\"\"\n",
        "    def kg_to_ltm(self, kg_data_str: str, experiment_id: str, run_timestamp: str):\n",
        "        \"\"\"Writes a KG fragment (JSON string) to Neo4j, applying experiment labels.\"\"\"\n",
        "        try:\n",
        "            data = json.loads(kg_data_str)\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.exception(f\"Error decoding JSON for LTM: {e}\")\n",
        "            raise\n",
        "\n",
        "        scoped_label_parts = self._get_scoped_label(experiment_id, run_timestamp).split(':')\n",
        "        experiment_label = scoped_label_parts[1].strip('`')\n",
        "        run_label = scoped_label_parts[2].strip('`')\n",
        "\n",
        "        with self.driver.session() as session:\n",
        "            tx = session.begin_transaction()\n",
        "            try:\n",
        "                # Use constants from GraphSchema\n",
        "                for node in data.get(GraphSchema.JSON_KEY_NODES, []):\n",
        "                    all_labels = node.get(GraphSchema.PROP_LABELS, []) + [experiment_label, run_label]\n",
        "                    tx.run(\"\"\"\n",
        "                        MERGE (n:Node {id: $id})\n",
        "                        ON CREATE SET n.created = timestamp(), n.mergeCount = 0, n.refinementCount = 0, n.lastRefined = timestamp()\n",
        "                        ON MATCH SET n.mergeCount = COALESCE(n.mergeCount, 0), n.refinementCount = COALESCE(n.refinementCount, 0), n.lastRefined = COALESCE(n.lastRefined, timestamp())\n",
        "                        SET n += $props\n",
        "                        WITH n\n",
        "                        CALL apoc.create.addLabels(n, $labels) YIELD node\n",
        "                        RETURN node\n",
        "                    \"\"\", id=node[GraphSchema.PROP_ID], props=node.get(GraphSchema.PROP_PROPERTIES, {}), labels=all_labels)\n",
        "\n",
        "                for rel in data.get(GraphSchema.JSON_KEY_RELATIONSHIPS, []):\n",
        "                    tx.run(\"\"\"\n",
        "                        MATCH (source:Node {id: $source_id})\n",
        "                        MATCH (target:Node {id: $target_id})\n",
        "                        CALL apoc.create.relationship(source, $rel_type, $props, target) YIELD rel\n",
        "                        RETURN rel\n",
        "                    \"\"\", source_id=rel[GraphSchema.PROP_SOURCE], target_id=rel[GraphSchema.PROP_TARGET], rel_type=rel[GraphSchema.PROP_TYPE].upper(), props=rel.get(GraphSchema.PROP_PROPERTIES, {}))\n",
        "\n",
        "                tx.commit()\n",
        "                logging.info(\"Wrote KG fragment to LTM with experiment and run labels.\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"An error occurred during transaction in kg_to_ltm: {e}\")\n",
        "                tx.rollback()\n",
        "                raise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SPPp2Lwomacp"
      },
      "source": [
        "## Graph Database Refiner Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 459,
      "metadata": {
        "id": "6d3RPLqmmjS4"
      },
      "outputs": [],
      "source": [
        "## Graph Database Refiner Class\n",
        "class GraphDBRefiner(GraphDBBase):\n",
        "    \"\"\"Handles graph refinement and consolidation operations in Neo4j.\"\"\"\n",
        "\n",
        "    def __init__(self, driver):\n",
        "        super().__init__(driver)\n",
        "        self.reader = GraphDBReader(driver)\n",
        "\n",
        "    def merge_duplicate_relationships(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"\n",
        "        Finds and merges duplicate relationships between the same two nodes.\n",
        "        It keeps one relationship and adds a 'strength' property indicating\n",
        "        how many duplicates were merged.\n",
        "        \"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"\"\"\n",
        "            MATCH (a{scoped_label})-[r]->(b{scoped_label})\n",
        "            // Group by the start node, end node, and relationship type\n",
        "            WITH a, b, type(r) AS relType, collect(r) AS rels\n",
        "            WHERE size(rels) > 1\n",
        "\n",
        "            // Keep the first relationship, mark the rest for deletion\n",
        "            WITH rels[0] AS firstRel, tail(rels) AS relsToDelete, size(rels) as strength\n",
        "\n",
        "            // Set a strength property on the relationship we're keeping\n",
        "            SET firstRel.strength = strength\n",
        "\n",
        "            // Delete all the duplicate relationships\n",
        "            FOREACH (r IN relsToDelete | DELETE r)\n",
        "\n",
        "            RETURN count(firstRel) AS merged_rel_groups\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            merge_count = result.single()[0] or 0\n",
        "        return merge_count\n",
        "\n",
        "    def merge_exact_duplicates_by_name_and_type(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"\n",
        "        Deterministically merges nodes that have the exact same name and primary type label.\n",
        "        This is a faster, non-LLM approach to handle obvious duplicates.\n",
        "        \"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        # This query finds nodes with the same name and the same primary type label\n",
        "        # (ignoring framework labels like 'Instance', 'Node', 'EXP_...', etc.),\n",
        "        # and merges them into a single node using the APOC library.\n",
        "        query = f\"\"\"\n",
        "            MATCH (n{scoped_label})\n",
        "            WHERE NOT n:DoNotChange AND n.name IS NOT NULL\n",
        "            WITH n.name AS name,\n",
        "                 // Extract the primary type label, ignoring framework-specific labels\n",
        "                 [label IN labels(n) WHERE NOT label IN ['Instance', 'Node', 'DoNotChange'] AND NOT label STARTS WITH 'EXP_' AND NOT label STARTS WITH 'RUN_'] [0] AS primaryType,\n",
        "                 collect(n) AS nodes\n",
        "            WHERE size(nodes) > 1 AND primaryType IS NOT NULL\n",
        "\n",
        "            // Pre-calculate the total merge count BEFORE merging to avoid accessing deleted nodes.\n",
        "            WITH nodes, reduce(total = 0, x IN nodes | total + coalesce(x.mergeCount, 0)) AS totalExistingMergeCount\n",
        "\n",
        "            // Merge the collected nodes into a single node\n",
        "            CALL apoc.refactor.mergeNodes(nodes, {{properties: 'overwrite', mergeRels: true}}) YIELD node\n",
        "\n",
        "            // Update the mergeCount on the new canonical node using the pre-calculated value\n",
        "            SET node.mergeCount = size(nodes) - 1 + totalExistingMergeCount,\n",
        "                node.refinementCount = coalesce(node.refinementCount, 0) + 1,\n",
        "                node.lastRefined = timestamp()\n",
        "\n",
        "            RETURN count(node) AS merge_operations\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            merge_count = result.single()[0] or 0\n",
        "        return merge_count\n",
        "    def clear_database(self):\n",
        "        \"\"\"Clears all nodes and relationships from the database.\"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            session.run(\"MATCH (n) DETACH DELETE n\")\n",
        "        logging.info(\"Neo4j database cleared.\")\n",
        "\n",
        "    # --- MODIFICATION START ---\n",
        "    # NEW: Method to clear only the golden standard data\n",
        "    def clear_golden_standard_data(self):\n",
        "        \"\"\"Deletes all nodes and relationships labeled as the golden standard.\"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            query = f\"\"\"\n",
        "                MATCH (n:`EXP_{Experiment.GOLDEN_STANDARD_EXP_ID}`)\n",
        "                DETACH DELETE n\n",
        "            \"\"\"\n",
        "            result = session.run(query)\n",
        "            logging.info(f\"Cleared {result.consume().counters.nodes_deleted} previous Golden Standard nodes.\")\n",
        "    # --- MODIFICATION END ---\n",
        "\n",
        "    def merge_entities(self, merge_data: dict, experiment_id: str, run_timestamp: str) -> tuple[int, set]:\n",
        "        \"\"\"\n",
        "        Merges nodes based on LLM output, scoped to the current experiment.\n",
        "        Handles transitive merges within a single batch and returns the count of merges and a set of removed node IDs.\n",
        "        \"\"\"\n",
        "        if not merge_data or GraphSchema.JSON_KEY_MERGE_PAIRS not in merge_data or not isinstance(merge_data.get(GraphSchema.JSON_KEY_MERGE_PAIRS), list):\n",
        "            logging.info(\"Merge data is missing, not a list, or has no 'merge_pairs' key. Skipping merge.\")\n",
        "            return 0, set()\n",
        "\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        merged_pairs_count = 0\n",
        "        removed_ids = set()\n",
        "        redirects = {}\n",
        "\n",
        "        valid_pairs = [\n",
        "            pair for pair in merge_data.get(GraphSchema.JSON_KEY_MERGE_PAIRS, [])\n",
        "            if isinstance(pair, list) and len(pair) == 2 and isinstance(pair[0], str) and isinstance(pair[1], str)\n",
        "        ]\n",
        "\n",
        "        if not valid_pairs:\n",
        "            logging.debug(\"No valid merge pairs were found to process.\")\n",
        "            return 0, set()\n",
        "\n",
        "        all_ids_in_pairs = {id for pair in valid_pairs for id in pair}\n",
        "        display_name_map = self.reader.get_display_names_for_ids(list(all_ids_in_pairs))\n",
        "\n",
        "        with self.driver.session() as session:\n",
        "            for pair in valid_pairs:\n",
        "                id_to_keep_candidate, id_to_remove_candidate = sorted(pair)\n",
        "                final_id_to_keep = redirects.get(id_to_keep_candidate, id_to_keep_candidate)\n",
        "                while final_id_to_keep in redirects:\n",
        "                    final_id_to_keep = redirects[final_id_to_keep]\n",
        "                final_id_to_remove = redirects.get(id_to_remove_candidate, id_to_remove_candidate)\n",
        "                while final_id_to_remove in redirects:\n",
        "                    final_id_to_remove = redirects[final_id_to_remove]\n",
        "                if final_id_to_remove == final_id_to_keep:\n",
        "                    continue\n",
        "                name_to_keep = display_name_map.get(final_id_to_keep, \"N/A\")\n",
        "                name_to_remove = display_name_map.get(final_id_to_remove, \"N/A\")\n",
        "                try:\n",
        "                    count_result = session.run(\"\"\"\n",
        "                        MATCH (a {id: $id1})\n",
        "                        MATCH (b {id: $id2})\n",
        "                        RETURN a.mergeCount AS count1, b.mergeCount AS count2\n",
        "                    \"\"\", id1=final_id_to_keep, id2=final_id_to_remove).single()\n",
        "                    if not count_result:\n",
        "                        logging.debug(f\"Skipping merge for pair ['{name_to_remove}' ({final_id_to_remove}), '{name_to_keep}' ({final_id_to_keep})]: one or both nodes not found in the database.\")\n",
        "                        continue\n",
        "                    count1 = count_result['count1'] or 0\n",
        "                    count2 = count_result['count2'] or 0\n",
        "                    new_merge_count = count1 + count2 + 1\n",
        "                    merge_query = f\"\"\"\n",
        "                        MATCH (a{scoped_label} {{id: $id1}}) WHERE NOT a:DoNotChange\n",
        "                        MATCH (b{scoped_label} {{id: $id2}}) WHERE NOT b:DoNotChange\n",
        "                        CALL apoc.refactor.mergeNodes([a, b], {{properties: 'overwrite', mergeRels: true}}) YIELD node\n",
        "                        SET node.mergeCount = $new_count,\n",
        "                            node.refinementCount = COALESCE(node.refinementCount, 0) + 1,\n",
        "                            node.lastRefined = timestamp()\n",
        "                        RETURN node\n",
        "                    \"\"\"\n",
        "                    result = session.run(merge_query, id1=final_id_to_keep, id2=final_id_to_remove, new_count=new_merge_count)\n",
        "                    if result.single():\n",
        "                        logging.info(f\"Merged '{name_to_remove}' ({final_id_to_remove}) into '{name_to_keep}' ({final_id_to_keep}).\")\n",
        "                        merged_pairs_count += 1\n",
        "                        removed_ids.add(final_id_to_remove)\n",
        "                        redirects[id_to_remove_candidate] = final_id_to_keep\n",
        "                        redirects[final_id_to_remove] = final_id_to_keep\n",
        "                        display_name_map[id_to_remove_candidate] = name_to_keep\n",
        "                        display_name_map[final_id_to_remove] = name_to_keep\n",
        "                except Exception as e:\n",
        "                    logging.warning(f\"Could not merge pair ['{name_to_remove}' ({final_id_to_remove}), '{name_to_keep}' ({final_id_to_keep})], it may have been merged already. Error: {e}\")\n",
        "        return merged_pairs_count, removed_ids\n",
        "\n",
        "    def create_and_relate_type_entities_by_id(self, relationships: list) -> int:\n",
        "        \"\"\"Creates IS_A relationships between Type nodes using their unique IDs and logs the action.\"\"\"\n",
        "        relationships_created_count = 0\n",
        "        with self.driver.session() as session:\n",
        "            for rel in relationships:\n",
        "                if isinstance(rel, dict) and 'parent_id' in rel and 'child_id' in rel:\n",
        "                    p_id, c_id = rel['parent_id'], rel['child_id']\n",
        "                    p_name, c_name = rel.get('parent_name', p_id), rel.get('child_name', c_id)\n",
        "\n",
        "                    if p_id and c_id:\n",
        "                        query = \"\"\"\n",
        "                        MATCH (c:Type {id: $c_id})\n",
        "                        MATCH (p:Type {id: $p_id})\n",
        "                        MERGE (c)-[r:IS_A]->(p)\n",
        "                        ON CREATE SET\n",
        "                            r.created_now = true,\n",
        "                            c.refinementCount = COALESCE(c.refinementCount, 0) + 1, c.lastRefined = timestamp(),\n",
        "                            p.refinementCount = COALESCE(p.refinementCount, 0) + 1, p.lastRefined = timestamp(),\n",
        "                            r.creationStage = $stage\n",
        "                        WITH r, r.created_now as created_flag\n",
        "                        REMOVE r.created_now\n",
        "                        RETURN created_flag\n",
        "                        \"\"\"\n",
        "                        result = session.run(query, c_id=c_id, p_id=p_id, stage=GraphSchema.STAGE_CONSOLIDATION).single()\n",
        "                        if result and result['created_flag']:\n",
        "                            logging.info(f\"Created IS_A relationship: '{c_name}' ({c_id}) -> '{p_name}' ({p_id}).\")\n",
        "                            relationships_created_count += 1\n",
        "        return relationships_created_count\n",
        "\n",
        "\n",
        "    def process_nodes_without_is_a(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Finds nodes within an experiment that lack an IS_A relationship and links them to 'Thing'.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        # Task 132: Set creationStage for new relationship\n",
        "        query = f\"\"\"\n",
        "            MATCH (n{scoped_label})\n",
        "            WHERE NOT (n)-[:IS_A]->() AND NOT n:DoNotChange AND NOT n:Type\n",
        "            MERGE (thing:Type {{name: 'thing'}})\n",
        "            ON CREATE SET thing.id = 'thing-type-global', thing.displayName = 'Thing', thing.mergeCount = 0, thing.refinementCount = 0, thing.lastRefined = timestamp()\n",
        "            MERGE (n)-[r:IS_A]->(thing)\n",
        "            SET n.refinementCount = COALESCE(n.refinementCount, 0) + 1, n.lastRefined = timestamp(),\n",
        "                r.creationStage = '{GraphSchema.STAGE_CONSOLIDATION}'\n",
        "            RETURN count(n) as updated_count\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return result.single()[0] or 0\n",
        "\n",
        "    def mark_source_nodes_as_donotchange(self):\n",
        "        \"\"\"Applies the DoNotChange label to all nodes with the Source label.\"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(\"MATCH (n:Source) SET n:DoNotChange RETURN count(n) as nodes_processed\")\n",
        "            logging.info(f\"Marked {result.single()[0]} source nodes with 'DoNotChange' label.\")\n",
        "\n",
        "    def link_orphan_types_to_thing(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Finds orphan :Type nodes within an experiment and connects them to the global 'Thing' node.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        # Task 132: Set creationStage for new relationship\n",
        "        query = f\"\"\"\n",
        "            MERGE (thing:Type {{name: 'thing'}})\n",
        "            ON CREATE SET thing.id = 'thing-type-global', thing.displayName = 'Thing', thing.mergeCount = 0, thing.refinementCount = 0, thing.lastRefined = timestamp()\n",
        "            WITH thing\n",
        "            MATCH (t:Type{scoped_label})\n",
        "            WHERE t.name <> 'thing' AND NOT (t)-[:IS_A]->(:Type)\n",
        "            MERGE (t)-[r:IS_A]->(thing)\n",
        "            SET t.refinementCount = COALESCE(t.refinementCount, 0) + 1, t.lastRefined = timestamp(),\n",
        "                r.creationStage = '{GraphSchema.STAGE_CONSOLIDATION}'\n",
        "            RETURN count(t) as linked_count\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return result.single()[0] or 0\n",
        "\n",
        "    def create_part_of_relationships(self, part_of_data: list, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Creates PART_OF relationships based on a list of {'part_id': id, 'whole_id': id} dicts.\"\"\"\n",
        "        if not part_of_data:\n",
        "            return 0\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        created_count = 0\n",
        "        with self.driver.session() as session:\n",
        "            # Task 132: Set creationStage for new relationship\n",
        "            query = f\"\"\"\n",
        "                UNWIND $pairs AS pair\n",
        "                MATCH (p{scoped_label} {{id: pair.part_id}})\n",
        "                MATCH (w{scoped_label} {{id: pair.whole_id}})\n",
        "                MERGE (p)-[r:PART_OF]->(w)\n",
        "                SET p.refinementCount = COALESCE(p.refinementCount, 0) + 1, p.lastRefined = timestamp(),\n",
        "                    w.refinementCount = COALESCE(w.refinementCount, 0) + 1, w.lastRefined = timestamp(),\n",
        "                    r.creationStage = '{GraphSchema.STAGE_CONSOLIDATION}'\n",
        "                RETURN count(r)\n",
        "            \"\"\"\n",
        "            result = session.run(query, pairs=part_of_data)\n",
        "            created_count = result.single()[0] or 0\n",
        "        return created_count\n",
        "\n",
        "\n",
        "    def reclassify_instance(self, instance_id: str, new_type_id: str, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Reclassifies an instance by only changing its IS_A relationship, not its labels.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        with self.driver.session() as session:\n",
        "            tx = session.begin_transaction()\n",
        "            try:\n",
        "                result = tx.run(\n",
        "                    f\"MATCH (i{scoped_label} {{id: $instance_id}})-[r:IS_A]->(t:Type) RETURN i, r, t\",\n",
        "                    instance_id=instance_id\n",
        "                )\n",
        "                record = result.single()\n",
        "                if not record:\n",
        "                    logging.info(f\"Could not find instance with ID '{instance_id}' for re-classification.\")\n",
        "                    tx.rollback(); return 0\n",
        "\n",
        "                instance_node, old_rel, old_type_node = record['i'], record['r'], record['t']\n",
        "                old_type_name = old_type_node.get('name', 'Unknown')\n",
        "                instance_display_name = instance_node.get('displayName', instance_id)\n",
        "\n",
        "                new_type_result = tx.run(\"MATCH (t:Type {id: $new_type_id}) RETURN t.name AS name\", new_type_id=new_type_id)\n",
        "                new_type_record = new_type_result.single()\n",
        "                if not new_type_record:\n",
        "                    logging.warning(f\"Could not find new type with ID '{new_type_id}' for re-classification.\")\n",
        "                    tx.rollback(); return 0\n",
        "                new_type_name = new_type_record['name']\n",
        "\n",
        "                if old_type_node.get('id') == new_type_id or old_type_name == new_type_name:\n",
        "                    logging.debug(f\"Skipping re-classification for '{instance_display_name}': type is already '{new_type_name}'.\")\n",
        "                    tx.commit(); return 0\n",
        "\n",
        "                tx.run(\"MATCH ()-[r]->() WHERE elementId(r) = $rel_id DELETE r\", rel_id=old_rel.element_id)\n",
        "\n",
        "                # Task 132: Set creationStage for new relationship\n",
        "                tx.run(\n",
        "                    \"\"\"\n",
        "                    MATCH (i {id: $id})\n",
        "                    MATCH (t {id: $type_id})\n",
        "                    MERGE (i)-[r:IS_A]->(t)\n",
        "                    SET r.creationStage = $stage\n",
        "                    \"\"\",\n",
        "                    id=instance_id, type_id=new_type_id, stage=GraphSchema.STAGE_CONSOLIDATION\n",
        "                )\n",
        "                tx.run(\"MATCH (i {id: $id}) SET i.refinementCount = COALESCE(i.refinementCount, 0) + 1, i.lastRefined = timestamp()\", id=instance_id)\n",
        "\n",
        "                tx.commit()\n",
        "                logging.info(f\"Reclassified '{instance_display_name}' from '{old_type_name}({old_type_node.get('id')})' to '{new_type_name}({new_type_id})'.\")\n",
        "                return 1\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error reclassifying instance '{instance_id}': {e}\", exc_info=True)\n",
        "                tx.rollback()\n",
        "                return 0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4ROhrS1nU9Q"
      },
      "source": [
        "## Graph Database Reader Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 460,
      "metadata": {
        "id": "3UZHlLrFM_w7"
      },
      "outputs": [],
      "source": [
        "class GraphDBReader(GraphDBBase):\n",
        "    \"\"\"Handles all read and query operations from the Neo4j database.\"\"\"\n",
        "\n",
        "    def get_display_names_for_ids(self, ids: list[str]) -> dict[str, str]:\n",
        "        \"\"\"Fetches the display names for a given list of node IDs.\"\"\"\n",
        "        if not ids:\n",
        "            return {}\n",
        "        query = \"\"\"\n",
        "        UNWIND $ids AS node_id\n",
        "        MATCH (n {id: node_id})\n",
        "        RETURN n.id AS id, n.displayName AS displayName\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query, ids=ids)\n",
        "            return {record[\"id\"]: record[\"displayName\"] for record in result}\n",
        "\n",
        "    def get_ontology_graph_data(self, experiment_id: str, run_timestamp: str) -> list:\n",
        "        \"\"\"Fetches only the :Type nodes and their :IS_A relationships for ontology visualization.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        # This query finds all :Type nodes connected by an :IS_A relationship\n",
        "        # within the scope of the current experiment.\n",
        "        query = f\"\"\"\n",
        "            MATCH (n:Type{scoped_label})-[r:IS_A]->(m:Type{scoped_label})\n",
        "            RETURN n, r, m\n",
        "        \"\"\"\n",
        "        records, _, _ = self.driver.execute_query(query, database_=\"neo4j\")\n",
        "        return [(record[\"n\"], record[\"r\"], record[\"m\"]) for record in records]\n",
        "\n",
        "    def get_all_instance_display_names(self, experiment_id: str, run_timestamp: str) -> list[str]:\n",
        "        \"\"\"Fetches the display names of all instance nodes for an experiment.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"\"\"\n",
        "            MATCH (n:Instance{scoped_label})\n",
        "            WHERE NOT n:Source AND NOT n:Type\n",
        "            RETURN n.displayName AS displayName\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return [record[\"displayName\"] for record in result if record[\"displayName\"]]\n",
        "\n",
        "    def get_all_relationship_types(self, experiment_id: str, run_timestamp: str) -> list[str]:\n",
        "        \"\"\"Fetches all relationship types for an experiment, excluding IS_A and FROM.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"\"\"\n",
        "            MATCH (n{scoped_label})-[r]->(m{scoped_label})\n",
        "            WHERE NOT type(r) IN ['IS_A', 'FROM']\n",
        "            RETURN type(r) AS relType\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return [record[\"relType\"] for record in result if record[\"relType\"]]\n",
        "\n",
        "    def get_random_instances_with_types(self, num_entities: int, experiment_id: str, run_timestamp: str) -> list:\n",
        "        \"\"\"\n",
        "        Selects random instance nodes using Python's seeded random sampler for reproducible results.\n",
        "        First, it fetches all candidate nodes from the database, then samples them in Python.\n",
        "        \"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"\"\"\n",
        "            MATCH (i{scoped_label})-[:IS_A]->(t:Type)\n",
        "            WHERE NOT i:Type AND NOT i:Source AND i.name IS NOT NULL\n",
        "            RETURN i.id AS id, i.displayName AS name, t.name AS entityType\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            all_candidates = [record.data() for record in result]\n",
        "\n",
        "        if not all_candidates:\n",
        "            return []\n",
        "\n",
        "        sample_size = min(num_entities, len(all_candidates))\n",
        "        return random.sample(all_candidates, sample_size)\n",
        "\n",
        "    def get_discovered_entity_count(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Gets the count of discovered entities (instance nodes), excluding Source and Type nodes.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"MATCH (n{scoped_label}) WHERE NOT n:Source AND NOT n:Type RETURN count(n)\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return result.single()[0] or 0\n",
        "\n",
        "    def get_discovered_relationship_count(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Gets the count of discovered relationships, excluding 'FROM' relationships.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"MATCH (n{scoped_label})-[r]->(m{scoped_label}) WHERE type(r) <> 'FROM' RETURN count(r)\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return result.single()[0] or 0\n",
        "\n",
        "    # --- MODIFICATION START ---\n",
        "    # This method has been updated to count only UNIQUE entities and relationships to fix the recall > 100% bug.\n",
        "    def get_cypher_comparison_scores(self, experiment_id: str, run_timestamp: str, golden_exp_id: str, golden_run_ts: str) -> dict:\n",
        "        \"\"\"Compares an experiment's graph to the golden standard using direct Cypher queries that ensure uniqueness.\"\"\"\n",
        "        exp_scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        golden_scoped_label = self._get_scoped_label(golden_exp_id, golden_run_ts)\n",
        "\n",
        "        # Query to count total entities in the golden standard graph (excluding Type/Source nodes)\n",
        "        golden_entities_query = f\"\"\"\n",
        "            MATCH (g{golden_scoped_label})\n",
        "            WHERE NOT g:Source AND NOT g:Type\n",
        "            RETURN count(g) AS count\n",
        "        \"\"\"\n",
        "\n",
        "        # Query to count total relationships in the golden standard graph (excluding framework rels)\n",
        "        golden_rels_query = f\"\"\"\n",
        "            MATCH (g1{golden_scoped_label})-[r]->(g2{golden_scoped_label})\n",
        "            WHERE NOT type(r) IN ['IS_A', 'FROM']\n",
        "            RETURN count(r) AS count\n",
        "        \"\"\"\n",
        "\n",
        "        # --- FIX: Query to count total UNIQUE discovered entities by name ---\n",
        "        discovered_unique_entities_query = f\"\"\"\n",
        "            MATCH (exp_node{exp_scoped_label})\n",
        "            WHERE NOT exp_node:Source AND NOT exp_node:Type\n",
        "            RETURN count(DISTINCT exp_node.name) AS count\n",
        "        \"\"\"\n",
        "\n",
        "        # --- FIX: Query to count total UNIQUE discovered relationships ---\n",
        "        discovered_unique_rels_query = f\"\"\"\n",
        "            MATCH (exp_source{exp_scoped_label})-[exp_rel]->(exp_target{exp_scoped_label})\n",
        "            WHERE NOT type(exp_rel) IN ['IS_A', 'FROM']\n",
        "            RETURN count(DISTINCT {{source: exp_source.name, target: exp_target.name, type: type(exp_rel)}}) AS count\n",
        "        \"\"\"\n",
        "\n",
        "        # --- FIX: Query to count UNIQUE matched entities by name ---\n",
        "        matched_entities_query = f\"\"\"\n",
        "            MATCH (exp_node{exp_scoped_label})\n",
        "            WHERE NOT exp_node:Source AND NOT exp_node:Type\n",
        "            MATCH (golden_node{golden_scoped_label})\n",
        "            WHERE NOT golden_node:Source AND NOT golden_node:Type AND exp_node.name = golden_node.name\n",
        "            RETURN count(DISTINCT exp_node.name) AS count\n",
        "        \"\"\"\n",
        "\n",
        "        # --- FIX: Query to count UNIQUE matched relationships ---\n",
        "        matched_rels_query = f\"\"\"\n",
        "            MATCH (exp_source{exp_scoped_label})-[exp_rel]->(exp_target{exp_scoped_label})\n",
        "            WHERE NOT type(exp_rel) IN ['IS_A', 'FROM']\n",
        "            MATCH (golden_source{golden_scoped_label})-[golden_rel]->(golden_target{golden_scoped_label})\n",
        "            WHERE exp_source.name = golden_source.name\n",
        "              AND exp_target.name = golden_target.name\n",
        "              AND type(exp_rel) = type(golden_rel)\n",
        "            RETURN count(DISTINCT {{source: exp_source.name, target: exp_target.name, type: type(exp_rel)}}) AS count\n",
        "        \"\"\"\n",
        "\n",
        "        with self.driver.session() as session:\n",
        "            golden_entities_count = session.run(golden_entities_query).single()['count']\n",
        "            golden_rels_count = session.run(golden_rels_query).single()['count']\n",
        "            discovered_unique_entities_count = session.run(discovered_unique_entities_query).single()['count']\n",
        "            discovered_unique_rels_count = session.run(discovered_unique_rels_query).single()['count']\n",
        "            matched_entities_count = session.run(matched_entities_query).single()['count']\n",
        "            matched_rels_count = session.run(matched_rels_query).single()['count']\n",
        "\n",
        "        return {\n",
        "            \"ground_truth_entities\": golden_entities_count or 0,\n",
        "            \"matched_entities\": matched_entities_count or 0,\n",
        "            \"ground_truth_relationships\": golden_rels_count or 0,\n",
        "            \"matched_relationships\": matched_rels_count or 0,\n",
        "            \"total_discovered_unique_entities\": discovered_unique_entities_count or 0,\n",
        "            \"total_discovered_unique_relationships\": discovered_unique_rels_count or 0,\n",
        "        }\n",
        "    # --- MODIFICATION END ---\n",
        "\n",
        "    def select_random_entities(self, entity_type: str, num_entities: int, experiment_id: str, run_timestamp: str) -> list:\n",
        "        \"\"\"\n",
        "        Selects random entities using a two-step, seedable Python-based approach.\n",
        "        \"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "\n",
        "        where_clause = \"WHERE NOT n:DoNotChange AND n.name IS NOT NULL\"\n",
        "        if not entity_type:\n",
        "            where_clause += \" AND NOT n:Type AND NOT n:Source\"\n",
        "\n",
        "        match_clause = f\"MATCH (n{scoped_label})\"\n",
        "        if entity_type:\n",
        "            sanitized_entity_type = entity_type.replace('`', '``')\n",
        "            match_clause = f\"MATCH (n:`{sanitized_entity_type}`{scoped_label})\"\n",
        "\n",
        "        id_query = f\"\"\"\n",
        "            {match_clause}\n",
        "            {where_clause}\n",
        "            RETURN n.id AS id\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(id_query)\n",
        "            all_ids = [record[\"id\"] for record in result]\n",
        "\n",
        "        if not all_ids:\n",
        "            return []\n",
        "\n",
        "        sample_size = min(num_entities, len(all_ids))\n",
        "        sampled_ids = random.sample(all_ids, sample_size)\n",
        "\n",
        "        nodes_query = \"\"\"\n",
        "            MATCH (n)\n",
        "            WHERE n.id IN $ids\n",
        "            RETURN n\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(nodes_query, ids=sampled_ids)\n",
        "            return [record[\"n\"] for record in result]\n",
        "\n",
        "    def get_all_instance_nodes_for_experiment(self, experiment_id: str, run_timestamp: str) -> list:\n",
        "        \"\"\"Retrieves all instance nodes for a specific experiment run.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"MATCH (n{scoped_label}) WHERE NOT n:Type AND NOT n:Source AND n.name IS NOT NULL RETURN n\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return [record[\"n\"] for record in result]\n",
        "\n",
        "    def get_node_count(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Gets the total number of nodes for a specific experiment.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(f\"MATCH (n{scoped_label}) RETURN count(n)\")\n",
        "            return result.single()[0] or 0\n",
        "\n",
        "    def get_relationship_count(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Gets the total number of relationships for a specific experiment.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(f\"MATCH (n{scoped_label})-[r]->(m) RETURN count(r)\")\n",
        "            return result.single()[0] or 0\n",
        "\n",
        "    def get_property_count(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Gets the total count of all properties on nodes and relationships for an experiment.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        with self.driver.session() as session:\n",
        "            node_props_result = session.run(f\"MATCH (n{scoped_label}) UNWIND keys(n) AS key RETURN count(key)\")\n",
        "            rel_props_result = session.run(f\"MATCH (n{scoped_label})-[r]->() UNWIND keys(r) AS key RETURN count(key)\")\n",
        "            return (node_props_result.single()[0] or 0) + (rel_props_result.single()[0] or 0)\n",
        "\n",
        "    def get_root_node_count(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Gets the count of root nodes (no incoming relationships) for an experiment.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(f\"MATCH (n{scoped_label}) WHERE NOT ()-->(n) RETURN count(n)\")\n",
        "            return result.single()[0] or 0\n",
        "\n",
        "    def get_all_nodes_with_is_a_counts(self, experiment_id: str, run_timestamp: str) -> list:\n",
        "        \"\"\"Retrieves all nodes for an experiment with their incoming/outgoing IS_A relationship counts.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"\"\"\n",
        "            MATCH (n{scoped_label})\n",
        "            WHERE n.displayName IS NOT NULL\n",
        "            OPTIONAL MATCH (n)<-[:IS_A]-(child)\n",
        "            OPTIONAL MATCH (n)-[:IS_A]->(parent)\n",
        "            RETURN n.id AS NodeId, n.displayName AS NodeName, n.mergeCount as MergeCount,\n",
        "                   count(DISTINCT parent) AS ParentISACount, count(DISTINCT child) AS ChildISACount\n",
        "            ORDER BY NodeName\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return [record.data() for record in result]\n",
        "\n",
        "    def get_all_relationships(self, experiment_id: str, run_timestamp: str) -> list:\n",
        "        \"\"\"Retrieves all relationships for a specific experiment run.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"\"\"\n",
        "            MATCH (source{scoped_label})-[r]->(target)\n",
        "            RETURN source.displayName AS SourceNode, labels(source) AS SourceLabels,\n",
        "                   type(r) AS RelationshipType, target.displayName AS TargetNode, labels(target) AS TargetLabels\n",
        "            ORDER BY SourceNode, TargetNode, RelationshipType\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return [record.data() for record in result]\n",
        "\n",
        "    def get_graph_for_visualization(self, experiment_id: str, run_timestamp: str) -> list:\n",
        "        \"\"\"Fetches the core semantic graph for visualization (excluding 'FROM' relationships).\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"\"\"\n",
        "            MATCH (n)-[r]-(m)\n",
        "            WHERE (n{scoped_label} OR m{scoped_label}) AND type(r) <> 'FROM'\n",
        "            RETURN DISTINCT n, r, m\n",
        "        \"\"\"\n",
        "        records, _, _ = self.driver.execute_query(query, database_=\"neo4j\")\n",
        "        return [(record[\"n\"], record[\"r\"], record[\"m\"]) for record in records]\n",
        "\n",
        "    def get_all_nodes_and_relationships_for_test(self, experiment_id: str, run_timestamp: str) -> str:\n",
        "        \"\"\"Retrieves graph data formatted as a simple text block for LLM comparison.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        with self.driver.session() as session:\n",
        "            entity_query = f\"\"\"\n",
        "                MATCH (n{scoped_label}) WHERE NOT n:Type AND NOT n:Source\n",
        "                RETURN n.displayName AS name, labels(n) AS labels ORDER BY name\n",
        "            \"\"\"\n",
        "            rel_query = f\"\"\"\n",
        "                MATCH (n{scoped_label})-[r]->(m)\n",
        "                WHERE NOT n:Type AND NOT m:Type AND NOT type(r) IN ['IS_A', 'FROM']\n",
        "                RETURN n.displayName AS source, type(r) as type, m.displayName AS target\n",
        "                ORDER BY source, type, target\n",
        "            \"\"\"\n",
        "            entity_result = session.run(entity_query)\n",
        "            entity_map = {}\n",
        "            for record in entity_result:\n",
        "                primary_label = next((l for l in record[\"labels\"] if l not in ['Node', 'DoNotChange'] and not l.startswith(('EXP_', 'RUN_'))), 'Unknown')\n",
        "                entity_map.setdefault(primary_label, []).append(record[\"name\"])\n",
        "\n",
        "            output_lines = [\"Entities\"]\n",
        "            for label, names in sorted(entity_map.items()):\n",
        "                output_lines.append(f\"{label}: {', '.join(sorted(names))}\")\n",
        "\n",
        "            rel_result = session.run(rel_query)\n",
        "            output_lines.append(\"\\\\nRelationships\")\n",
        "            for record in rel_result:\n",
        "                output_lines.append(f\"{record['type']}: {record['source']} {record['type']} {record['target']}\")\n",
        "\n",
        "            return \"\\\\n\".join(output_lines)\n",
        "\n",
        "    def get_duplicate_name_count(self, experiment_id: str, run_timestamp: str) -> int:\n",
        "        \"\"\"Counts the number of nodes that share a name within an experiment.\"\"\"\n",
        "        scoped_label = self._get_scoped_label(experiment_id, run_timestamp)\n",
        "        query = f\"\"\"\n",
        "            MATCH (n{scoped_label}) WHERE NOT n:Type AND NOT n:Source\n",
        "            WITH n.name as name, count(n) as count\n",
        "            WHERE count > 1\n",
        "            RETURN sum(count) as totalDuplicates\n",
        "        \"\"\"\n",
        "        with self.driver.session() as session:\n",
        "            result = session.run(query)\n",
        "            return result.single()[0] or 0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZQ69ZQjO0T0Q"
      },
      "source": [
        "#LLM Classes\n",
        "\n",
        "This section centralizes all communication with the various Large Language Models (LLMs). The classes here are designed to create a unified interface that abstracts away the provider-specific details of APIs like Google's Gemini and OpenAI's GPT. This approach allows the core application to interact with any supported LLM without needing to manage the unique implementation of each one. The section also includes a robust set of custom exceptions to gracefully handle common API errors.\n",
        "\n",
        "* **`LLMService`**: This is the main gateway for all LLM interactions. It is responsible for initializing the selected model, sending prompts, and processing the responses. It incorporates key features for resilience, such as automatic retry logic for token limit errors and standardized handling for different LLM provider outputs.\n",
        "\n",
        "* **`OpenAIBatchProcessor`**: A specialized utility class that encapsulates the entire asynchronous workflow for OpenAI's Batch API. It manages the multi-step process of creating, uploading, monitoring, and retrieving results from batch jobs, simplifying the use of large-scale, non-real-time requests."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_YstBKblLUa"
      },
      "source": [
        "## LLM Service Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 461,
      "metadata": {
        "id": "QsInh4IFyXnl"
      },
      "outputs": [],
      "source": [
        "## LLM Service Class\n",
        "class LLMService:\n",
        "    \"\"\"\n",
        "    Handles all interactions with the Large Language Model.\n",
        "    \"\"\"\n",
        "    # Define model-specific maximum output token limits\n",
        "    MODEL_MAX_TOKENS = {\n",
        "    # OpenAI Models\n",
        "    \"gpt-4.1\": 8000,\n",
        "    \"gpt-4o\": 16384,\n",
        "    \"gpt-4-turbo\": 4096,\n",
        "    \"gpt-4\": 8192,\n",
        "    \"gpt-3.5-turbo\": 4096,\n",
        "\n",
        "    # Google Models\n",
        "    \"gemini-2.5-pro\": 65535,\n",
        "    \"gemini-1.5-pro\": 8192,\n",
        "    \"gemini-1.5-flash\": 8192,\n",
        "    \"gemini-1.0-pro\": 2048,\n",
        "    }\n",
        "\n",
        "    def __init__(self, llm, model_name, api_key, temperature, max_output_tokens):\n",
        "        \"\"\"\n",
        "        Initializes the LLMService with the specified provider and model.\n",
        "        \"\"\"\n",
        "        self.llm = llm\n",
        "        self.model_name = model_name\n",
        "        self.api_key = api_key\n",
        "        self.temperature = temperature\n",
        "        self.max_output_tokens = max_output_tokens\n",
        "        self.model = None\n",
        "\n",
        "        try:\n",
        "            match self.llm:\n",
        "                case \"Gemini\":\n",
        "                    genai.configure(api_key=self.api_key)\n",
        "                    try:\n",
        "                        self.model = genai.GenerativeModel(self.model_name)\n",
        "                    except Exception as e:\n",
        "                        raise GeminiInitializationError(f\"Error initializing Gemini model: {str(e)}.\") from e\n",
        "                case \"OpenAI\":\n",
        "                    try:\n",
        "                        self.model = OpenAI(api_key=self.api_key)\n",
        "                    except Exception as e:\n",
        "                        raise OpenAIInitializationError(f\"Error initializing OpenAI client: {str(e)}.\") from e\n",
        "                case _:\n",
        "                    raise ValueError(f\"Unknown LLM: {self.llm}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred during LLMService initialization: {e}\")\n",
        "            if isinstance(e, (GeminiInitializationError, OpenAIInitializationError, ValueError)):\n",
        "                raise\n",
        "            raise RuntimeError(f\"An unexpected error occurred during initialization: {e}\") from e\n",
        "\n",
        "    def prepare_batch_request(self, prompt: str, custom_id: str) -> dict:\n",
        "        \"\"\"\n",
        "        Prepares a single request dictionary for the OpenAI Batch API.\n",
        "        \"\"\"\n",
        "        if self.llm != \"OpenAI\":\n",
        "            raise ValueError(\"Batch processing is only supported for the OpenAI provider.\")\n",
        "\n",
        "        return {\n",
        "            \"custom_id\": custom_id,\n",
        "            \"method\": \"POST\",\n",
        "            \"url\": \"/v1/chat/completions\",\n",
        "            \"body\": {\n",
        "                \"model\": self.model_name,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"temperature\": self.temperature,\n",
        "                \"max_tokens\": self.max_output_tokens,\n",
        "                \"response_format\": {\"type\": \"json_object\"},\n",
        "            },\n",
        "        }\n",
        "\n",
        "    async def query_llm_async(self, prompt: str, chunk_num: int):\n",
        "        \"\"\"\n",
        "        Asynchronously sends a prepared prompt to the Gemini LLM and returns the result along with the chunk number.\n",
        "        \"\"\"\n",
        "        if self.llm != \"Gemini\":\n",
        "            raise NotImplementedError(\"Async querying is only implemented for Gemini.\")\n",
        "\n",
        "        start_time = datetime.now()\n",
        "        logging.info(f\"Submitting async query for chunk {chunk_num} ({len(prompt):,} characters).\")\n",
        "        response_text = \"\"\n",
        "\n",
        "        try:\n",
        "            generation_config = {\n",
        "                \"max_output_tokens\": self.max_output_tokens,\n",
        "                \"temperature\": self.temperature,\n",
        "                \"response_mime_type\": \"application/json\",\n",
        "            }\n",
        "            safety_settings = [\n",
        "                {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "            ]\n",
        "\n",
        "            response = await self.model.generate_content_async(\n",
        "                prompt, generation_config=generation_config, safety_settings=safety_settings\n",
        "            )\n",
        "\n",
        "            if not response.candidates:\n",
        "                block_reason = response.prompt_feedback.block_reason.name if response.prompt_feedback else \"UNKNOWN\"\n",
        "                raise GeminiResponseBlockedError(f\"Gemini response was blocked for chunk {chunk_num}. Reason: {block_reason}.\")\n",
        "\n",
        "            response_text = response.text\n",
        "            return chunk_num, response_text\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error during async query for chunk {chunk_num}: {e}\")\n",
        "            return chunk_num, None # Return None on failure to avoid breaking the batch\n",
        "        finally:\n",
        "            duration = (datetime.now() - start_time).total_seconds()\n",
        "            logging.info(f\"Async query for chunk {chunk_num} completed in {duration:.2f} seconds.\")\n",
        "\n",
        "\n",
        "    def query_llm(self, prompt):\n",
        "        \"\"\"\n",
        "        Sends a prepared prompt to the LLM, logs the execution time, and retries with more tokens if the response is truncated.\n",
        "        \"\"\"\n",
        "        #time.sleep(1) # Prevent rate limiting\n",
        "\n",
        "        if len(prompt) > MindConfig.MAX_VARIABLE_LENGTH:\n",
        "            raise ParagraphsTooLong(f\"Prompt is too long: {len(prompt):,} characters.\")\n",
        "\n",
        "        start_time = datetime.now()\n",
        "        logging.info(f\"Querying LLM ({self.model_name}) with prompt of {len(prompt):,} characters.\")\n",
        "        response_text = \"\"\n",
        "\n",
        "        try:\n",
        "            match self.llm:\n",
        "                case \"Gemini\":\n",
        "                    generation_config = {\n",
        "                        \"max_output_tokens\": self.max_output_tokens,\n",
        "                        \"temperature\": self.temperature,\n",
        "                        \"response_mime_type\": \"application/json\",\n",
        "                    }\n",
        "                    safety_settings = [\n",
        "                        {\"category\": \"HARM_CATEGORY_HARASSMENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                        {\"category\": \"HARM_CATEGORY_HATE_SPEECH\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                        {\"category\": \"HARM_CATEGORY_SEXUALLY_EXPLICIT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                        {\"category\": \"HARM_CATEGORY_DANGEROUS_CONTENT\", \"threshold\": \"BLOCK_MEDIUM_AND_ABOVE\"},\n",
        "                    ]\n",
        "                    try:\n",
        "                        response = self.model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)\n",
        "\n",
        "                        if not response.candidates:\n",
        "                            block_reason = response.prompt_feedback.block_reason.name if response.prompt_feedback else \"UNKNOWN\"\n",
        "                            error_msg = f\"Gemini response was blocked. Reason: {block_reason}. No candidates returned.\"\n",
        "                            logging.error(error_msg)\n",
        "                            raise GeminiResponseBlockedError(error_msg)\n",
        "\n",
        "                        candidate = response.candidates[0]\n",
        "                        if candidate.finish_reason.name not in (\"STOP\", \"MAX_TOKENS\"):\n",
        "                            error_msg = f\"Gemini response was blocked or incomplete. Finish Reason: {candidate.finish_reason.name}.\"\n",
        "                            logging.error(error_msg)\n",
        "                            raise GeminiResponseBlockedError(error_msg)\n",
        "\n",
        "                        logging.info(f\"Length of LLM response: {len(response.text):,}\")\n",
        "                        cleaned_log_output = ' '.join(response.text.split())\n",
        "                        logging.info(f\"A: LLM Response --{cleaned_log_output}--\")\n",
        "\n",
        "                        if response.candidates and response.candidates[0].finish_reason.name == \"MAX_TOKENS\":\n",
        "                            logging.warning(\"Gemini response was truncated. Retrying with 10% more tokens...\")\n",
        "                            new_max_tokens = int(self.max_output_tokens * 1.1)\n",
        "                            generation_config[\"max_output_tokens\"] = new_max_tokens\n",
        "\n",
        "                            response = self.model.generate_content(prompt, generation_config=generation_config, safety_settings=safety_settings)\n",
        "\n",
        "                            if not response.candidates or response.candidates[0].finish_reason.name not in (\"STOP\"):\n",
        "                                 error_msg = f\"Gemini retry failed. Finish Reason: {response.candidates[0].finish_reason.name if response.candidates else 'N/A'}.\"\n",
        "                                 logging.error(error_msg)\n",
        "                                 raise MaxTokensExceededError(error_msg)\n",
        "\n",
        "                            logging.info(f\"Length of LLM response: {len(response.text):,}\")\n",
        "                            cleaned_log_output = ' '.join(response.text.split())\n",
        "                            logging.info(f\"B: LLM Response --{cleaned_log_output}--\")\n",
        "                            logging.info(\"Retry with increased tokens was successful.\")\n",
        "\n",
        "                        response_text = response.text\n",
        "                    except GeminiResponseBlockedError:\n",
        "                        raise\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error querying Gemini API: {e}\")\n",
        "                        raise GeminiQueryError(f\"Error querying Gemini API: {str(e)}\") from e\n",
        "\n",
        "                case \"OpenAI\":\n",
        "                    try:\n",
        "                        response = self.model.chat.completions.create(\n",
        "                            model=self.model_name,\n",
        "                            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                            temperature=self.temperature,\n",
        "                            max_tokens=self.max_output_tokens,\n",
        "                            response_format={\"type\": \"json_object\"},\n",
        "                        )\n",
        "\n",
        "                        ### CHANGE START ###\n",
        "                        # Access the content correctly for OpenAI API v1.0+\n",
        "                        response_content = response.choices[0].message.content\n",
        "                        logging.info(f\"Length of LLM response: {len(response_content):,}\")\n",
        "                        cleaned_log_output = ' '.join(response_content.split())\n",
        "                        ### CHANGE END ###\n",
        "\n",
        "                        logging.info(f\"C: LLM Response --{cleaned_log_output}--\")\n",
        "\n",
        "                        if response.choices[0].finish_reason == \"length\":\n",
        "                            model_limit = self.MODEL_MAX_TOKENS.get(self.model_name, self.max_output_tokens)\n",
        "                            logging.info(response.choices[0].message.content.strip())\n",
        "\n",
        "                            if self.max_output_tokens >= model_limit:\n",
        "                                error_msg = f\"LLM response was truncated even at the model's maximum token limit of {model_limit}. The response is too large to process.\"\n",
        "                                logging.error(error_msg)\n",
        "                                raise MaxTokensExceededError(error_msg)\n",
        "\n",
        "                            logging.warning(\"OpenAI response was truncated. Retrying with more tokens...\")\n",
        "                            new_max_tokens = min(int(self.max_output_tokens * 1.2), model_limit)\n",
        "                            logging.info(f\"Increasing max_output_tokens from {self.max_output_tokens} to {new_max_tokens}.\")\n",
        "\n",
        "                            response = self.model.chat.completions.create(\n",
        "                                model=self.model_name,\n",
        "                                messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "                                temperature=self.temperature,\n",
        "                                max_tokens=new_max_tokens,\n",
        "                                response_format={\"type\": \"json_object\"}\n",
        "                            )\n",
        "\n",
        "                            ### CHANGE START ###\n",
        "                            # Access the content correctly for OpenAI API v1.0+ on retry\n",
        "                            response_content_retry = response.choices[0].message.content\n",
        "                            logging.info(f\"Length of LLM response on retry: {len(response_content_retry):,}\")\n",
        "                            cleaned_log_output_retry = ' '.join(response_content_retry.split())\n",
        "                            logging.info(f\"D: LLM Response --{cleaned_log_output_retry}--\")\n",
        "                            ### CHANGE END ###\n",
        "\n",
        "                            if response.choices[0].finish_reason == \"length\":\n",
        "                                error_msg = \"LLM call failed: The response was truncated even after increasing the token limit.\"\n",
        "                                logging.error(error_msg)\n",
        "                                raise MaxTokensExceededError(error_msg)\n",
        "\n",
        "                            logging.info(\"Retry with increased tokens was successful.\")\n",
        "                            response_content = response_content_retry\n",
        "\n",
        "                        response_text = response_content.strip()\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error querying OpenAI API: {e}\")\n",
        "                        raise OpenAIQueryError(f\"Error querying OpenAI API: {str(e)}\") from e\n",
        "                case _:\n",
        "                    raise ValueError(f\"Unknown LLM: {self.llm}\")\n",
        "        finally:\n",
        "            end_time = datetime.now()\n",
        "            duration = end_time - start_time\n",
        "            total_seconds = duration.total_seconds()\n",
        "            hours, remainder = divmod(total_seconds, 3600)\n",
        "            minutes, seconds = divmod(remainder, 60)\n",
        "            milliseconds = duration.microseconds // 1000\n",
        "            formatted_duration = f\"{int(hours):02}:{int(minutes):02}:{int(seconds):02}.{milliseconds:03}\"\n",
        "            logging.info(f\"LLM call duration: {formatted_duration}\")\n",
        "\n",
        "        return response_text\n",
        "\n",
        "    def query_llm_batch(self, requests: list[dict], output_file_path: Path):\n",
        "        \"\"\"\n",
        "        Submits a batch of requests to the OpenAI API and retrieves the results.\n",
        "        \"\"\"\n",
        "        raise NotImplementedError(\"This method is not yet implemented. Use the BatchProcessor class.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYvhuPw3unDf"
      },
      "source": [
        "## OpenAI Batch Processor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 462,
      "metadata": {
        "id": "yRVUUVAjunrK"
      },
      "outputs": [],
      "source": [
        "class OpenAIBatchProcessor:\n",
        "    \"\"\"\n",
        "    Handles the creation, execution, and result retrieval of OpenAI batch jobs.\n",
        "    \"\"\"\n",
        "    def __init__(self, openai_client: OpenAI, output_path: Path):\n",
        "        self.client = openai_client\n",
        "        self.output_path = output_path\n",
        "        self.output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def process_batch(self, requests: list[dict], batch_file_name: str) -> Path:\n",
        "        \"\"\"\n",
        "        Orchestrates the entire batch processing workflow.\n",
        "        \"\"\"\n",
        "        input_file_path = self._create_batch_input_file(requests, batch_file_name)\n",
        "        uploaded_file = self._upload_batch_file(input_file_path)\n",
        "        batch_job = self._create_batch_job(uploaded_file.id)\n",
        "        completed_job = self._monitor_batch_job(batch_job.id)\n",
        "        return self._download_batch_results(completed_job.output_file_id, f\"{batch_file_name}_results.jsonl\")\n",
        "\n",
        "    def _create_batch_input_file(self, requests: list[dict], file_name: str) -> Path:\n",
        "        \"\"\"\n",
        "        Creates a JSONL file from a list of request dictionaries.\n",
        "        \"\"\"\n",
        "        file_path = self.output_path / file_name\n",
        "        with open(file_path, 'w') as f:\n",
        "            for request in requests:\n",
        "                f.write(json.dumps(request) + '\\n')\n",
        "        logging.info(f\"Batch input file created at: {file_path}\")\n",
        "        return file_path\n",
        "\n",
        "    def _upload_batch_file(self, file_path: Path) -> any:\n",
        "        \"\"\"\n",
        "        Uploads the batch input file to OpenAI.\n",
        "        \"\"\"\n",
        "        with open(file_path, 'rb') as f:\n",
        "            uploaded_file = self.client.files.create(file=f, purpose=\"batch\")\n",
        "        logging.info(f\"File {file_path.name} uploaded with ID: {uploaded_file.id}\")\n",
        "        return uploaded_file\n",
        "\n",
        "    def _create_batch_job(self, file_id: str) -> any:\n",
        "        \"\"\"\n",
        "        Creates a new batch job from an uploaded file.\n",
        "        \"\"\"\n",
        "        batch_job = self.client.batches.create(\n",
        "            input_file_id=file_id,\n",
        "            endpoint=\"/v1/chat/completions\",\n",
        "            completion_window=\"24h\"\n",
        "        )\n",
        "        logging.info(f\"Batch job created with ID: {batch_job.id}\")\n",
        "        return batch_job\n",
        "\n",
        "    def _monitor_batch_job(self, batch_id: str) -> any:\n",
        "        \"\"\"\n",
        "        Monitors the status of the batch job until it is completed.\n",
        "        \"\"\"\n",
        "        logging.info(f\"Monitoring batch job {batch_id}...\")\n",
        "        while True:\n",
        "            batch_job = self.client.batches.retrieve(batch_id)\n",
        "            if batch_job.status == 'completed':\n",
        "                logging.info(f\"Batch job {batch_id} completed successfully.\")\n",
        "                return batch_job\n",
        "            elif batch_job.status in ['failed', 'expired', 'cancelled']:\n",
        "                logging.error(f\"Batch job {batch_id} ended with status: {batch_job.status}\")\n",
        "                raise Exception(f\"Batch job {batch_id} failed.\")\n",
        "            time.sleep(30) # Poll every 30 seconds\n",
        "\n",
        "    def _download_batch_results(self, file_id: str, output_file_name: str) -> Path:\n",
        "        \"\"\"\n",
        "        Downloads the results of a completed batch job.\n",
        "        \"\"\"\n",
        "        result_content = self.client.files.content(file_id).content\n",
        "        output_file_path = self.output_path / output_file_name\n",
        "        with open(output_file_path, 'wb') as f:\n",
        "            f.write(result_content)\n",
        "        logging.info(f\"Batch results downloaded to: {output_file_path}\")\n",
        "        return output_file_path"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fd2pIjp4fLpX"
      },
      "source": [
        "# Document Classes\n",
        "This section is dedicated to the handling of source documents. It defines the Document class, a utility designed to load, parse, and iterate through .docx files. Its primary function is to break down a large document into smaller, manageable chunks of text that can be fed to the language model without exceeding token limits. The class includes methods for controlling the chunk size and the overlap between chunks. Also included in this section is the TestDocument class, which contains a suite of unit tests to verify the functionality and reliability of the document processing logic, ensuring that documents are read and chunked correctly."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "McN6gYcbr_HO"
      },
      "source": [
        "## Document Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 463,
      "metadata": {
        "id": "HrKlx_qve7_c"
      },
      "outputs": [],
      "source": [
        "# --- Document Class ---\n",
        "class Document:\n",
        "    \"\"\"\n",
        "    Handles loading, parsing, and iterating over a .docx document.\n",
        "    Supports the 'with' statement for context management.\n",
        "\n",
        "    Attributes:\n",
        "        doc_path (Path): The path to the .docx document.\n",
        "        doc (docx.Document): The loaded document object.\n",
        "        num_paragraphs (int): The total number of paragraphs in the document.\n",
        "        firstParagraphSet (int): Number of paragraphs in the first chunk.\n",
        "        remainingParagraphSet (int): Number of paragraphs in subsequent chunks.\n",
        "        overlap (int): Number of overlapping paragraphs between chunks.\n",
        "        iteration (int): Current iteration count (chunk number).\n",
        "        current_paragraph (int): Index of the starting paragraph for the current chunk.\n",
        "        currentParagraphSet (int): Number of paragraphs in the current chunk.\n",
        "    \"\"\"\n",
        "    def __init__(self, path, firstParagraphSet=50, remainingParagraphSet=50, overlap=0):\n",
        "        \"\"\"\n",
        "        Initializes the Document object.\n",
        "\n",
        "        Args:\n",
        "            path (str or Path): The path to the .docx document.\n",
        "            firstParagraphSet (int): The number of paragraphs in the first chunk. Defaults to 50.\n",
        "            remainingParagraphSet (int): The number of paragraphs in subsequent chunks. Defaults to 50.\n",
        "            overlap (int): The number of paragraphs to overlap between chunks. Defaults to 0.\n",
        "\n",
        "        Raises:\n",
        "            ValueError: If there is an error opening the document.\n",
        "        \"\"\"\n",
        "        self.doc_path = Path(path) # Ensure path is a Path object\n",
        "        try:\n",
        "            self.doc = docx.Document(self.doc_path)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error opening document at {self.doc_path}: {e}\")\n",
        "            raise ValueError(f\"Error opening document at {self.doc_path}: {e}\") from e\n",
        "        self.num_paragraphs = len(self.doc.paragraphs)\n",
        "        self.restart(firstParagraphSet, remainingParagraphSet, overlap)\n",
        "        logging.info(f\"Document initialized for: {self.doc_path}\")\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Returns the iterator object.\"\"\"\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"\n",
        "        Retrieves the next chunk of paragraphs from the document.\n",
        "\n",
        "        Returns:\n",
        "            tuple: A tuple containing the concatenated text of the paragraph set (str),\n",
        "                   the starting paragraph index (int), and the ending paragraph index (int) (exclusive).\n",
        "\n",
        "        Raises:\n",
        "            StopIteration: When all paragraphs have been processed.\n",
        "        \"\"\"\n",
        "        # Stop iteration if the overlap extends beyond the last paragraph\n",
        "        if self.current_paragraph + self.overlap >= self.num_paragraphs and self.iteration > 0:\n",
        "             # If it's not the first iteration and overlap goes beyond, stop.\n",
        "             # Handle the case where overlap might mean the last chunk was already small enough.\n",
        "             if self.current_paragraph >= self.num_paragraphs:\n",
        "                  raise StopIteration\n",
        "        elif self.current_paragraph >= self.num_paragraphs and self.iteration == 0:\n",
        "             # Handle empty document case\n",
        "             raise StopIteration\n",
        "        elif self.current_paragraph >= self.num_paragraphs and self.iteration > 0:\n",
        "             # Catch-all for when current_paragraph exceeds total after some iterations\n",
        "              raise StopIteration\n",
        "\n",
        "\n",
        "        start_paragraph = self.current_paragraph\n",
        "        # Calculate the end paragraph, ensuring it doesn't exceed the total number of paragraphs\n",
        "        end_paragraph = min(self.current_paragraph + self.currentParagraphSet, self.num_paragraphs)\n",
        "        # Extract text from the selected range of paragraphs\n",
        "        try:\n",
        "            extracted_text = [self.doc.paragraphs[i].text for i in range(start_paragraph, end_paragraph)]\n",
        "            retval = '\\n'.join(extracted_text)\n",
        "        except IndexError as e:\n",
        "             logging.error(f\"Index error while extracting paragraphs {start_paragraph} to {end_paragraph}: {e}\")\n",
        "             # This indicates an issue with the paragraph indexing logic. Re-raise.\n",
        "             raise\n",
        "        except Exception as e:\n",
        "             logging.error(f\"An unexpected error occurred while extracting paragraphs {start_paragraph} to {end_paragraph}: {e}\")\n",
        "             # Re-raise any other unexpected exceptions\n",
        "             raise\n",
        "\n",
        "\n",
        "        # Calculate the starting paragraph for the next iteration, considering overlap\n",
        "        # This needs careful handling for the last chunk to avoid infinite loops or missing the end\n",
        "        self.current_paragraph = (end_paragraph) - self.overlap\n",
        "        # Ensure current_paragraph does not go below 0, although with typical usage it won't\n",
        "        self.current_paragraph = max(0, self.current_paragraph)\n",
        "\n",
        "\n",
        "        # After the first iteration, switch to the remainingParagraphSet size\n",
        "        if self.iteration == 0:\n",
        "            self.currentParagraphSet = self.remainingParagraphSet\n",
        "        self.iteration += 1\n",
        "\n",
        "        logging.debug(f\"Returning chunk (Iteration {self.iteration}): From paragraph {start_paragraph} to {end_paragraph}, received {len(retval)} characters.\")\n",
        "        #logging.debug(f\"Extracted text: {retval}\") # Avoid logging large text chunks in debug\n",
        "\n",
        "        # Check if this was the last possible chunk based on the end_paragraph calculation\n",
        "        if end_paragraph >= self.num_paragraphs:\n",
        "             # If the end of this chunk reached or passed the total paragraphs, the next call should stop.\n",
        "             # Set current_paragraph to num_paragraphs or more to ensure StopIteration on next call.\n",
        "             self.current_paragraph = self.num_paragraphs + 1 # Ensure next __next__ call raises StopIteration\n",
        "\n",
        "        return retval, start_paragraph, end_paragraph\n",
        "\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Returns a string representation of the Document object.\"\"\"\n",
        "        return f\"Document Path: {self.doc_path}\\n  Total Paragraphs: {self.num_paragraphs}\"\n",
        "\n",
        "    def __enter__(self):\n",
        "        \"\"\"Enter the runtime context related to this object.\"\"\"\n",
        "        logging.info(f\"Opening document context for {self.get_file_name()}\")\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
        "        \"\"\"Exit the runtime context and ensure resources are closed.\"\"\"\n",
        "        logging.info(f\"Closing document context for {self.get_file_name()}\")\n",
        "        # Return False to propagate exceptions\n",
        "        return False\n",
        "\n",
        "    def restart(self, firstParagraphSet=50, remainingParagraphSet=50, overlap=0):\n",
        "        \"\"\"\n",
        "        Resets the document iterator to the beginning with potentially new chunking parameters.\n",
        "\n",
        "        Args:\n",
        "            firstParagraphSet (int): The number of paragraphs in the first chunk after restart. Defaults to 50.\n",
        "            remainingParagraphSet (int): The number of paragraphs in subsequent chunks after restart. Defaults to 50.\n",
        "            overlap (int): The number of paragraphs to overlap between chunks after restart. Defaults to 0.\n",
        "        \"\"\"\n",
        "        self.firstParagraphSet = firstParagraphSet\n",
        "        self.remainingParagraphSet = remainingParagraphSet\n",
        "        self.overlap = overlap\n",
        "        self.iteration = 0\n",
        "        self.current_paragraph = 0\n",
        "        self.currentParagraphSet = self.firstParagraphSet\n",
        "\n",
        "    def get_iteration(self):\n",
        "        \"\"\"\n",
        "        Gets the current iteration (chunk) number.\n",
        "\n",
        "        Returns:\n",
        "            int: The current iteration number, starting from 0.\n",
        "        \"\"\"\n",
        "        return self.iteration\n",
        "\n",
        "    def get_file_name(self):\n",
        "        \"\"\"\n",
        "        Gets the base name of the document file.\n",
        "\n",
        "        Returns:\n",
        "            str: The filename.\n",
        "        \"\"\"\n",
        "        return self.doc_path.name\n",
        "\n",
        "    def get_num_paragraphs(self):\n",
        "        \"\"\"\n",
        "        Gets the total number of paragraphs in the document.\n",
        "\n",
        "        Returns:\n",
        "            int: The total number of paragraphs.\n",
        "        \"\"\"\n",
        "        return self.num_paragraphs\n",
        "\n",
        "    def get_all_text(self):\n",
        "        \"\"\"\n",
        "        Concatenates and returns the text from all paragraphs in the document.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return '\\n'.join([p.text for p in self.doc.paragraphs])\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error getting all text from document: {e}\")\n",
        "            return \"\"\n",
        "\n",
        "    def get_word_count(self):\n",
        "        \"\"\"\n",
        "        Calculates the total word count in the document.\n",
        "\n",
        "        Returns:\n",
        "            int: The total number of words.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return sum(len(p.text.split()) for p in self.doc.paragraphs)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error calculating word count: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def get_word_frequencies(self):\n",
        "        \"\"\"\n",
        "        Calculates the frequency of each word in the document.\n",
        "\n",
        "        Returns:\n",
        "            collections.Counter: A Counter object mapping words to their frequencies.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            full_text = self.get_all_text()\n",
        "            # Normalize text: lowercase and find all word sequences\n",
        "            words = re.findall(r'\\b\\w+\\b', full_text.lower())\n",
        "            return Counter(words)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error calculating word frequencies: {e}\")\n",
        "            return Counter()\n",
        "\n",
        "    def get_unique_word_count(self):\n",
        "        \"\"\"Calculates the number of unique words in the document.\"\"\"\n",
        "        return len(self.get_word_frequencies())\n",
        "\n",
        "    def get_chapter_count(self):\n",
        "        \"\"\"\n",
        "        Infers the number of chapters by counting paragraphs that start with 'Chapter'.\n",
        "        This is an estimation and may need adjustment based on document format.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return sum(1 for p in self.doc.paragraphs if p.text.strip().lower().startswith('chapter'))\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error calculating chapter count: {e}\")\n",
        "            return 0\n",
        "\n",
        "    def get_page_count(self):\n",
        "        \"\"\"\n",
        "        Returns 'N/A' as page count cannot be determined accurately by python-docx.\n",
        "        Pagination is handled by rendering applications like MS Word.\n",
        "        \"\"\"\n",
        "        return \"N/A\"\n",
        "\n",
        "    def search_keyword(self, keyword):\n",
        "        \"\"\"\n",
        "        Finds paragraphs containing a specific keyword.\n",
        "\n",
        "        Args:\n",
        "            keyword (str): The keyword to search for.\n",
        "\n",
        "        Returns:\n",
        "            list: A list of strings, where each string is a paragraph containing the keyword.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return [p.text for p in self.doc.paragraphs if keyword in p.text]\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error searching for keyword '{keyword}': {e}\")\n",
        "            return []\n",
        "\n",
        "\n",
        "    def get_paragraph(self, index):\n",
        "        \"\"\"\n",
        "        Retrieves the text of a specific paragraph by index.\n",
        "\n",
        "        Args:\n",
        "            index (int): The index of the paragraph (0-based).\n",
        "\n",
        "        Returns:\n",
        "            str: The text of the specified paragraph.\n",
        "\n",
        "        Raises:\n",
        "            IndexError: If the paragraph index is out of range.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if not 0 <= index < self.num_paragraphs:\n",
        "                raise IndexError(\"Paragraph index out of range.\")\n",
        "            return self.doc.paragraphs[index].text\n",
        "        except IndexError as e:\n",
        "             logging.error(f\"IndexError getting paragraph {index}: {e}\")\n",
        "             raise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred getting paragraph {index}: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def save(self, path=None):\n",
        "        \"\"\"\n",
        "        Saves the document to a specified path or overwrites the original.\n",
        "\n",
        "        Args:\n",
        "            path (str or Path, optional): The path to save the document.\n",
        "                                           If None, overwrites the original file.\n",
        "                                           Defaults to None.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            save_path = Path(path) if path else self.doc_path\n",
        "            self.doc.save(save_path)\n",
        "            logging.info(f\"Document saved to {save_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving document to {path or self.doc_path}: {e}\")\n",
        "            raise\n",
        "\n",
        "\n",
        "    def get_statistics(self):\n",
        "        \"\"\"\n",
        "        Gets basic statistics about the document.\n",
        "\n",
        "        Returns:\n",
        "            dict: A dictionary containing \"Total Paragraphs\" and \"Total Words\".\n",
        "        \"\"\"\n",
        "        try:\n",
        "            return {\n",
        "                \"Total Paragraphs\": self.num_paragraphs,\n",
        "                \"Total Words\": self.get_word_count(),\n",
        "            }\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error getting document statistics: {e}\")\n",
        "            return {\"Total Paragraphs\": 0, \"Total Words\": 0}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oeqLmHMcfRJv"
      },
      "source": [
        "## Document Unit Test Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 464,
      "metadata": {
        "id": "NfgPOjFFhpE-"
      },
      "outputs": [],
      "source": [
        "# --- Unit Test Class ---\n",
        "class TestDocument(unittest.TestCase):\n",
        "    \"\"\"\n",
        "    Unit tests for the Document class.\n",
        "    \"\"\"\n",
        "\n",
        "    def setUp(self):\n",
        "        \"\"\"\n",
        "        Set up a temporary document for each test.\n",
        "        This method is called before each test method.\n",
        "        \"\"\"\n",
        "        self.test_file_path = Path(\"test_document.docx\")\n",
        "        doc = docx.Document()\n",
        "        # Create a document with 100 paragraphs.\n",
        "        # Each paragraph has 6 words from the initial string plus 5 \"word\"s, totaling 11 words.\n",
        "        for i in range(100):\n",
        "            doc.add_paragraph(f\"This is test paragraph number {i}. \" + \" \".join([\"word\"] * 5))\n",
        "        try:\n",
        "            doc.save(self.test_file_path)\n",
        "            logging.info(f\"Created temporary document: {self.test_file_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error creating temporary document in setUp: {e}\")\n",
        "            # Re-raise to fail the test setup\n",
        "            raise\n",
        "\n",
        "\n",
        "    def tearDown(self):\n",
        "        \"\"\"\n",
        "        Remove the temporary document after each test.\n",
        "        This method is called after each test method.\n",
        "        \"\"\"\n",
        "        try:\n",
        "            if os.path.exists(self.test_file_path):\n",
        "                os.remove(self.test_file_path)\n",
        "                logging.info(f\"Removed temporary document: {self.test_file_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error removing temporary document in tearDown: {e}\")\n",
        "            # Log the error but don't re-raise, as tearDown should clean up as much as possible\n",
        "            pass\n",
        "\n",
        "\n",
        "    def test_initial_iteration(self):\n",
        "        \"\"\"\n",
        "        Test the initial iteration of the Document class with specific chunk and overlap sizes.\n",
        "        Verifies the number of words and the paragraph ranges returned by the iterator.\n",
        "        \"\"\"\n",
        "        logging.info(\"\\n--- Running test_initial_iteration ---\")\n",
        "        try:\n",
        "            with Document(self.test_file_path, firstParagraphSet=80, remainingParagraphSet=80, overlap=10) as a_document:\n",
        "                # The test setup creates 100 paragraphs with 11 words each.\n",
        "                # Corrected assertion to check for 1100 words.\n",
        "                self.assertEqual(a_document.get_word_count(), 1100)\n",
        "\n",
        "                chunk_count = 0\n",
        "                for paragraphSet, start_paragraph, end_paragraph in a_document:\n",
        "                    logging.info(f\"Chunk {chunk_count}: From paragraph {start_paragraph} to {end_paragraph}, received {len(paragraphSet)} characters.\")\n",
        "                    self.assertIsNotNone(paragraphSet)\n",
        "                    if chunk_count == 0:\n",
        "                        self.assertEqual(start_paragraph, 0)\n",
        "                        self.assertEqual(end_paragraph, 80)\n",
        "                    elif chunk_count == 1:\n",
        "                        # Expected start is 80 (end of first chunk) - 10 (overlap) = 70\n",
        "                        self.assertEqual(start_paragraph, 70)\n",
        "                        # End is 70 + 80 = 150, but capped at 100 paragraphs total\n",
        "                        self.assertEqual(end_paragraph, 100)\n",
        "                    chunk_count += 1\n",
        "                # Expect 2 chunks: (0-79) and (70-99)\n",
        "                self.assertEqual(chunk_count, 2)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred during test_initial_iteration: {e}\")\n",
        "            # Re-raise to indicate test failure\n",
        "            raise\n",
        "        finally:\n",
        "            logging.info(\"--- Finished test_initial_iteration ---\")\n",
        "\n",
        "    def test_restart_functionality(self):\n",
        "        \"\"\"\n",
        "        Test the restart functionality of the Document class with different chunk sizes.\n",
        "        Verifies that restarting resets the iterator and applies the new chunking parameters.\n",
        "        \"\"\"\n",
        "        logging.info(\"\\n--- Running test_restart_functionality ---\")\n",
        "        try:\n",
        "            with Document(self.test_file_path, firstParagraphSet=80, remainingParagraphSet=80, overlap=10) as a_document:\n",
        "                # Iterate once to move the internal pointer\n",
        "                try:\n",
        "                    next(a_document)\n",
        "                except StopIteration:\n",
        "                    # Handle cases where the document might be too short for a second iteration\n",
        "                    pass\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Error during first iteration in test_restart_functionality: {e}\")\n",
        "                    # Re-raise to indicate test failure\n",
        "                    raise\n",
        "\n",
        "\n",
        "                # Now, restart with new parameters\n",
        "                a_document.restart(firstParagraphSet=40, remainingParagraphSet=40, overlap=5)\n",
        "                self.assertEqual(a_document.current_paragraph, 0)\n",
        "                self.assertEqual(a_document.currentParagraphSet, 40)\n",
        "\n",
        "                chunk_count = 0\n",
        "                for paragraphSet, start_paragraph, end_paragraph in a_document:\n",
        "                    logging.info(f\"Chunk {chunk_count}: From paragraph {start_paragraph} to {end_paragraph}, received {len(paragraphSet)} characters.\")\n",
        "                    self.assertIsNotNone(paragraphSet)\n",
        "                    if chunk_count == 0:\n",
        "                        self.assertEqual(start_paragraph, 0)\n",
        "                        self.assertEqual(end_paragraph, 40)\n",
        "                    elif chunk_count == 1:\n",
        "                        self.assertEqual(start_paragraph, 35) # 40 - 5\n",
        "                        self.assertEqual(end_paragraph, 75) # 35 + 40\n",
        "                    elif chunk_count == 2:\n",
        "                        self.assertEqual(start_paragraph, 70) # 75 - 5\n",
        "                        self.assertEqual(end_paragraph, 100) # 70 + 40, capped at 100\n",
        "                    chunk_count += 1\n",
        "                # Expect 3 chunks: (0-39), (35-74), (70-99)\n",
        "                self.assertEqual(chunk_count, 3)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred during test_restart_functionality: {e}\")\n",
        "            # Re-raise to indicate test failure\n",
        "            raise\n",
        "        finally:\n",
        "            logging.info(\"--- Finished test_restart_functionality ---\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb7BLESvkkg3"
      },
      "source": [
        "# Mind Classes\n",
        "\n",
        "This section contains the core object-oriented components that form the brain of the Mnemosyne system. It is organized into several key classes, each with a distinct responsibility. The MindConfig class centralizes all static configurations and prompt templates. The LLMService class abstracts interactions with the various Large Language Models. The GraphDBManager is responsible for all communication with the Neo4j database. The MemoryManager orchestrates the flow of data from short-term to long-term memory. Finally, the Mind class acts as the main orchestrator, coordinating the activities of all other classes to execute the end-to-end knowledge extraction and consolidation process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DEtUcpD66qF7"
      },
      "source": [
        "## Mind Configuration Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 465,
      "metadata": {
        "id": "jqOvCj8F1r33"
      },
      "outputs": [],
      "source": [
        "# --- Configuration Class ---\n",
        "class MindConfig:\n",
        "    \"\"\"\n",
        "    Holds all static configuration, constants, and prompt templates for the Mind.\n",
        "    \"\"\"\n",
        "    INDENT = 4\n",
        "    MAX_VARIABLE_LENGTH = 2 ** 16 # Maximum length for a variable storing prompt/response\n",
        "    STM_MAX = 10000 # Short-Term Memory maximum size (in characters)\n",
        "    DECL_STMT = \"Declarative\" # Type for declarative statements in STM\n",
        "    INTER_STMT = \"Interrogative\" # Type for interrogative statements in STM\n",
        "\n",
        "    # --- MODIFICATION START --\n",
        "    # NEW: Added a processing stage for LangExtract\n",
        "    STAGE_DOC_TO_STM = \"D2STM\"\n",
        "    STAGE_DOC_TO_LTM = \"D2STM2LTM\"\n",
        "    STAGE_FULL_PROCESS = \"D2STM2LTMCON\"\n",
        "    STAGE_CONSOLIDATE_ONLY = \"CON\"\n",
        "    STAGE_LANGEXTRACT = \"LE\" # LangExtract ground truth generation\n",
        "    PROCESSING_STAGES = [STAGE_DOC_TO_STM, STAGE_DOC_TO_LTM, STAGE_FULL_PROCESS, STAGE_CONSOLIDATE_ONLY, STAGE_LANGEXTRACT]\n",
        "    # --- MODIFICATION END --\n",
        "\n",
        "    # JSON schema for the knowledge graph output from the LLM\n",
        "    JSON_SCHEMA = '''\n",
        "              {\n",
        "                \"$schema\": \"https://json-schema.org/draft/2020-12/schema\",\n",
        "                \"$id\": \"https://example.com/knowledge_graph.schema.json\",\n",
        "                \"title\": \"Knowledge Graph for Neo4j Import\",\n",
        "                \"description\": \"A JSON structure representing a knowledge graph with nodes and relationships decoupled for efficient loading into Neo4j.\",\n",
        "                \"type\": \"object\",\n",
        "                \"properties\": {\n",
        "                  \"nodes\": {\n",
        "                    \"description\": \"A list of all unique entities (nodes) in the graph.\",\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                      \"type\": \"object\",\n",
        "                      \"properties\": {\n",
        "                        \"id\": {\n",
        "                          \"description\": \"A unique identifier for the node within this JSON document.\",\n",
        "                          \"type\": \"string\"\n",
        "                        },\n",
        "                        \"labels\": {\n",
        "                          \"description\": \"An array of labels for the node, corresponding to Neo4j labels (e.g., ['Person', 'Author']).\",\n",
        "                          \"type\": \"array\",\n",
        "                          \"items\": {\n",
        "                            \"type\": \"string\"\n",
        "                          },\n",
        "                          \"minItems\": 1\n",
        "                        },\n",
        "                        \"properties\": {\n",
        "                          \"description\": \"A key-value map of the node's properties.\",\n",
        "                          \"type\": \"object\",\n",
        "                          \"additionalProperties\": true\n",
        "                        }\n",
        "                      },\n",
        "                      \"required\": [\"id\", \"labels\", \"properties\"]\n",
        "                    }\n",
        "                  },\n",
        "                  \"relationships\": {\n",
        "                    \"description\": \"A list of all relationships connecting the nodes.\",\n",
        "                    \"type\": \"array\",\n",
        "                    \"items\": {\n",
        "                      \"type\": \"object\",\n",
        "                      \"properties\": {\n",
        "                        \"source\": {\n",
        "                          \"description\": \"The 'id' of the starting node for the relationship.\",\n",
        "                          \"type\": \"string\"\n",
        "                        },\n",
        "                        \"target\": {\n",
        "                          \"description\": \"The 'id' of the ending node for the relationship.\",\n",
        "                          \"type\": \"string\"\n",
        "                        },\n",
        "                        \"type\": {\n",
        "                          \"description\": \"The type of the relationship, corresponding to a Neo4j relationship type (e.g., 'EDUCATED_AT').\",\n",
        "                          \"type\": \"string\"\n",
        "                        },\n",
        "                        \"properties\": {\n",
        "                          \"description\": \"An optional key-value map of the relationship's properties.\",\n",
        "                          \"type\": \"object\",\n",
        "                          \"additionalProperties\": true\n",
        "                        }\n",
        "                      },\n",
        "                      \"required\": [\"source\", \"target\", \"type\"]\n",
        "                    }\n",
        "                  }\n",
        "                },\n",
        "                \"required\": [\"nodes\", \"relationships\"]\n",
        "              }\n",
        "    '''\n",
        "\n",
        "    Q_KG_BASE = \"\"\"\n",
        "    You are an expert assistant that analyzes text to build a knowledge graph. Your goal is to extract named entities, their conceptual types, and the relationships between them. Follow the instructions and the example precisely.\n",
        "\n",
        "    1. Instructions\n",
        "\n",
        "    A. Instance and Type Nodes: For every named entity you identify:\n",
        "        Create an Instance Node: This represents the specific entity (e.g., \"Ordinance 15-01\").\n",
        "            It must have the single label `[\"Instance\"]`.\n",
        "            Its `properties` must include:\n",
        "                `id`: Assign a unique `id` starting with the chunk prefix `c{chunk}-node-` followed by a sequential number (e.g., \"c{chunk}-node-1\", \"c{chunk}-node-2\").\n",
        "                `name`: The lowercase version of the entity's text (e.g., \"ordinance 15-01\").\n",
        "                `displayName`: The Title Case version of the entity's text (e.g., \"Ordinance 15-01\").\n",
        "                `mergeCount`: Initialized to `0`.\n",
        "                `refinementCount`: Initialized to `0`.\n",
        "                `lastRefined`: Initialized to the current timestamp.\n",
        "            Add any other properties you find relevant.\n",
        "        Create a Type Node: This represents the abstract concept (e.g., \"Ordinance\").\n",
        "            It must have the single label `[\"Type\"]`.\n",
        "            Do not worry about creating duplicate Type nodes; the system will handle them later.\n",
        "            Its `properties` must include:\n",
        "                `id`: Assign a unique `id` starting with the chunk prefix `c{chunk}-node-` followed by a sequential number (e.g., \"c{chunk}-node-1\", \"c{chunk}-node-2\").\n",
        "                `name`: The lowercase version of the concept's name (e.g., \"ordinance\").\n",
        "                `displayName`: The Title Case of the concept's name (e.g., \"Ordinance\").\n",
        "                `mergeCount`: Initialized to `0`.\n",
        "                `refinementCount`: Initialized to `0`.\n",
        "                `lastRefined`: Initialized to the current timestamp.\n",
        "\n",
        "    B. Relationships:\n",
        "        `IS_A` Relationship: For every Instance Node, you MUST create an `IS_A` relationship connecting it to its corresponding Type Node.\n",
        "        Other Relationships: Create any other relationships you find between Instance Nodes (e.g., `AMENDS`, `CONTAINS`).\n",
        "\n",
        "    2. Crucial Example\n",
        "\n",
        "    If the text is \"Ordinance 15-01 amends ¬ß 20-7.\" from chunk {chunk}, your JSON output must include:\n",
        "\n",
        "    An Instance Node for \"Ordinance 15-01\": `{{\"id\": \"c{chunk}-node-1\", \"labels\": [\"Instance\"], \"properties\": {{\"name\": \"ordinance 15-01\", \"displayName\": \"Ordinance 15-01\", \"mergeCount\": 0, \"refinementCount\": 0, \"lastRefined\": timestamp()}}}}`\n",
        "    An Instance Node for \"¬ß 20-7\": `{{\"id\": \"c{chunk}-node-2\", \"labels\": [\"Instance\"], \"properties\": {{\"name\": \"¬ß 20-7\", \"displayName\": \"¬ß 20-7\", \"mergeCount\": 0, \"refinementCount\": 0, \"lastRefined\": timestamp()}}}}`\n",
        "    A Type Node for \"Ordinance\": `{{\"id\": \"c{chunk}-node-3\", \"labels\": [\"Type\"], \"properties\": {{\"name\": \"ordinance\", \"displayName\": \"Ordinance\", \"mergeCount\": 0, \"refinementCount\": 0, \"lastRefined\": timestamp()}}}}`\n",
        "    A Type Node for \"DocumentSection\": `{{\"id\": \"c{chunk}-node-4\", \"labels\": [\"Type\"], \"properties\": {{\"name\": \"documentsection\", \"displayName\": \"DocumentSection\", \"mergeCount\": 0, \"refinementCount\": 0, \"lastRefined\": timestamp()}}}}`\n",
        "    Relationships:\n",
        "        `{{\"source\": \"c{chunk}-node-1\", \"target\": \"c{chunk}-node-3\", \"type\": \"IS_A\"}}`\n",
        "        `{{\"source\": \"c{chunk}-node-2\", \"target\": \"c{chunk}-node-4\", \"type\": \"IS_A\"}}`\n",
        "        `{{\"source\": \"c{chunk}-node-1\", \"target\": \"c{chunk}-node-2\", \"type\": \"AMENDS\"}}`\n",
        "\n",
        "    3. JSON Output\n",
        "\n",
        "    Provide ONLY the JSON output formatted according to the schema. Do not include explanations.\n",
        "\n",
        "    JSON Schema Format:\n",
        "    \"\"\"\n",
        "\n",
        "    # Suffix for the knowledge graph extraction prompt, followed by the text to analyze\n",
        "    Q_KG_SUFFIX = \"Analyze the following text and follow all instructions, especially the creation of `IS_A` relationships for every entity: \"\n",
        "\n",
        "    # --- MODIFICATION START ---\n",
        "    # Restored the original, simple prompt for LangExtract\n",
        "    Q_LE = \"\"\"\n",
        "    Extract named entities and relationships in order of appearance.\n",
        "    Use exact text for extractions. Do not paraphrase or overlap entities.\n",
        "    Provide meaningful attributes for each entity to add context.\n",
        "    For relationships, please include 'source_entity', 'target_entity', and 'relationship_type' in the attributes.\n",
        "    \"\"\"\n",
        "    # --- MODIFICATION END ---\n",
        "\n",
        "\n",
        "    # --- CHANGE: Added JSON Schemas to all refinement prompts ---\n",
        "    # Prompt for merging INSTANCE entities based on semantic similarity\n",
        "    Q_MIE = \"\"\"\n",
        "    You are a data quality expert responsible for entity resolution. Your task is to identify and merge duplicate entity instances from the provided list.\n",
        "    Analyze the following JSON list of entity instances. Identify pairs that represent the exact same real-world object or concept.\n",
        "\n",
        "    **Key Rules:**\n",
        "    1.  Merge based on semantic similarity or clear redundancy (e.g., \"King Theron\" and \"Theron\" refer to the same person).\n",
        "    2.  The output **must** be a single, valid JSON object that conforms to the provided schema.\n",
        "    3.  Do **not** include any text or explanations outside of the final JSON object.\n",
        "    4.  Do **not** suggest merging an instance with itself.\n",
        "    5.  If no instances should be merged, return an empty list of pairs.\n",
        "\n",
        "    **JSON Schema:**\n",
        "    {{\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {{\n",
        "        \"merge_pairs\": {{\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {{\n",
        "            \"type\": \"array\",\n",
        "            \"items\": [{{ \"type\": \"string\" }}, {{ \"type\": \"string\" }}],\n",
        "            \"minItems\": 2,\n",
        "            \"maxItems\": 2\n",
        "          }}\n",
        "        }}\n",
        "      }},\n",
        "      \"required\": [\"merge_pairs\"]\n",
        "    }}\n",
        "\n",
        "    Provide ONLY the JSON response.\n",
        "\n",
        "    **Entity Instances to Analyze:**\n",
        "    \"\"\"\n",
        "\n",
        "    # --- CHANGE: Updated prompt to be explicit about returning IDs ---\n",
        "    # Prompt for merging TYPE entities based on semantic similarity\n",
        "    Q_MTT = \"\"\"\n",
        "    You are a data quality expert. Analyze the following list of JSON objects, where each object represents an entity type with a 'name' and a unique 'id'.\n",
        "    Identify pairs of types that are semantically equivalent (e.g., 'Rule' and 'Regulation' are the same concept).\n",
        "\n",
        "    **Key Rules:**\n",
        "    1.  The output **must** be a single, valid JSON object that conforms to the provided schema.\n",
        "    2.  The `merge_pairs` array should contain pairs of the unique **'id's** for the types that should be merged.\n",
        "    3.  Do **not** return the names of the types, only their IDs.\n",
        "    4.  Do **not** suggest merging a type with itself.\n",
        "    5.  If no types should be merged, return an empty list.\n",
        "\n",
        "    **JSON Schema:**\n",
        "    {{\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {{\n",
        "        \"merge_pairs\": {{\n",
        "          \"type\": \"array\",\n",
        "          \"items\": {{\n",
        "            \"type\": \"array\",\n",
        "            \"items\": [{{ \"type\": \"string\" }}, {{ \"type\": \"string\" }}],\n",
        "            \"minItems\": 2,\n",
        "            \"maxItems\": 2\n",
        "          }}\n",
        "        }}\n",
        "      }},\n",
        "      \"required\": [\"merge_pairs\"]\n",
        "    }}\n",
        "\n",
        "    Provide ONLY the JSON response.\n",
        "\n",
        "    **Entity Types to Analyze:**\n",
        "    \"\"\"\n",
        "\n",
        "    # --- NEW: Focused prompt to check if a node IS A PART of another. ---\n",
        "    Q_IS_PART_OF = \"\"\"\n",
        "    You are an ontology expert specializing in meronymy (part-whole relationships).\n",
        "    Your task is to determine if the 'part_candidate' entity is a component of ANY of the 'whole_candidates'.\n",
        "\n",
        "    **Instructions:**\n",
        "    1.  Analyze the 'part_candidate' (ID and name).\n",
        "    2.  Review the list of 'whole_candidates' (ID and name).\n",
        "    3.  Identify the SINGLE most likely entity from the list that the candidate is a part of.\n",
        "    4.  The relationship must be a clear part-whole connection, either structural (a chapter is part of a book) or conceptual (a wheel is part of a car).\n",
        "    5.  Return a JSON object with the 'part_id' and the 'whole_id' of the single best match.\n",
        "    6.  If no valid part-whole relationship exists, return an empty JSON object: {{}}.\n",
        "\n",
        "    **JSON Schema:**\n",
        "    {{\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {{\n",
        "        \"part_id\": {{\"type\": \"string\"}},\n",
        "        \"whole_id\": {{\"type\": \"string\"}}\n",
        "      }}\n",
        "    }}\n",
        "\n",
        "    Provide ONLY the JSON response.\n",
        "\n",
        "    ---\n",
        "    **Part Candidate to Analyze:**\n",
        "    {part_candidate_json}\n",
        "\n",
        "    **List of Potential Wholes:**\n",
        "    {whole_candidates_json}\n",
        "    ---\n",
        "    \"\"\"\n",
        "\n",
        "    # --- NEW: Focused prompt to check if a node HAS a part. ---\n",
        "    Q_HAS_PART = \"\"\"\n",
        "    You are an ontology expert specializing in meronymy (part-whole relationships).\n",
        "    Your task is to determine if the 'whole_candidate' entity contains ANY of the 'part_candidates' as components.\n",
        "\n",
        "    **Instructions:**\n",
        "    1.  Analyze the 'whole_candidate' (ID and name).\n",
        "    2.  Review the list of 'part_candidates' (ID and name).\n",
        "    3.  Identify ALL entities from the list that are clear parts of the 'whole_candidate'.\n",
        "    4.  The relationship must be a clear part-whole connection, either structural (a book has a chapter) or conceptual (a car has a wheel).\n",
        "    5.  Return a JSON object containing a list of all matching part IDs.\n",
        "    6.  If the whole contains no parts from the list, return an empty list.\n",
        "\n",
        "    **JSON Schema:**\n",
        "    {{\n",
        "        \"type\": \"object\",\n",
        "        \"properties\": {{\n",
        "            \"part_ids\": {{\n",
        "                \"type\": \"array\",\n",
        "                \"items\": {{\"type\": \"string\"}}\n",
        "            }}\n",
        "        }},\n",
        "        \"required\": [\"part_ids\"]\n",
        "    }}\n",
        "\n",
        "    Provide ONLY the JSON response.\n",
        "\n",
        "    ---\n",
        "    **Whole Candidate to Analyze:**\n",
        "    {whole_candidate_json}\n",
        "\n",
        "    **List of Potential Parts:**\n",
        "    {part_candidates_json}\n",
        "    ---\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt for correcting the type of an instance\n",
        "    Q_ITC = \"\"\"\n",
        "    You are a data quality analyst. Your task is to review an entity instance and its current type, then determine if a more specific classification exists from the list of available types.\n",
        "\n",
        "    **Instructions:**\n",
        "    1.  Analyze the `instance_name`, `instance_id`, and `current_type`.\n",
        "    2.  Review the `available_types`, which is a list of JSON objects, each with a unique `id` and `name`.\n",
        "    3.  If you find a more appropriate type, return a JSON object containing the original `instance_id` and the `id` of the new type (`new_type_id`).\n",
        "    4.  If the current type is the most accurate, return an empty JSON object: {{}}.\n",
        "\n",
        "    **JSON Schema:**\n",
        "    {{\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {{\n",
        "        \"instance_id\": {{\"type\": \"string\"}},\n",
        "        \"new_type_id\": {{\"type\": \"string\"}}\n",
        "      }}\n",
        "    }}\n",
        "\n",
        "    **Instance to Review:**\n",
        "    {instance_json}\n",
        "\n",
        "    **Available Types:**\n",
        "    {types_json}\n",
        "\n",
        "    Provide ONLY the JSON response.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt for organizing ontology hierarchy\n",
        "    Q_OOH = \"\"\"\n",
        "    Given the child entity type \"{child_name}\" (id: \"{child_id}\"), which of the following is its most direct parent?\n",
        "    A direct parent should be the next most general concept.\n",
        "\n",
        "    **Available parent types (with their IDs):**\n",
        "    {potential_parents}\n",
        "\n",
        "    **JSON Schema:**\n",
        "    {{\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {{\n",
        "        \"child_id\": {{\"type\": \"string\"}},\n",
        "        \"parent_id\": {{\"type\": \"string\"}}\n",
        "      }},\n",
        "      \"required\": [\"child_id\", \"parent_id\"]\n",
        "    }}\n",
        "\n",
        "    Return a single JSON object with the ID of the child and the ID of the selected parent.\n",
        "    \"\"\"\n",
        "\n",
        "    # Prompt for comparing KGs\n",
        "    Q_COMPARE_KGS = \"\"\"\n",
        "    You are an expert system for comparing knowledge graphs. Compare the \"Generated Graph\" to the \"Ground Truth\" and provide a quantitative analysis.\n",
        "\n",
        "    **Instructions:**\n",
        "    1.  **Analyze Entities**: Count the total entities in the \"Ground Truth\" and how many of them are present in the \"Generated Graph\". A match occurs if the name is identical or a very close semantic equivalent.\n",
        "    2.  **Analyze Relationships**: Count the total relationships in the \"Ground Truth\" and how many are present in the \"Generated Graph\". A match requires the source, target, and relationship type to be correct.\n",
        "\n",
        "    **JSON Schema:**\n",
        "    {{\n",
        "      \"type\": \"object\",\n",
        "      \"properties\": {{\n",
        "        \"ground_truth_entities\": {{\"type\": \"integer\"}},\n",
        "        \"matched_entities\": {{\"type\": \"integer\"}},\n",
        "        \"ground_truth_relationships\": {{\"type\": \"integer\"}},\n",
        "        \"matched_relationships\": {{\"type\": \"integer\"}}\n",
        "      }},\n",
        "      \"required\": [\"ground_truth_entities\", \"matched_entities\", \"ground_truth_relationships\", \"matched_relationships\"]\n",
        "    }}\n",
        "\n",
        "    Provide your analysis ONLY in the specified JSON format.\n",
        "\n",
        "    ---\n",
        "    **Ground Truth:**\n",
        "    {ground_truth_text}\n",
        "    ---\n",
        "    **Generated Graph:**\n",
        "    {generated_graph_text}\n",
        "    ---\n",
        "    \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Oe3GgLoweXx"
      },
      "source": [
        "## Utilities Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 466,
      "metadata": {
        "id": "IVpo7CydwfYp"
      },
      "outputs": [],
      "source": [
        "class Utils:\n",
        "    \"\"\"A collection of static utility methods for data normalization.\"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_pascal_case(input_string: str) -> str:\n",
        "        \"\"\"Converts a string to PascalCase for node labels.\"\"\"\n",
        "        s = re.sub(r'[^a-zA-Z0-9]+', ' ', str(input_string)).strip()\n",
        "        if not s:\n",
        "            return \"\"\n",
        "        return \"\".join(word.capitalize() for word in s.split())\n",
        "\n",
        "    @staticmethod\n",
        "    def _to_upper_snake_case(input_string: str) -> str:\n",
        "        \"\"\"Converts a string to UPPER_SNAKE_CASE for relationship types.\"\"\"\n",
        "        s = re.sub(r'[^a-zA-Z0-9]+', '_', str(input_string)).strip('_')\n",
        "        if not s:\n",
        "            return \"\"\n",
        "        # Preserve special cases\n",
        "        if s.upper() in ['IS_A', 'PART_OF']:\n",
        "            return s.upper()\n",
        "        return s.upper()\n",
        "\n",
        "    # In the Utils Class\n",
        "    @staticmethod\n",
        "    def _to_camel_case(input_string: str) -> str:\n",
        "        \"\"\"Converts a string to camelCase for property keys.\"\"\"\n",
        "        # This line converts separators like '_' or ' ' into a single space\n",
        "        s = re.sub(r'[^a-zA-Z0-9]+', ' ', str(input_string)).strip()\n",
        "        if not s:\n",
        "            return \"\"\n",
        "\n",
        "        words = s.split()\n",
        "\n",
        "        # If there's only one \"word\" (e.g., \"displayName\" or \"Name\"),\n",
        "        # this makes the first character lowercase and leaves the rest.\n",
        "        if len(words) == 1:\n",
        "            return words[0][0].lower() + words[0][1:]\n",
        "\n",
        "        # If there are multiple words (e.g., \"Merge Count\"),\n",
        "        # this lowercases the first and capitalizes the rest.\n",
        "        return words[0].lower() + \"\".join(word.capitalize() for word in words[1:])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W_j9N26QwrMN"
      },
      "source": [
        "## STM Processor Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 467,
      "metadata": {
        "id": "4LMoue7Kwws-"
      },
      "outputs": [],
      "source": [
        "## STM Processor Class\n",
        "class STMProcessor:\n",
        "    \"\"\"Handles processing a single text chunk into a clean knowledge graph fragment.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_service: LLMService):\n",
        "        self.llm_service = llm_service\n",
        "        self.config = MindConfig()\n",
        "\n",
        "    def process_chunk(self, text_chunk: str, chunk_num: int, file_name: str) -> str:\n",
        "        \"\"\"Takes a text chunk and returns a normalized JSON string of the graph fragment.\"\"\"\n",
        "        prompt = (\n",
        "            self.config.Q_KG_BASE.format(chunk=chunk_num) +\n",
        "            self.config.JSON_SCHEMA +\n",
        "            self.config.Q_KG_SUFFIX +\n",
        "            text_chunk\n",
        "        )\n",
        "        response_str = self.llm_service.query_llm(prompt)\n",
        "        logging.debug(f\"Text for chunk {chunk_num}: {text_chunk}\")\n",
        "        logging.debug(f\"LLM response for chunk {chunk_num}: {response_str}\")\n",
        "\n",
        "        try:\n",
        "            json_data = json.loads(response_str)\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.warning(f\"Initial JSON parsing failed for chunk {chunk_num}: {e}. Retrying with self-correction...\")\n",
        "            correction_prompt = f\"The following text is not valid JSON. Please fix it and return only the corrected JSON object.\\\\n\\\\n{response_str}\"\n",
        "            corrected_response_str = self.llm_service.query_llm(correction_prompt)\n",
        "            try:\n",
        "                json_data = json.loads(corrected_response_str)\n",
        "            except json.JSONDecodeError as final_e:\n",
        "                raise JSONParsingError(f\"Self-correction failed for chunk {chunk_num}: {final_e}\") from final_e\n",
        "\n",
        "        if not json_data:\n",
        "            raise JSONParsingError(f\"LLM returned empty JSON for chunk {chunk_num}.\")\n",
        "\n",
        "        preprocessed_data = self._preprocess_llm_output(json_data, chunk_num)\n",
        "        logging.debug(f\"Preprocessed data for chunk {chunk_num}: {preprocessed_data}\")\n",
        "\n",
        "        augmented_data = self._augment_graph_data(preprocessed_data, file_name, chunk_num)\n",
        "        normalized_json_str = self._normalize_graph(augmented_data)\n",
        "        return normalized_json_str\n",
        "\n",
        "    def _preprocess_llm_output(self, graph_data: dict, chunk_num: int) -> dict:\n",
        "        \"\"\"\n",
        "        Validates and consolidates raw graph data from the LLM before further processing.\n",
        "        1. Removes invalid IS_A relationships (Type -> Instance).\n",
        "        2. Deduplicates Type nodes with the same name within the same chunk.\n",
        "        3. Adds creationStage property.\n",
        "        \"\"\"\n",
        "        if not isinstance(graph_data.get('nodes'), list) or not isinstance(graph_data.get('relationships'), list):\n",
        "            logging.warning(f\"Skipping pre-processing for chunk {chunk_num} due to malformed graph data.\")\n",
        "            return graph_data\n",
        "\n",
        "        nodes = graph_data.get('nodes', [])\n",
        "        relationships = graph_data.get('relationships', [])\n",
        "        node_map = {node['id']: node for node in nodes}\n",
        "\n",
        "        # --- Task 132: Add creationStage to all LLM-generated elements ---\n",
        "        for node in nodes:\n",
        "            node.setdefault('properties', {})[GraphSchema.PROP_CREATION_STAGE] = GraphSchema.STAGE_SOURCE_EXTRACTION\n",
        "        for rel in relationships:\n",
        "            rel.setdefault('properties', {})[GraphSchema.PROP_CREATION_STAGE] = GraphSchema.STAGE_SOURCE_EXTRACTION\n",
        "        # --- End Task 132 Change ---\n",
        "\n",
        "        # 1. Validate and remove incorrect IS_A relationships\n",
        "        valid_relationships = []\n",
        "        for rel in relationships:\n",
        "            source_id = rel.get('source')\n",
        "            target_id = rel.get('target')\n",
        "            rel_type = str(rel.get('type', '')).upper()\n",
        "\n",
        "            if rel_type == 'IS_A':\n",
        "                source_node = node_map.get(source_id)\n",
        "                target_node = node_map.get(target_id)\n",
        "                if source_node and target_node:\n",
        "                    source_labels = source_node.get('labels', [])\n",
        "                    target_labels = target_node.get('labels', [])\n",
        "                    if 'Type' in source_labels and 'Instance' in target_labels:\n",
        "                        # --- MODIFICATION START ---\n",
        "                        # Log with both name and ID for clarity\n",
        "                        source_name = source_node.get('properties', {}).get('displayName', source_id)\n",
        "                        target_name = target_node.get('properties', {}).get('displayName', target_id)\n",
        "                        logging.info(f\"Removing invalid IS_A relationship from Type ('{source_name}' ({source_id})) to Instance ('{target_name}' ({target_id})).\")\n",
        "                        # --- MODIFICATION END ---\n",
        "                        continue\n",
        "            valid_relationships.append(rel)\n",
        "        graph_data['relationships'] = valid_relationships\n",
        "\n",
        "        # 2. Deduplicate Type nodes generated in the same chunk\n",
        "        type_nodes_by_name = {}\n",
        "        for node in nodes:\n",
        "            if 'Type' in node.get('labels', []):\n",
        "                name = node.get('properties', {}).get('name')\n",
        "                if name:\n",
        "                    type_nodes_by_name.setdefault(name, []).append(node)\n",
        "\n",
        "        id_redirects = {}\n",
        "        nodes_to_remove = set()\n",
        "        for name, type_nodes in type_nodes_by_name.items():\n",
        "            if len(type_nodes) > 1:\n",
        "                canonical_node = type_nodes[0]\n",
        "                # --- MODIFICATION START ---\n",
        "                # Log with both name and ID for clarity\n",
        "                canonical_name = canonical_node.get('properties', {}).get('displayName', canonical_node['id'])\n",
        "                logging.info(f\"Found {len(type_nodes)} Type nodes for '{name}'. Consolidating to '{canonical_name}' ({canonical_node['id']}).\")\n",
        "                # --- MODIFICATION END ---\n",
        "                for duplicate_node in type_nodes[1:]:\n",
        "                    id_redirects[duplicate_node['id']] = canonical_node['id']\n",
        "                    nodes_to_remove.add(duplicate_node['id'])\n",
        "\n",
        "        if id_redirects:\n",
        "            for rel in graph_data.get('relationships', []):\n",
        "                if rel.get('target') in id_redirects:\n",
        "                    # --- MODIFICATION START ---\n",
        "                    # Log with both name and ID for clarity\n",
        "                    original_target_id = rel['target']\n",
        "                    new_target_id = id_redirects[original_target_id]\n",
        "                    rel['target'] = new_target_id\n",
        "\n",
        "                    original_target_node = node_map.get(original_target_id, {})\n",
        "                    original_target_name = original_target_node.get('properties', {}).get('displayName', original_target_id)\n",
        "\n",
        "                    new_target_node = node_map.get(new_target_id, {})\n",
        "                    new_target_name = new_target_node.get('properties', {}).get('displayName', new_target_id)\n",
        "\n",
        "                    logging.info(f\"Redirected relationship target from '{original_target_name}' ({original_target_id}) to '{new_target_name}' ({new_target_id})\")\n",
        "                    # --- MODIFICATION END ---\n",
        "            graph_data['nodes'] = [node for node in nodes if node['id'] not in nodes_to_remove]\n",
        "\n",
        "        return graph_data\n",
        "\n",
        "    def _augment_graph_data(self, graph_data: dict, file_name: str, chunk_num: int) -> dict:\n",
        "        \"\"\"Injects Source and Thing nodes and their relationships into the graph data.\"\"\"\n",
        "        source_instance_id = f\"source-instance-{re.sub(r'[^a-zA-Z0-9_.-]', '-', file_name)}\"\n",
        "        source_type_id = GraphSchema.CANONICAL_ID_SOURCE\n",
        "        thing_type_id = GraphSchema.CANONICAL_ID_THING\n",
        "\n",
        "        graph_data.setdefault('nodes', [])\n",
        "        graph_data.setdefault('relationships', [])\n",
        "        existing_ids = {node['id'] for node in graph_data['nodes']}\n",
        "\n",
        "        # Task 132: Add creationStage property to framework-generated nodes\n",
        "        ingestion_props = {\n",
        "            \"mergeCount\": 0,\n",
        "            \"refinementCount\": 0,\n",
        "            \"lastRefined\": datetime.now(timezone.utc).isoformat(),\n",
        "            GraphSchema.PROP_CREATION_STAGE: GraphSchema.STAGE_INGESTION\n",
        "        }\n",
        "\n",
        "        if source_instance_id not in existing_ids:\n",
        "            graph_data['nodes'].append({\n",
        "                \"id\": source_instance_id,\n",
        "                \"labels\": [\"Node\", \"Instance\", \"Source\"],\n",
        "                \"properties\": {\"name\": file_name.lower(), \"displayName\": file_name, **ingestion_props}\n",
        "            })\n",
        "\n",
        "        if source_type_id not in existing_ids:\n",
        "            graph_data['nodes'].append({\n",
        "                \"id\": source_type_id,\n",
        "                \"labels\": [\"Node\", \"Type\"],\n",
        "                \"properties\": {\"name\": \"source\", \"displayName\": \"Source\", **ingestion_props}\n",
        "            })\n",
        "\n",
        "        if thing_type_id not in existing_ids:\n",
        "            graph_data['nodes'].append({\n",
        "                \"id\": thing_type_id,\n",
        "                \"labels\": [\"Node\", \"Type\"],\n",
        "                \"properties\": {\"name\": \"thing\", \"displayName\": \"Thing\", **ingestion_props}\n",
        "            })\n",
        "\n",
        "        existing_rels = {(rel['source'], rel['target'], rel['type']) for rel in graph_data['relationships']}\n",
        "        if (source_instance_id, source_type_id, \"IS_A\") not in existing_rels:\n",
        "            graph_data['relationships'].append({\n",
        "                \"source\": source_instance_id,\n",
        "                \"target\": source_type_id,\n",
        "                \"type\": \"IS_A\",\n",
        "                \"properties\": {GraphSchema.PROP_CREATION_STAGE: GraphSchema.STAGE_INGESTION} # Task 132\n",
        "            })\n",
        "\n",
        "        for node in graph_data.get('nodes', []):\n",
        "            if node['id'] in [source_instance_id, source_type_id, thing_type_id]:\n",
        "                continue\n",
        "            if (node['id'], source_instance_id, \"FROM\") not in existing_rels:\n",
        "                graph_data['relationships'].append({\n",
        "                    \"source\": node['id'],\n",
        "                    \"target\": source_instance_id,\n",
        "                    \"type\": \"FROM\",\n",
        "                    \"properties\": {\"chunk\": chunk_num, GraphSchema.PROP_CREATION_STAGE: GraphSchema.STAGE_INGESTION} # Task 132\n",
        "                })\n",
        "                existing_rels.add((node['id'], source_instance_id, \"FROM\"))\n",
        "\n",
        "        logging.debug(f\"Augmented graph for chunk {chunk_num} with Source and Thing info.\")\n",
        "        return graph_data\n",
        "\n",
        "    def _normalize_graph(self, graph_data: dict) -> str:\n",
        "        \"\"\"Applies consistent naming conventions and ensures required properties exist.\"\"\"\n",
        "        for node in graph_data.get('nodes', []):\n",
        "            if 'labels' in node:\n",
        "                node['labels'] = [Utils._to_pascal_case(label) for label in node['labels']]\n",
        "            if 'properties' in node:\n",
        "                props = node['properties']\n",
        "                if 'displayname' in props and 'displayName' not in props:\n",
        "                    props['displayName'] = props.pop('displayname')\n",
        "                if 'mergecount' in props and 'mergeCount' not in props:\n",
        "                    props['mergeCount'] = props.pop('mergecount')\n",
        "                displayName = props.get('displayName')\n",
        "                name = props.get('name')\n",
        "                if displayName and name is None:\n",
        "                    props['name'] = str(displayName).lower()\n",
        "                elif name and displayName is None:\n",
        "                    is_type_node = \"Type\" in node.get('labels', [])\n",
        "                    props['displayName'] = str(name).title() if not is_type_node else str(name).capitalize()\n",
        "                if 'mergeCount' not in props:\n",
        "                    props['mergeCount'] = 0\n",
        "        for rel in graph_data.get('relationships', []):\n",
        "            if 'type' in rel:\n",
        "                rel['type'] = Utils._to_upper_snake_case(rel['type'])\n",
        "            if 'properties' in rel:\n",
        "                rel['properties'] = {Utils._to_camel_case(k): v for k, v in rel['properties'].items()}\n",
        "        return json.dumps(graph_data, indent=self.config.INDENT)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QH-Pn4kMw2tt"
      },
      "source": [
        "## LTM Consolidator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 468,
      "metadata": {
        "id": "jjSLkhrLw7fX"
      },
      "outputs": [],
      "source": [
        "## Long Term Memory Consolidator Class\n",
        "class LTMConsolidator:\n",
        "    \"\"\"Handles all stages of the Long-Term Memory consolidation and refinement process.\"\"\"\n",
        "\n",
        "    def __init__(self, llm_service: LLMService, graph_db: GraphDB, experiment_id: str, run_timestamp: str,\n",
        "                 num_cycles: int, merge_sample_size: int, hierarchy_sample_size: int,\n",
        "                 part_of_sample_size: int, part_of_candidate_pool_size: int):\n",
        "        self.llm_service = llm_service\n",
        "        self.graph_db = graph_db\n",
        "        self.experiment_id = experiment_id\n",
        "        self.run_timestamp = run_timestamp\n",
        "        self.num_refinement_cycles = num_cycles\n",
        "        self.ltm_merge_sample_size = merge_sample_size\n",
        "        self.ltm_hierarchy_sample_size = hierarchy_sample_size\n",
        "        self.part_of_sample_size = part_of_sample_size\n",
        "        self.part_of_candidate_pool_size = part_of_candidate_pool_size\n",
        "        self.config = MindConfig()\n",
        "\n",
        "    def _merge_duplicate_relationships(self):\n",
        "        \"\"\"Wrapper for the deterministic relationship merge process.\"\"\"\n",
        "        with log_step(\"Deterministically merging duplicate relationships\"):\n",
        "            merge_count = self.graph_db.refiner.merge_duplicate_relationships(\n",
        "                self.experiment_id, self.run_timestamp\n",
        "            )\n",
        "            logging.info(f\"Consolidated {merge_count} groups of duplicate relationships.\")\n",
        "\n",
        "    def _merge_exact_duplicates(self):\n",
        "        \"\"\"Wrapper for the deterministic merge process.\"\"\"\n",
        "        with log_step(\"Deterministically merging exact duplicates by name and type\"):\n",
        "            merge_count = self.graph_db.refiner.merge_exact_duplicates_by_name_and_type(\n",
        "                self.experiment_id, self.run_timestamp\n",
        "            )\n",
        "            logging.info(f\"Completed {merge_count} deterministic merge operations.\")\n",
        "\n",
        "    def run_consolidation(self):\n",
        "        \"\"\"Orchestrates the entire LTM consolidation process.\"\"\"\n",
        "        with log_step(\"Running LTM Consolidation\"):\n",
        "            self.graph_db.refiner.mark_source_nodes_as_donotchange()\n",
        "\n",
        "            # Run the fast, deterministic merges first\n",
        "            self._merge_exact_duplicates()\n",
        "            self._merge_duplicate_relationships() # <-- NEW STEP ADDED HERE\n",
        "\n",
        "            self._classify_untyped_nodes()\n",
        "\n",
        "            for i in range(self.num_refinement_cycles):\n",
        "                with log_step(f\"Refinement Cycle {i+1}/{self.num_refinement_cycles}\"):\n",
        "                    # --- NEW: In-cycle tracker for deleted nodes ---\n",
        "                    deleted_nodes_in_cycle = set()\n",
        "\n",
        "                    self._refine_meronymic_relationships(deleted_nodes_in_cycle)\n",
        "\n",
        "                    _, removed_type_ids = self._refine_type_system(deleted_nodes_in_cycle)\n",
        "                    deleted_nodes_in_cycle.update(removed_type_ids)\n",
        "\n",
        "                    self._organize_ontology_hierarchy(deleted_nodes_in_cycle)\n",
        "                    self._correct_instance_types(deleted_nodes_in_cycle)\n",
        "\n",
        "                    _, removed_instance_ids = self._merge_similar_instances(deleted_nodes_in_cycle)\n",
        "                    deleted_nodes_in_cycle.update(removed_instance_ids)\n",
        "\n",
        "            with log_step(\"Final Hierarchy Cleanup\"):\n",
        "                linked_orphans = self.graph_db.refiner.link_orphan_types_to_thing(self.experiment_id, self.run_timestamp)\n",
        "                logging.info(f\"Linked {linked_orphans} orphan Type nodes to '{GraphSchema.CANONICAL_NAME_THING}'.\")\n",
        "\n",
        "    def _classify_untyped_nodes(self):\n",
        "        with log_step(f\"Classifying untyped nodes to '{GraphSchema.CANONICAL_NAME_THING}' type\"):\n",
        "            nodes_processed = self.graph_db.refiner.process_nodes_without_is_a(self.experiment_id, self.run_timestamp)\n",
        "            logging.info(f\"Classified {nodes_processed} untyped nodes to '{GraphSchema.CANONICAL_NAME_THING}' type.\")\n",
        "\n",
        "    def _refine_meronymic_relationships(self, deleted_nodes_in_cycle: set):\n",
        "        with log_step(f\"Refining {GraphSchema.REL_PART_OF} relationships\"):\n",
        "            try:\n",
        "                all_nodes_to_test = self.graph_db.reader.select_random_entities(None, self.part_of_sample_size, self.experiment_id, self.run_timestamp)\n",
        "                all_candidates = self.graph_db.reader.select_random_entities(None, self.part_of_candidate_pool_size, self.experiment_id, self.run_timestamp)\n",
        "\n",
        "                # --- NEW: Filter out already deleted nodes ---\n",
        "                nodes_to_test = [n for n in all_nodes_to_test if n.get(GraphSchema.PROP_ID) not in deleted_nodes_in_cycle]\n",
        "                candidate_pool = [n for n in all_candidates if n.get(GraphSchema.PROP_ID) not in deleted_nodes_in_cycle]\n",
        "\n",
        "                if not nodes_to_test or len(candidate_pool) < 2:\n",
        "                    logging.info(f\"Not enough entities to refine {GraphSchema.REL_PART_OF} relationships.\")\n",
        "                    return\n",
        "\n",
        "                relationships_found = []\n",
        "                for node in nodes_to_test:\n",
        "                    other_nodes = [n for n in candidate_pool if n.get(GraphSchema.PROP_ID) != node.get(GraphSchema.PROP_ID)]\n",
        "                    if not other_nodes: continue\n",
        "\n",
        "                    is_part_of_rels = self._ask_is_part_of(node, other_nodes)\n",
        "                    relationships_found.extend(is_part_of_rels)\n",
        "\n",
        "                    has_part_rels = self._ask_has_part(node, other_nodes)\n",
        "                    relationships_found.extend(has_part_rels)\n",
        "\n",
        "                if relationships_found:\n",
        "                    unique_rels_set = {tuple(sorted(d.items())) for d in relationships_found}\n",
        "                    unique_rels = [dict(t) for t in unique_rels_set]\n",
        "                    created_count = self.graph_db.refiner.create_part_of_relationships(unique_rels, self.experiment_id, self.run_timestamp)\n",
        "                    logging.info(f\"Created {created_count} new {GraphSchema.REL_PART_OF} relationships.\")\n",
        "                else:\n",
        "                    logging.info(f\"No new {GraphSchema.REL_PART_OF} relationships were identified.\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error during {GraphSchema.REL_PART_OF} refinement: {e}\", exc_info=True)\n",
        "\n",
        "    def _ask_is_part_of(self, part_candidate_node, whole_candidates):\n",
        "        \"\"\"Asks the LLM if a node is a part of any node in a given list.\"\"\"\n",
        "        part_candidate_json = json.dumps({GraphSchema.PROP_ID: part_candidate_node.get(GraphSchema.PROP_ID), GraphSchema.PROP_NAME: part_candidate_node.get(GraphSchema.PROP_DISPLAY_NAME)})\n",
        "        whole_candidates_json = json.dumps([{GraphSchema.PROP_ID: n.get(GraphSchema.PROP_ID), GraphSchema.PROP_NAME: n.get(GraphSchema.PROP_DISPLAY_NAME)} for n in whole_candidates])\n",
        "\n",
        "        prompt = self.config.Q_IS_PART_OF.format(part_candidate_json=part_candidate_json, whole_candidates_json=whole_candidates_json)\n",
        "        try:\n",
        "            response_str = self.llm_service.query_llm(prompt)\n",
        "            data = json.loads(response_str)\n",
        "            if data and GraphSchema.JSON_KEY_PART_ID in data and GraphSchema.JSON_KEY_WHOLE_ID in data:\n",
        "                logging.debug(f\"LLM identified that '{data[GraphSchema.JSON_KEY_PART_ID]}' is part of '{data[GraphSchema.JSON_KEY_WHOLE_ID]}'.\")\n",
        "                return [{GraphSchema.JSON_KEY_PART_ID: data[GraphSchema.JSON_KEY_PART_ID], GraphSchema.JSON_KEY_WHOLE_ID: data[GraphSchema.JSON_KEY_WHOLE_ID]}]\n",
        "        except (json.JSONDecodeError, KeyError) as e:\n",
        "            logging.warning(f\"Could not parse 'is part of' response or key error: {e}\")\n",
        "        return []\n",
        "\n",
        "    def _ask_has_part(self, whole_candidate_node, part_candidates):\n",
        "        \"\"\"Asks the LLM if a node has any parts from a given list.\"\"\"\n",
        "        whole_candidate_json = json.dumps({GraphSchema.PROP_ID: whole_candidate_node.get(GraphSchema.PROP_ID), GraphSchema.PROP_NAME: whole_candidate_node.get(GraphSchema.PROP_DISPLAY_NAME)})\n",
        "        part_candidates_json = json.dumps([{GraphSchema.PROP_ID: n.get(GraphSchema.PROP_ID), GraphSchema.PROP_NAME: n.get(GraphSchema.PROP_DISPLAY_NAME)} for n in part_candidates])\n",
        "\n",
        "        prompt = self.config.Q_HAS_PART.format(whole_candidate_json=whole_candidate_json, part_candidates_json=part_candidates_json)\n",
        "        try:\n",
        "            response_str = self.llm_service.query_llm(prompt)\n",
        "            data = json.loads(response_str)\n",
        "            if data and GraphSchema.JSON_KEY_PART_IDS in data and data[GraphSchema.JSON_KEY_PART_IDS]:\n",
        "                rels = [{GraphSchema.JSON_KEY_PART_ID: pid, GraphSchema.JSON_KEY_WHOLE_ID: whole_candidate_node.get(GraphSchema.PROP_ID)} for pid in data[GraphSchema.JSON_KEY_PART_IDS]]\n",
        "                logging.debug(f\"LLM identified that '{whole_candidate_node.get(GraphSchema.PROP_ID)}' has parts: {data[GraphSchema.JSON_KEY_PART_IDS]}.\")\n",
        "                return rels\n",
        "        except (json.JSONDecodeError, KeyError) as e:\n",
        "            logging.warning(f\"Could not parse 'has part' response or key error: {e}\")\n",
        "        return []\n",
        "\n",
        "    def _refine_type_system(self, deleted_nodes_in_cycle: set):\n",
        "        with log_step(\"Refining type system\"):\n",
        "            try:\n",
        "                all_types = self.graph_db.reader.select_random_entities(GraphSchema.NODE_LABEL_TYPE, self.ltm_hierarchy_sample_size, self.experiment_id, self.run_timestamp)\n",
        "                types = [t for t in all_types if t.get(GraphSchema.PROP_ID) not in deleted_nodes_in_cycle]\n",
        "\n",
        "                if len(types) < 2: return 0, set()\n",
        "\n",
        "                types_for_prompt = [{GraphSchema.PROP_NAME: t.get(GraphSchema.PROP_NAME), GraphSchema.PROP_ID: t.get(GraphSchema.PROP_ID)} for t in types if t.get(GraphSchema.PROP_NAME) not in [GraphSchema.CANONICAL_NAME_THING, GraphSchema.CANONICAL_NAME_SOURCE]]\n",
        "                if len(types_for_prompt) < 2: return 0, set()\n",
        "\n",
        "                prompt = self.config.Q_MTT + json.dumps(types_for_prompt)\n",
        "                response_str = self.llm_service.query_llm(prompt)\n",
        "                merge_data = json.loads(response_str)\n",
        "\n",
        "                if merge_data and GraphSchema.JSON_KEY_MERGE_PAIRS in merge_data:\n",
        "                    merged, removed_ids = self.graph_db.refiner.merge_entities(merge_data, self.experiment_id, self.run_timestamp)\n",
        "                    logging.info(f\"Refined {merged} pairs of types.\")\n",
        "                    return merged, removed_ids\n",
        "                else:\n",
        "                    logging.info(\"Refined no pairs of types.\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error during type system refinement: {e}\")\n",
        "        return 0, set()\n",
        "\n",
        "    def _organize_ontology_hierarchy(self, deleted_nodes_in_cycle: set):\n",
        "        with log_step(\"Organizing ontology hierarchy\"):\n",
        "            try:\n",
        "                all_types = self.graph_db.reader.select_random_entities(GraphSchema.NODE_LABEL_TYPE, self.ltm_hierarchy_sample_size, self.experiment_id, self.run_timestamp)\n",
        "                types = [t for t in all_types if t.get(GraphSchema.PROP_ID) not in deleted_nodes_in_cycle]\n",
        "\n",
        "                if len(types) < 2: return\n",
        "\n",
        "                # Create a map of ID to display name for logging\n",
        "                id_to_name_map = {t.get(GraphSchema.PROP_ID): t.get(GraphSchema.PROP_DISPLAY_NAME) for t in types}\n",
        "                id_to_name_map[GraphSchema.CANONICAL_ID_THING] = GraphSchema.CANONICAL_DISPLAY_NAME_THING\n",
        "\n",
        "                type_objects = [{GraphSchema.PROP_ID: t.get(GraphSchema.PROP_ID), GraphSchema.PROP_NAME: t.get(GraphSchema.PROP_NAME)} for t in types if t.get(GraphSchema.PROP_NAME) != GraphSchema.CANONICAL_NAME_THING]\n",
        "                thing_node_obj = {GraphSchema.PROP_ID: GraphSchema.CANONICAL_ID_THING, GraphSchema.PROP_NAME: GraphSchema.CANONICAL_NAME_THING}\n",
        "                relationships_to_create = []\n",
        "\n",
        "                for child_obj in type_objects:\n",
        "                    if child_obj[GraphSchema.PROP_ID] in deleted_nodes_in_cycle: continue\n",
        "\n",
        "                    potential_parents = [p for p in type_objects if p[GraphSchema.PROP_ID] != child_obj[GraphSchema.PROP_ID] and p[GraphSchema.PROP_ID] not in deleted_nodes_in_cycle] + [thing_node_obj]\n",
        "                    prompt = self.config.Q_OOH.format(child_name=child_obj[GraphSchema.PROP_NAME], child_id=child_obj[GraphSchema.PROP_ID], potential_parents=json.dumps(potential_parents, indent=2))\n",
        "                    response_str = self.llm_service.query_llm(prompt)\n",
        "\n",
        "                    try:\n",
        "                        data = json.loads(response_str)\n",
        "                        if data.get(\"child_id\") and data.get(\"parent_id\"):\n",
        "                            parent_id = data[\"parent_id\"]\n",
        "                            child_id = data[\"child_id\"]\n",
        "                            if parent_id in [p[GraphSchema.PROP_ID] for p in potential_parents]:\n",
        "                                parent_name = id_to_name_map.get(parent_id, \"N/A\")\n",
        "                                child_name = id_to_name_map.get(child_id, \"N/A\")\n",
        "                                relationships_to_create.append({\n",
        "                                    \"parent_id\": parent_id, \"child_id\": child_id,\n",
        "                                    \"parent_name\": parent_name, \"child_name\": child_name\n",
        "                                })\n",
        "                    except json.JSONDecodeError:\n",
        "                        logging.warning(f\"Could not parse JSON for ontology hierarchy: {response_str}\")\n",
        "\n",
        "                if relationships_to_create:\n",
        "                    created = self.graph_db.refiner.create_and_relate_type_entities_by_id(relationships_to_create)\n",
        "                    logging.info(f\"Organized {created} new hierarchical relationships.\")\n",
        "                else:\n",
        "                    logging.info(\"Organized no new hierarchical relationships.\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error during ontology organization: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "    def _correct_instance_types(self, deleted_nodes_in_cycle: set):\n",
        "        with log_step(\"Correcting instance types\"):\n",
        "            reclassified_count = 0\n",
        "            try:\n",
        "                all_instances = self.graph_db.reader.select_random_entities(None, self.ltm_merge_sample_size, self.experiment_id, self.run_timestamp)\n",
        "                instances = [i for i in all_instances if i.get(GraphSchema.PROP_ID) not in deleted_nodes_in_cycle]\n",
        "                types = self.graph_db.reader.select_random_entities(GraphSchema.NODE_LABEL_TYPE, 100, self.experiment_id, self.run_timestamp)\n",
        "\n",
        "                if not instances or not types: return\n",
        "\n",
        "                available_types_data = [{GraphSchema.PROP_ID: t.get(GraphSchema.PROP_ID), GraphSchema.PROP_NAME: t.get(GraphSchema.PROP_NAME)} for t in types]\n",
        "\n",
        "                for instance in instances:\n",
        "                    if instance.get(GraphSchema.PROP_ID) in deleted_nodes_in_cycle: continue\n",
        "                    if GraphSchema.NODE_LABEL_TYPE in instance.labels or GraphSchema.NODE_LABEL_SOURCE in instance.labels: continue\n",
        "\n",
        "                    excluded_labels = {GraphSchema.NODE_LABEL_NODE, GraphSchema.NODE_LABEL_DO_NOT_CHANGE, GraphSchema.NODE_LABEL_INSTANCE}\n",
        "                    primary_label = next((l for l in instance.labels if not l.startswith((GraphDBBase.EXP_PREFIX, GraphDBBase.RUN_PREFIX)) and l not in excluded_labels), GraphSchema.CANONICAL_DISPLAY_NAME_THING)\n",
        "\n",
        "                    instance_json = json.dumps({\"instance_id\": instance.get(GraphSchema.PROP_ID), \"instance_name\": instance.get(GraphSchema.PROP_DISPLAY_NAME), \"current_type\": primary_label})\n",
        "                    prompt = self.config.Q_ITC.format(instance_json=instance_json, types_json=json.dumps(available_types_data, indent=2))\n",
        "                    response_str = self.llm_service.query_llm(prompt)\n",
        "\n",
        "                    try:\n",
        "                        data = json.loads(response_str)\n",
        "                        if data and data.get(\"instance_id\") and data.get(\"new_type_id\"):\n",
        "                            reclassified_count += self.graph_db.refiner.reclassify_instance(data[\"instance_id\"], data[\"new_type_id\"], self.experiment_id, self.run_timestamp)\n",
        "                    except json.JSONDecodeError:\n",
        "                        logging.error(f\"Could not parse type correction JSON: {response_str}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Error during instance type correction: {e}\", exc_info=True)\n",
        "            finally:\n",
        "                logging.info(f\"Corrected {reclassified_count} instance types.\")\n",
        "\n",
        "    def _merge_similar_instances(self, deleted_nodes_in_cycle: set):\n",
        "        with log_step(\"Merging similar instances\"):\n",
        "            try:\n",
        "                all_instances_raw = self.graph_db.reader.get_random_instances_with_types(self.ltm_merge_sample_size, self.experiment_id, self.run_timestamp)\n",
        "                all_instances = [i for i in all_instances_raw if i.get(\"id\") not in deleted_nodes_in_cycle]\n",
        "\n",
        "                if len(all_instances) < 2:\n",
        "                    logging.info(\"Not enough instances to compare for merging.\")\n",
        "                    return 0, set()\n",
        "\n",
        "                logging.info(f\"Processing {len(all_instances)} instances in a single batch for merging...\")\n",
        "                prompt = self.config.Q_MIE + json.dumps(all_instances)\n",
        "\n",
        "                try:\n",
        "                    response_str = self.llm_service.query_llm(prompt)\n",
        "                    try:\n",
        "                        merge_data = json.loads(response_str)\n",
        "                    except json.JSONDecodeError:\n",
        "                        logging.warning(\"Initial JSON parsing failed for merge batch. Retrying with self-correction...\")\n",
        "                        correction_prompt = f\"The following text is not valid JSON. Please fix it and return only the corrected JSON object.\\\\n\\\\n{response_str}\"\n",
        "                        corrected_response_str = self.llm_service.query_llm(correction_prompt)\n",
        "                        try:\n",
        "                            merge_data = json.loads(corrected_response_str)\n",
        "                            logging.info(\"Successfully self-corrected JSON response.\")\n",
        "                        except json.JSONDecodeError as final_e:\n",
        "                            logging.error(f\"Self-correction for merge batch failed: {final_e}. Skipping merge operation.\")\n",
        "                            merge_data = {}\n",
        "\n",
        "                    if merge_data and GraphSchema.JSON_KEY_MERGE_PAIRS in merge_data and merge_data[GraphSchema.JSON_KEY_MERGE_PAIRS]:\n",
        "                        merged_count, removed_ids = self.graph_db.refiner.merge_entities(merge_data, self.experiment_id, self.run_timestamp)\n",
        "                        if merged_count > 0:\n",
        "                            logging.info(f\"Merged {merged_count} pairs of instances.\")\n",
        "                        return merged_count, removed_ids\n",
        "                    else:\n",
        "                        logging.info(\"No similar instances found to merge.\")\n",
        "\n",
        "                except MaxTokensExceededError as e:\n",
        "                    logging.error(f\"A token limit error occurred while processing instances for merging. The dataset may be too large for a single API call. Error: {e}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"An unexpected error occurred during instance merging: {e}\", exc_info=True)\n",
        "        return 0, set()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjJhEiiZlaYH"
      },
      "source": [
        "## Memory Manager Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 469,
      "metadata": {
        "id": "madKwS2mqoln"
      },
      "outputs": [],
      "source": [
        "class MemoryManager:\n",
        "    \"\"\"\n",
        "    Manages short-term memory and its consolidation to long-term memory.\n",
        "\n",
        "    Attributes:\n",
        "        short_term_memory (list): A list storing elements representing short-term memory.\n",
        "        db_writer (GraphDBWriter): An instance of GraphDBWriter for LTM write operations.\n",
        "    \"\"\"\n",
        "    def __init__(self, db_writer: GraphDBWriter, stm_threshold: int):\n",
        "        \"\"\"\n",
        "        Initializes the MemoryManager.\n",
        "\n",
        "        Args:\n",
        "            db_writer (GraphDBWriter): The database writer for LTM operations.\n",
        "            stm_threshold (int): The character count threshold for STM consolidation.\n",
        "        \"\"\"\n",
        "        self.short_term_memory = []\n",
        "        self.db_writer = db_writer\n",
        "        self.stm_threshold = stm_threshold\n",
        "\n",
        "    def add_to_stm(self, item_type: str, value):\n",
        "        \"\"\"Adds an item to the short-term memory.\"\"\"\n",
        "        self.short_term_memory.append({\"Type\": item_type, \"Value\": value})\n",
        "\n",
        "    def consolidate_stm_to_ltm(self, experiment_id: str, run_timestamp: str):\n",
        "        \"\"\"\n",
        "        Consolidates declarative statements from STM to LTM by writing them to the database.\n",
        "        After consolidation, declarative statements are removed from STM.\n",
        "        \"\"\"\n",
        "        logging.info(\"Consolidating Short-Term Memory to Long-Term Memory.\")\n",
        "        declarative_statements = [\n",
        "            element[\"Value\"] for element in self.short_term_memory\n",
        "            if isinstance(element, dict) and element.get(\"Type\") == MindConfig.DECL_STMT\n",
        "        ]\n",
        "\n",
        "        for kg_data in declarative_statements:\n",
        "            self.db_writer.kg_to_ltm(kg_data, experiment_id, run_timestamp)\n",
        "\n",
        "        # Keep only non-declarative elements\n",
        "        self.short_term_memory = [\n",
        "            element for element in self.short_term_memory\n",
        "            if not (isinstance(element, dict) and element.get(\"Type\") == MindConfig.DECL_STMT)\n",
        "        ]\n",
        "        logging.info(\"STM to LTM consolidation complete.\")\n",
        "\n",
        "    def should_consolidate(self) -> bool:\n",
        "        \"\"\"Checks if STM has reached its consolidation threshold.\"\"\"\n",
        "        stm_size = sum(len(str(e.get(\"Value\", \"\"))) for e in self.short_term_memory if e.get(\"Type\") == MindConfig.DECL_STMT)\n",
        "        return stm_size > self.stm_threshold"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7AsdmIclhuj"
      },
      "source": [
        "## Mind Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Main Orchestrator Class ---\n",
        "class Mind:\n",
        "    \"\"\"\n",
        "    Orchestrates document processing, memory management, and knowledge graph construction.\n",
        "    Delegates specific tasks to STMProcessor and LTMConsolidator.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: str, llm_model: str, temperature: float, max_output_tokens: int, graph_db: GraphDB, llm_api_key: str,\n",
        "                 stm_threshold: int, ltm_merge_sample_size: int, ltm_hierarchy_sample_size: int, max_chunks_to_process: int,\n",
        "                 num_refinement_cycles: int, experiment_id: str, run_timestamp: str,\n",
        "                 part_of_sample_size: int, part_of_candidate_pool_size: int, output_path: Path): # NEW: Accept output_path\n",
        "        \"\"\"Initializes the Mind and its specialized processing components.\"\"\"\n",
        "        self.config = MindConfig()\n",
        "        self.experiment_id = experiment_id\n",
        "        self.run_timestamp = run_timestamp\n",
        "        self.max_chunks_to_process = max_chunks_to_process\n",
        "        self.output_path = output_path\n",
        "\n",
        "        # Initialize services and specialized processors\n",
        "        self.llm_service = LLMService(llm, llm_model, llm_api_key, temperature, max_output_tokens)\n",
        "        self.memory_manager = MemoryManager(graph_db.writer, stm_threshold)\n",
        "        self.stm_processor = STMProcessor(self.llm_service)\n",
        "        self.ltm_consolidator = LTMConsolidator(\n",
        "            self.llm_service, graph_db, experiment_id, run_timestamp,\n",
        "            num_refinement_cycles, ltm_merge_sample_size, ltm_hierarchy_sample_size,\n",
        "            part_of_sample_size, part_of_candidate_pool_size\n",
        "        )\n",
        "        if llm == \"OpenAI\":\n",
        "            self.batch_processor = OpenAIBatchProcessor(self.llm_service.model, self.output_path)\n",
        "\n",
        "        logging.info(f\"Mind initialized with model: {llm} / {llm_model}\")\n",
        "\n",
        "\n",
        "    ### CHANGE START ###\n",
        "    # This entire method is refactored for clarity and to handle the asyncio event loop correctly.\n",
        "    def doc_to_stm(self, document: Document, use_batch: bool = False):\n",
        "        \"\"\"Processes a document and populates Short-Term Memory.\"\"\"\n",
        "        try:\n",
        "            if use_batch:\n",
        "                if self.llm_service.llm == \"OpenAI\":\n",
        "                    self._process_batch_openai(document)\n",
        "                elif self.llm_service.llm == \"Gemini\":\n",
        "                    # Add the import statement here\n",
        "                    import asyncio\n",
        "                    # For Jupyter/Colab environments, we need to get the existing event loop\n",
        "                    try:\n",
        "                        loop = asyncio.get_running_loop()\n",
        "                    except RuntimeError:  # 'RuntimeError: There is no current event loop...'\n",
        "                        loop = asyncio.new_event_loop()\n",
        "                        asyncio.set_event_loop(loop)\n",
        "\n",
        "                    if loop.is_running():\n",
        "                         # If the loop is already running, we can't use asyncio.run().\n",
        "                         # We create a task and use a helper to run it.\n",
        "                        logging.info(\"Detected running event loop. Scheduling async tasks.\")\n",
        "                        task = loop.create_task(self._process_batch_gemini(document))\n",
        "                        # This is a simple way to wait for the task in a Jupyter-like env\n",
        "                        # A more robust solution might use nest_asyncio if this causes issues.\n",
        "                        loop.run_until_complete(task)\n",
        "                    else:\n",
        "                        asyncio.run(self._process_batch_gemini(document))\n",
        "                else:\n",
        "                    logging.warning(f\"Batch processing not supported for {self.llm_service.llm}. Falling back to synchronous mode.\")\n",
        "                    self._process_synchronous(document)\n",
        "            else:\n",
        "                self._process_synchronous(document)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred during doc_to_stm processing: {e}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def _process_synchronous(self, document: Document):\n",
        "        \"\"\"Handles the standard, one-by-one processing of document chunks.\"\"\"\n",
        "        chunks_processed = 0\n",
        "        for text_chunk, start, end in document:\n",
        "            if self.max_chunks_to_process and chunks_processed >= self.max_chunks_to_process:\n",
        "                logging.info(f\"Reached max chunks limit of {self.max_chunks_to_process}.\")\n",
        "                break\n",
        "\n",
        "            logging.info(f\"Processing chunk {chunks_processed + 1} (paragraphs {start} to {end} of {document.get_num_paragraphs()}).\")\n",
        "            normalized_json_str = self.stm_processor.process_chunk(\n",
        "                text_chunk=text_chunk,\n",
        "                chunk_num=chunks_processed + 1,\n",
        "                file_name=document.get_file_name()\n",
        "            )\n",
        "            self._add_to_memory(normalized_json_str)\n",
        "            chunks_processed += 1\n",
        "        logging.info(f\"Finished processing document to STM. Processed {chunks_processed} chunks.\")\n",
        "\n",
        "    def _process_batch_openai(self, document: Document):\n",
        "        \"\"\"Handles the file-based batch processing workflow for OpenAI.\"\"\"\n",
        "        chunks_processed = 0\n",
        "        prompts_for_batch = []\n",
        "        for text_chunk, start, end in document:\n",
        "            if self.max_chunks_to_process and chunks_processed >= self.max_chunks_to_process:\n",
        "                logging.info(f\"Reached max chunks limit of {self.max_chunks_to_process}.\")\n",
        "                break\n",
        "            prompt = (\n",
        "                self.config.Q_KG_BASE.format(chunk=chunks_processed + 1) +\n",
        "                self.config.JSON_SCHEMA +\n",
        "                self.config.Q_KG_SUFFIX +\n",
        "                text_chunk\n",
        "            )\n",
        "            custom_id = f\"chunk_{chunks_processed + 1}\"\n",
        "            prompts_for_batch.append(self.llm_service.prepare_batch_request(prompt, custom_id))\n",
        "            chunks_processed += 1\n",
        "\n",
        "        if prompts_for_batch:\n",
        "            logging.info(f\"Submitting {len(prompts_for_batch)} chunks to OpenAI Batch API.\")\n",
        "            batch_file_name = f\"{self.experiment_id}_batch_input.jsonl\"\n",
        "            results_file_path = self.batch_processor.process_batch(prompts_for_batch, batch_file_name)\n",
        "            with open(results_file_path, 'r') as f:\n",
        "                for line in f:\n",
        "                    result_data = json.loads(line)\n",
        "                    custom_id = result_data.get('custom_id')\n",
        "                    chunk_num = int(custom_id.split('_')[-1])\n",
        "                    response_body = result_data.get('response', {}).get('body', {})\n",
        "                    response_str = response_body.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
        "                    if response_str:\n",
        "                        augmented_data = self.stm_processor._augment_graph_data(json.loads(response_str), document.get_file_name(), chunk_num)\n",
        "                        normalized_json_str = self.stm_processor._normalize_graph(augmented_data)\n",
        "                        self._add_to_memory(normalized_json_str)\n",
        "        logging.info(f\"Finished processing OpenAI batch for {chunks_processed} chunks.\")\n",
        "\n",
        "    async def _process_batch_gemini(self, document: Document):\n",
        "        \"\"\"Handles the asynchronous concurrent processing workflow for Gemini.\"\"\"\n",
        "        import asyncio\n",
        "        tasks = []\n",
        "        chunks_processed = 0\n",
        "        for text_chunk, start, end in document:\n",
        "            if self.max_chunks_to_process and chunks_processed >= self.max_chunks_to_process:\n",
        "                logging.info(f\"Reached max chunks limit of {self.max_chunks_to_process}.\")\n",
        "                break\n",
        "\n",
        "            chunk_num = chunks_processed + 1\n",
        "            prompt = (\n",
        "                self.config.Q_KG_BASE.format(chunk=chunk_num) +\n",
        "                self.config.JSON_SCHEMA +\n",
        "                self.config.Q_KG_SUFFIX +\n",
        "                text_chunk\n",
        "            )\n",
        "            tasks.append(self.llm_service.query_llm_async(prompt, chunk_num))\n",
        "            chunks_processed += 1\n",
        "\n",
        "        if tasks:\n",
        "            logging.info(f\"Running {len(tasks)} Gemini requests concurrently.\")\n",
        "            results = await asyncio.gather(*tasks)\n",
        "            for chunk_num, response_str in sorted(results, key=lambda x: x[0]): # Sort results to maintain order\n",
        "                if response_str:\n",
        "                    try:\n",
        "                        # Use the synchronous stm_processor methods to handle the results\n",
        "                        augmented_data = self.stm_processor._augment_graph_data(json.loads(response_str), document.get_file_name(), chunk_num)\n",
        "                        normalized_json_str = self.stm_processor._normalize_graph(augmented_data)\n",
        "                        self._add_to_memory(normalized_json_str)\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Failed to process result for chunk {chunk_num}: {e}\")\n",
        "        logging.info(f\"Finished processing Gemini batch for {chunks_processed} chunks.\")\n",
        "\n",
        "    def _add_to_memory(self, normalized_json_str: str):\n",
        "        \"\"\"Helper method to add a processed KG fragment to memory and consolidate if needed.\"\"\"\n",
        "        self.memory_manager.add_to_stm(self.config.DECL_STMT, normalized_json_str)\n",
        "        if self.memory_manager.should_consolidate():\n",
        "            self.memory_manager.consolidate_stm_to_ltm(self.experiment_id, self.run_timestamp)\n",
        "    ### CHANGE END ###\n",
        "\n",
        "    def stm_to_ltm(self):\n",
        "        \"\"\"Consolidates all items from STM into the LTM graph database.\"\"\"\n",
        "        self.memory_manager.consolidate_stm_to_ltm(self.experiment_id, self.run_timestamp)\n",
        "\n",
        "    def consolidate_ltm(self):\n",
        "        \"\"\"Starts the LTM consolidation and refinement process.\"\"\"\n",
        "        self.ltm_consolidator.run_consolidation()"
      ],
      "metadata": {
        "id": "7An_ndVI-h5I"
      },
      "execution_count": 470,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 471,
      "metadata": {
        "id": "s-qABwUs2pnA"
      },
      "outputs": [],
      "source": [
        "# --- Main Orchestrator Class ---\n",
        "class Mindold:\n",
        "    \"\"\"\n",
        "    Orchestrates document processing, memory management, and knowledge graph construction.\n",
        "    Delegates specific tasks to STMProcessor and LTMConsolidator.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm: str, llm_model: str, temperature: float, max_output_tokens: int, graph_db: GraphDB, llm_api_key: str,\n",
        "                 stm_threshold: int, ltm_merge_sample_size: int, ltm_hierarchy_sample_size: int, max_chunks_to_process: int,\n",
        "                 num_refinement_cycles: int, experiment_id: str, run_timestamp: str,\n",
        "                 part_of_sample_size: int, part_of_candidate_pool_size: int, output_path: Path): # NEW: Accept output_path\n",
        "        \"\"\"Initializes the Mind and its specialized processing components.\"\"\"\n",
        "        self.config = MindConfig()\n",
        "        self.experiment_id = experiment_id\n",
        "        self.run_timestamp = run_timestamp\n",
        "        self.max_chunks_to_process = max_chunks_to_process\n",
        "        self.output_path = output_path\n",
        "\n",
        "        # Initialize services and specialized processors\n",
        "        self.llm_service = LLMService(llm, llm_model, llm_api_key, temperature, max_output_tokens)\n",
        "        self.memory_manager = MemoryManager(graph_db.writer, stm_threshold)\n",
        "        self.stm_processor = STMProcessor(self.llm_service)\n",
        "        self.ltm_consolidator = LTMConsolidator(\n",
        "            self.llm_service, graph_db, experiment_id, run_timestamp,\n",
        "            num_refinement_cycles, ltm_merge_sample_size, ltm_hierarchy_sample_size,\n",
        "            part_of_sample_size, part_of_candidate_pool_size  # NEW: Pass to consolidator\n",
        "        )\n",
        "        if llm == \"OpenAI\":\n",
        "            self.batch_processor = OpenAIBatchProcessor(self.llm_service.model, self.output_path)\n",
        "\n",
        "        logging.info(f\"Mind initialized with model: {llm} / {llm_model}\")\n",
        "\n",
        "    def doc_to_stm(self, document: Document, use_batch: bool = False):\n",
        "        \"\"\"Processes a document and populates Short-Term Memory.\"\"\"\n",
        "        try:\n",
        "            chunks_processed = 0\n",
        "            prompts_for_batch = []\n",
        "\n",
        "            if use_batch and self.llm_service.llm == \"OpenAI\":\n",
        "                # Batch processing workflow for OpenAI\n",
        "                for text_chunk, start, end in document:\n",
        "                    if self.max_chunks_to_process and chunks_processed >= self.max_chunks_to_process:\n",
        "                        logging.info(f\"Reached max chunks limit of {self.max_chunks_to_process}.\")\n",
        "                        break\n",
        "\n",
        "                    prompt = (\n",
        "                        self.config.Q_KG_BASE.format(chunk=chunks_processed + 1) +\n",
        "                        self.config.JSON_SCHEMA +\n",
        "                        self.config.Q_KG_SUFFIX +\n",
        "                        text_chunk\n",
        "                    )\n",
        "                    custom_id = f\"chunk_{chunks_processed + 1}\"\n",
        "                    prompts_for_batch.append(self.llm_service.prepare_batch_request(prompt, custom_id))\n",
        "                    chunks_processed += 1\n",
        "\n",
        "                if prompts_for_batch:\n",
        "                    batch_file_name = f\"{self.experiment_id}_batch_input.jsonl\"\n",
        "                    results_file_path = self.batch_processor.process_batch(prompts_for_batch, batch_file_name)\n",
        "\n",
        "                    with open(results_file_path, 'r') as f:\n",
        "                        for line in f:\n",
        "                            result_data = json.loads(line)\n",
        "                            custom_id = result_data.get('custom_id')\n",
        "                            chunk_num = int(custom_id.split('_')[-1])\n",
        "\n",
        "                            response_body = result_data.get('response', {}).get('body', {})\n",
        "                            response_str = response_body.get('choices', [{}])[0].get('message', {}).get('content', '')\n",
        "\n",
        "                            if response_str:\n",
        "                                augmented_data = self.stm_processor._augment_graph_data(json.loads(response_str), document.get_file_name(), chunk_num)\n",
        "                                normalized_json_str = self.stm_processor._normalize_graph(augmented_data)\n",
        "                                self.memory_manager.add_to_stm(self.config.DECL_STMT, normalized_json_str)\n",
        "\n",
        "            else:\n",
        "                # Synchronous processing workflow\n",
        "                for text_chunk, start, end in document:\n",
        "                    if self.max_chunks_to_process and chunks_processed >= self.max_chunks_to_process:\n",
        "                        logging.info(f\"Reached max chunks limit of {self.max_chunks_to_process}.\")\n",
        "                        break\n",
        "\n",
        "                    logging.info(f\"Processing chunk {chunks_processed + 1} (paragraphs {start} to {end} of {document.get_num_paragraphs()}).\")\n",
        "\n",
        "                    normalized_json_str = self.stm_processor.process_chunk(\n",
        "                        text_chunk=text_chunk,\n",
        "                        chunk_num=chunks_processed + 1,\n",
        "                        file_name=document.get_file_name()\n",
        "                    )\n",
        "                    self.memory_manager.add_to_stm(self.config.DECL_STMT, normalized_json_str)\n",
        "                    chunks_processed += 1\n",
        "\n",
        "            logging.info(f\"Finished processing document to STM. Processed {chunks_processed} chunks.\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An error occurred during doc_to_stm processing: {e}\", exc_info=True)\n",
        "            raise\n",
        "\n",
        "    def stm_to_ltm(self):\n",
        "        \"\"\"Consolidates all items from STM into the LTM graph database.\"\"\"\n",
        "        self.memory_manager.consolidate_stm_to_ltm(self.experiment_id, self.run_timestamp)\n",
        "\n",
        "    def consolidate_ltm(self):\n",
        "        \"\"\"Starts the LTM consolidation and refinement process.\"\"\"\n",
        "        self.ltm_consolidator.run_consolidation()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLBnXLPV3Cur"
      },
      "source": [
        "# Experiment Classes\n",
        "This section encapsulates the logic for defining and running experiments. The ExperimentConfig class is responsible for parsing the parameters for a single experiment and setting up the necessary configuration. The Experiment class takes this configuration and orchestrates the entire workflow, from document processing to knowledge graph consolidation and results collection. This object-oriented approach simplifies the main execution block and makes the process more modular and maintainable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Y3ABo5S3HqC"
      },
      "source": [
        "##Experiment Configuration Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 472,
      "metadata": {
        "id": "urhKizid3PHj"
      },
      "outputs": [],
      "source": [
        "# --- CONFIGURATION AND EXPERIMENT CLASSES ---\n",
        "\n",
        "class ExperimentConfig:\n",
        "    \"\"\"\n",
        "    Holds all configuration for a single, specific experiment run.\n",
        "    Static settings are class attributes, dynamic ones are set in __init__.\n",
        "    \"\"\"\n",
        "    # --- Static Configurations (Don't change between experiments) ---\n",
        "    # Use the universal 'secrets' object which works in both environments\n",
        "    NEO4J_URI = secrets.get(\"NEO4J_URI\")\n",
        "    NEO4J_AUTH = (secrets.get(\"NEO4J_USERNAME\"), secrets.get(\"NEO4J_PASSWORD\"))\n",
        "    GEMINI_API_KEY = secrets.get('GOOGLE_API_KEY')\n",
        "    OPENAI_API_KEY = secrets.get('OPENAI_API_KEY')\n",
        "\n",
        "    LLM_PROVIDERS = {\n",
        "        \"Gemini\": {\"models\": [\"gemini-2.5-pro\", \"gemini-2.5-flash\", \"gemini-2.0-pro\"]},\n",
        "        \"OpenAI\": {\"models\": [\"gpt-4.1\", \"gpt-4.1-mini\", \"gpt-4.1-nano\", \"gpt-4o\", \"gpt-4-turbo\", \"gpt-3.5-turbo\"]},\n",
        "    }\n",
        "\n",
        "    def __init__(self, experiment_params: dict, group_params: dict, location_map: dict, root_path: Path, run_output_path: Path):\n",
        "        \"\"\"\n",
        "        Initializes the configuration for a single experiment from a dictionary.\n",
        "        \"\"\"\n",
        "        self.ROOT_PATH = root_path\n",
        "        self.GROUP_ID = group_params.get(\"group_id\")\n",
        "        self.EXPERIMENT_ID = experiment_params.get(\"experiment_id\")\n",
        "        self.run_timestamp_str = group_params.get(\"run_timestamp\")\n",
        "\n",
        "        # --- Execution Flags & Pipeline Control ---\n",
        "        self.RUN_EXPERIMENT = experiment_params.get(\"run_experiment\", True)\n",
        "        self.RUN_MIND_WORKFLOW = experiment_params.get(\"run_workflow\", True)\n",
        "        self.CLEAR_DATABASE = experiment_params.get(\"clear_database\", True)\n",
        "        self.USE_BATCH_PROCESSING = experiment_params.get(\"use_batch_processing\", False)\n",
        "\n",
        "\n",
        "        # --- Dynamic Hyperparameters ---\n",
        "        self.EXPERIMENT_NAME = experiment_params.get(\"experiment_name\", \"Unnamed Experiment\")\n",
        "        self.DESCRIPTION = experiment_params.get(\"description\", \"\")\n",
        "\n",
        "        # --- Path Resolution Logic ---\n",
        "        input_loc_name = group_params.get(\"input_location\")\n",
        "        diagram_loc_name = group_params.get(\"diagram_location\")\n",
        "\n",
        "        self.INPUT_PATH = self.ROOT_PATH / location_map[input_loc_name]['path'] if input_loc_name else self.ROOT_PATH\n",
        "        self.OUTPUT_PATH = run_output_path\n",
        "\n",
        "        diagram_relative_path = location_map.get(diagram_loc_name, {}).get('path')\n",
        "        self.DIAGRAM_PATH = run_output_path / diagram_relative_path if diagram_relative_path else run_output_path\n",
        "\n",
        "\n",
        "        doc_file_ref = experiment_params.get(\"doc_file_name\")\n",
        "        self.DOC_FILE_PATH = self.INPUT_PATH / location_map[doc_file_ref]['path'] if doc_file_ref else None\n",
        "\n",
        "        # --- MODIFICATION START ---\n",
        "        # Handle single or multiple ER files, storing both path and use\n",
        "        er_file_ref = experiment_params.get(\"er_file_name\")\n",
        "        self.ER_FILES_INFO = []\n",
        "        if isinstance(er_file_ref, list):\n",
        "            for ref in er_file_ref:\n",
        "                info = location_map.get(ref)\n",
        "                if info:\n",
        "                    self.ER_FILES_INFO.append({'path': self.INPUT_PATH / info['path'], 'use': info['use']})\n",
        "        elif er_file_ref:\n",
        "            info = location_map.get(er_file_ref)\n",
        "            if info:\n",
        "                self.ER_FILES_INFO.append({'path': self.INPUT_PATH / info['path'], 'use': info['use']})\n",
        "\n",
        "        self.RUN_ER_TEST = bool(self.ER_FILES_INFO)\n",
        "        # --- MODIFICATION END ---\n",
        "\n",
        "\n",
        "        self.PROCESSING_STAGE = experiment_params.get(\"processing_stage\")\n",
        "        self.LLM_PROVIDER = experiment_params.get(\"llm_provider\")\n",
        "        self.LLM_MODEL = experiment_params.get(\"llm_model\")\n",
        "        self.TEMPERATURE = experiment_params.get(\"temperature\")\n",
        "        self.DOC_CHUNK_SIZE = experiment_params.get(\"doc_chunk_size\")\n",
        "        self.DOC_CHUNK_OVERLAP = experiment_params.get(\"doc_chunk_overlap\")\n",
        "        self.MAX_CHUNKS_TO_PROCESS = experiment_params.get(\"max_chunks_to_process\")\n",
        "\n",
        "        self.MAX_OUTPUT_TOKENS = experiment_params.get(\"llm_max_output_tokens\", experiment_params.get(\"max_output_tokens\", 8192))\n",
        "\n",
        "        self.STM_FULLNESS_THRESHOLD = experiment_params.get(\"stm_fullness_threshold\", 10000)\n",
        "        self.LTM_MERGE_SAMPLE_SIZE = experiment_params.get(\"ltm_merge_sample_size\", 20)\n",
        "        self.LTM_HIERARCHY_SAMPLE_SIZE = experiment_params.get(\"ltm_hierarchy_sample_size\", 15)\n",
        "        self.NUM_REFINEMENT_CYCLES = experiment_params.get(\"num_refinement_cycles\", 3)\n",
        "        self.PART_OF_SAMPLE_SIZE = experiment_params.get(\"part_of_sample_size\", 10)\n",
        "        self.PART_OF_CANDIDATE_POOL_SIZE = experiment_params.get(\"part_of_candidate_pool_size\", 50)\n",
        "\n",
        "        # --- Dynamic Credential Loading ---\n",
        "        if self.LLM_PROVIDER == \"Gemini\":\n",
        "            self.LLM_API_KEY = self.GEMINI_API_KEY\n",
        "        elif self.LLM_PROVIDER == \"OpenAI\":\n",
        "            self.LLM_API_KEY = self.OPENAI_API_KEY\n",
        "        else:\n",
        "            self.LLM_API_KEY = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wAK6KjctCofR"
      },
      "source": [
        "## LaTeX Report Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 473,
      "metadata": {
        "id": "X0wgs70GClve"
      },
      "outputs": [],
      "source": [
        "# --- NEW: LaTeX Report Generator ---\n",
        "class LatexGenerator:\n",
        "    \"\"\"\n",
        "    Generates a LaTeX summary report from the experiment results.\n",
        "    \"\"\"\n",
        "    def __init__(self, run_output_path: Path, run_id: str, experiment_groups: list, all_summaries: list):\n",
        "        self.output_path = run_output_path\n",
        "        self.run_id = run_id\n",
        "        self.groups = experiment_groups\n",
        "        self.summaries = all_summaries\n",
        "        self.latex_content = []\n",
        "\n",
        "    @staticmethod\n",
        "    def _escape_latex(text: str) -> str:\n",
        "        \"\"\"Escapes special LaTeX characters in a string.\"\"\"\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        return text.replace('\\\\', r'\\\\') \\\n",
        "                   .replace('{', r'\\{') \\\n",
        "                   .replace('}', r'\\}') \\\n",
        "                   .replace('#', r'\\#') \\\n",
        "                   .replace('$', r'\\$') \\\n",
        "                   .replace('%', r'\\%') \\\n",
        "                   .replace('&', r'\\&') \\\n",
        "                   .replace('_', r'\\_') \\\n",
        "                   .replace('^', r'\\^') \\\n",
        "                   .replace('~', r'\\textasciitilde{}')\n",
        "\n",
        "    def _add_line(self, line=\"\"):\n",
        "        \"\"\"Adds a line of text to the LaTeX content.\"\"\"\n",
        "        self.latex_content.append(line)\n",
        "\n",
        "    def _generate_header(self):\n",
        "        \"\"\"Generates the LaTeX document preamble.\"\"\"\n",
        "        self._add_line(r\"\\chapter{Experimental Results}\")\n",
        "\n",
        "    def _generate_introduction(self):\n",
        "        \"\"\"Generates the introduction section with a list of all experiments.\"\"\"\n",
        "        self._add_line(r\"\\section{Overview of Experiments}\")\n",
        "        self._add_line(\"This document details the results of the experimental run conducted on \\\\today.\")\n",
        "        self._add_line(\"The following experiment groups and individual experiments were executed:\")\n",
        "        self._add_line(r\"\\begin{itemize}\")\n",
        "        for group in self.groups:\n",
        "            # --- MODIFICATION START ---\n",
        "            # Only include groups that are configured to generate output in the report.\n",
        "            if group.get(\"run_group\") and group.get(\"generate_output\", True):\n",
        "            # --- MODIFICATION END ---\n",
        "                self._add_line(r\"  \\item \\textbf{Group: \" + self._escape_latex(group.get('group_name', 'N/A')) + \"}\")\n",
        "                self._add_line(r\"  \\begin{itemize}\")\n",
        "                for exp in group.get(\"experiments\", []):\n",
        "                    if exp.get(\"run_experiment\"):\n",
        "                        self._add_line(r\"    \\item \" + self._escape_latex(exp.get('experiment_name', 'N/A')))\n",
        "                self._add_line(r\"  \\end{itemize}\")\n",
        "        self._add_line(r\"\\end{itemize}\")\n",
        "\n",
        "    def _generate_summary_table(self):\n",
        "        \"\"\"Generates a summary table of key results for all experiments.\"\"\"\n",
        "        self._add_line(r\"\\section{Summary of Results}\")\n",
        "        self._add_line(r\"\\begin{longtable}{p{0.4\\textwidth}rrr}\")\n",
        "        self._add_line(r\"\\toprule\")\n",
        "        self._add_line(r\"\\textbf{Experiment} & \\textbf{Overall Score} & \\textbf{Entity F1} & \\textbf{Relationship F1} \\\\\")\n",
        "        self._add_line(r\"\\midrule\")\n",
        "        self._add_line(r\"\\endfirsthead\")\n",
        "        self._add_line(r\"\\toprule\")\n",
        "        self._add_line(r\"\\textbf{Experiment} & \\textbf{Overall Score} & \\textbf{Entity F1} & \\textbf{Relationship F1} \\\\\")\n",
        "        self._add_line(r\"\\midrule\")\n",
        "        self._add_line(r\"\\endhead\")\n",
        "        for summary in self.summaries:\n",
        "            if summary.get('status') == 'Success':\n",
        "                name = self._escape_latex(summary.get('experiment_name', 'N/A'))\n",
        "                overall = self._escape_latex(summary.get('overall_score', 'N/A'))\n",
        "                entity_f1 = self._escape_latex(summary.get('entity_f1_score', 'N/A'))\n",
        "                rel_f1 = self._escape_latex(summary.get('relationship_f1_score', 'N/A'))\n",
        "                self._add_line(f\"{name} & {overall} & {entity_f1} & {rel_f1} \\\\\\\\\")\n",
        "        self._add_line(r\"\\bottomrule\")\n",
        "        self._add_line(r\"\\caption{Summary of key performance metrics for each successful experiment.}\")\n",
        "        self._add_line(r\"\\end{longtable}\")\n",
        "\n",
        "    def _generate_group_sections(self):\n",
        "        \"\"\"Generates a section for each experiment group.\"\"\"\n",
        "        for group in self.groups:\n",
        "            # --- MODIFICATION START ---\n",
        "            # Only include groups that are configured to generate output in the report.\n",
        "            if group.get(\"run_group\") and group.get(\"generate_output\", True):\n",
        "            # --- MODIFICATION END ---\n",
        "                group_name = self._escape_latex(group.get('group_name', 'N/A'))\n",
        "                self._add_line(r\"\\clearpage\")\n",
        "                self._add_line(r\"\\section{\" + group_name + \"}\")\n",
        "                for exp_params in group.get(\"experiments\", []):\n",
        "                    if exp_params.get(\"run_experiment\"):\n",
        "                        summary = next((s for s in self.summaries if s['experiment_id'] == exp_params['experiment_id']), None)\n",
        "                        if summary:\n",
        "                            self._generate_experiment_subsection(exp_params, summary)\n",
        "\n",
        "    def _generate_experiment_subsection(self, exp_params, summary):\n",
        "        \"\"\"Generates a subsection for a single experiment.\"\"\"\n",
        "        exp_name = self._escape_latex(exp_params.get('experiment_name', 'N/A'))\n",
        "        self._add_line(r\"\\subsection{\" + exp_name + \"}\")\n",
        "        self._add_line(self._escape_latex(exp_params.get('description', 'No description.')))\n",
        "        self._add_line()\n",
        "\n",
        "        # Include figures\n",
        "        self._add_line(r\"\\begin{figure}[!ht]\")\n",
        "        self._add_line(r\"  \\centering\")\n",
        "        img_path_base = f\"{exp_params['experiment_id']}\"\n",
        "        self._add_line(r\"  \\includegraphics[width=0.48\\textwidth]{\" + self._escape_latex(f\"figures/appendix_fig/{img_path_base}_input_wordcloud.png\") + \"}\")\n",
        "        self._add_line(r\"  \\includegraphics[width=0.48\\textwidth]{\" + self._escape_latex(f\"figures/appendix_fig/{img_path_base}_entity_wordcloud.png\") + \"}\")\n",
        "        self._add_line(r\"  \\includegraphics[width=0.48\\textwidth]{\" + self._escape_latex(f\"figures/appendix_fig/{img_path_base}_relationship_wordcloud.png\") + \"}\")\n",
        "        self._add_line(r\"  \\includegraphics[width=0.48\\textwidth]{\" + self._escape_latex(f\"figures/appendix_fig/{img_path_base}_ontology_graph.png\") + \"}\")\n",
        "        self._add_line(r\"  \\caption{Visualizations for \" + exp_name + \". Top-left: Input Text. Top-right: Extracted Entities. Bottom-left: Relationship Types. Bottom-right: Type Ontology.}\")\n",
        "        self._add_line(r\"\\end{figure}\")\n",
        "        self._add_line(r\"\\clearpage\")\n",
        "\n",
        "\n",
        "        # Hyperparameters Table\n",
        "        self._add_line(r\"\\subsubsection{Hyperparameters}\")\n",
        "        self._add_line(r\"\\begin{tabular}{ll}\")\n",
        "        self._add_line(r\"\\toprule\")\n",
        "        self._add_line(r\"\\textbf{Parameter} & \\textbf{Value} \\\\\")\n",
        "        self._add_line(r\"\\midrule\")\n",
        "        h_params = ['llm_provider', 'llm_model', 'temperature', 'chunk_size', 'chunk_overlap', 'num_refinement_cycles']\n",
        "        for p in h_params:\n",
        "            self._add_line(f\"{self._escape_latex(p.replace('_', ' ').title())} & {self._escape_latex(summary.get(p, 'N/A'))} \\\\\\\\\")\n",
        "        self._add_line(r\"\\bottomrule\")\n",
        "        self._add_line(r\"\\end{tabular}\")\n",
        "        self._add_line()\n",
        "\n",
        "        # Results Table\n",
        "        self._add_line(r\"\\subsubsection{Results}\")\n",
        "        self._add_line(r\"\\begin{tabular}{ll}\")\n",
        "        self._add_line(r\"\\toprule\")\n",
        "        self._add_line(r\"\\textbf{Metric} & \\textbf{Value} \\\\\")\n",
        "        self._add_line(r\"\\midrule\")\n",
        "        r_params = ['status', 'duration_seconds', 'final_nodes', 'final_relationships', 'overall_score', 'entity_f1_score', 'relationship_f1_score']\n",
        "        for p in r_params:\n",
        "             self._add_line(f\"{self._escape_latex(p.replace('_', ' ').title())} & {self._escape_latex(summary.get(p, 'N/A'))} \\\\\\\\\")\n",
        "        self._add_line(r\"\\bottomrule\")\n",
        "        self._add_line(r\"\\end{tabular}\")\n",
        "\n",
        "    def _generate_footer(self):\n",
        "        \"\"\"Generates the end of the LaTeX document.\"\"\"\n",
        "        self._add_line(r\"% Done.\")\n",
        "\n",
        "    def generate_report(self):\n",
        "        \"\"\"Generates the full LaTeX report and saves it to a file.\"\"\"\n",
        "        self._generate_header()\n",
        "        self._generate_introduction()\n",
        "        self._generate_summary_table()\n",
        "        self._generate_group_sections()\n",
        "        self._generate_footer()\n",
        "\n",
        "        latex_str = \"\\n\".join(self.latex_content)\n",
        "        file_path = self.output_path / Experiment.FNAME_LATEX_SUMMARY\n",
        "        with open(file_path, 'w') as f:\n",
        "            f.write(latex_str)\n",
        "        logging.info(f\"Successfully saved LaTeX summary to: {file_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4b07MWJ3v69"
      },
      "source": [
        "## Ground Truth Generator Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- NEW: Ground Truth Generator using LangExtract ---\n",
        "class GroundTruthGenerator:\n",
        "    \"\"\"\n",
        "    A utility class to generate a ground truth JSON file from a source document\n",
        "    using Google's LangExtract library and a Python-based transformer.\n",
        "    \"\"\"\n",
        "    def __init__(self, llm_service: LLMService):\n",
        "        \"\"\"\n",
        "        Initializes the generator. The llm_service is needed for the lx.extract call.\n",
        "        \"\"\"\n",
        "        self.llm_service = llm_service\n",
        "        self.config = MindConfig()\n",
        "\n",
        "    def _normalize_and_save_json(self, data: dict, output_filepath: Path):\n",
        "        \"\"\"\n",
        "        Applies the name/displayName normalization and saves the final JSON file.\n",
        "        \"\"\"\n",
        "        for node in data.get('nodes', []):\n",
        "            if 'properties' in node:\n",
        "                props = node['properties']\n",
        "                displayName = props.get('displayName')\n",
        "                name = props.get('name')\n",
        "                if displayName and name is None:\n",
        "                    props['name'] = str(displayName).lower()\n",
        "                elif name and displayName is None:\n",
        "                    props['displayName'] = str(name).title()\n",
        "\n",
        "        with open(output_filepath, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data, f, indent=2)\n",
        "        logging.info(f\"Successfully saved normalized ground truth to: {output_filepath}\")\n",
        "\n",
        "    def _convert_langextract_to_mnemosyne(self, raw_extractions: list) -> dict:\n",
        "        \"\"\"\n",
        "        Converts the native output from LangExtract into the Mnemosyne JSON format.\n",
        "        \"\"\"\n",
        "        nodes = []\n",
        "        relationships = []\n",
        "        type_nodes_map = {}\n",
        "        instance_name_to_id = {}\n",
        "        relationship_extractions = []\n",
        "        node_counter = 0\n",
        "\n",
        "        # --- First Pass: Create all entity and type nodes ---\n",
        "        for extraction in raw_extractions:\n",
        "            attributes = extraction.attributes or {}\n",
        "            if 'source_entity' in attributes and 'target_entity' in attributes:\n",
        "                relationship_extractions.append(extraction)\n",
        "                continue\n",
        "\n",
        "            instance_id = f\"c0-node-{node_counter}\"\n",
        "            display_name = extraction.extraction_text\n",
        "            instance_props = { \"name\": display_name.lower(), \"displayName\": display_name, **attributes }\n",
        "            nodes.append({ \"id\": instance_id, \"labels\": [\"Instance\"], \"properties\": instance_props })\n",
        "            instance_name_to_id[display_name] = instance_id\n",
        "            node_counter += 1\n",
        "\n",
        "            type_name_raw = extraction.extraction_class\n",
        "            type_name_pascal = Utils._to_pascal_case(type_name_raw)\n",
        "            if type_name_pascal not in type_nodes_map:\n",
        "                type_id = f\"c0-node-{node_counter}\"\n",
        "                type_nodes_map[type_name_pascal] = type_id\n",
        "                nodes.append({\n",
        "                    \"id\": type_id,\n",
        "                    \"labels\": [\"Type\"],\n",
        "                    \"properties\": { \"name\": type_name_pascal.lower(), \"displayName\": type_name_pascal }\n",
        "                })\n",
        "                node_counter += 1\n",
        "            else:\n",
        "                type_id = type_nodes_map[type_name_pascal]\n",
        "\n",
        "            relationships.append({ \"source\": instance_id, \"target\": type_id, \"type\": \"IS_A\" })\n",
        "\n",
        "        # --- MODIFICATION START ---\n",
        "        # --- Second Pass: Create semantic relationships, creating missing nodes on the fly ---\n",
        "        for rel_extraction in relationship_extractions:\n",
        "            attrs = rel_extraction.attributes\n",
        "            source_name = attrs.get('source_entity')\n",
        "            target_name = attrs.get('target_entity')\n",
        "            rel_type = attrs.get('relationship_type')\n",
        "\n",
        "            if not all([source_name, target_name, rel_type]):\n",
        "                logging.warning(f\"Could not create relationship for extraction '{rel_extraction.extraction_text}' due to incomplete attributes: {attrs}\")\n",
        "                continue\n",
        "\n",
        "            # Check for source node, create if it doesn't exist\n",
        "            source_id = instance_name_to_id.get(source_name)\n",
        "            if not source_id:\n",
        "                logging.warning(f\"Source entity '{source_name}' not found as a node. Creating it now.\")\n",
        "                source_id = f\"c0-node-{node_counter}\"\n",
        "                nodes.append({\n",
        "                    \"id\": source_id,\n",
        "                    \"labels\": [\"Instance\"],\n",
        "                    \"properties\": {\"name\": source_name.lower(), \"displayName\": source_name}\n",
        "                })\n",
        "                instance_name_to_id[source_name] = source_id\n",
        "                node_counter += 1\n",
        "                # Link the newly created node to a generic \"Thing\" type\n",
        "                thing_type_name = \"Thing\"\n",
        "                if thing_type_name not in type_nodes_map:\n",
        "                    type_id = f\"c0-node-{node_counter}\"\n",
        "                    type_nodes_map[thing_type_name] = type_id\n",
        "                    nodes.append({\"id\": type_id, \"labels\": [\"Type\"], \"properties\": {\"name\": \"thing\", \"displayName\": \"Thing\"}})\n",
        "                    node_counter += 1\n",
        "                relationships.append({\"source\": source_id, \"target\": type_nodes_map[thing_type_name], \"type\": \"IS_A\"})\n",
        "\n",
        "            # Check for target node, create if it doesn't exist\n",
        "            target_id = instance_name_to_id.get(target_name)\n",
        "            if not target_id:\n",
        "                logging.warning(f\"Target entity '{target_name}' not found as a node. Creating it now.\")\n",
        "                target_id = f\"c0-node-{node_counter}\"\n",
        "                nodes.append({\n",
        "                    \"id\": target_id,\n",
        "                    \"labels\": [\"Instance\"],\n",
        "                    \"properties\": {\"name\": target_name.lower(), \"displayName\": target_name}\n",
        "                })\n",
        "                instance_name_to_id[target_name] = target_id\n",
        "                node_counter += 1\n",
        "                # Link the newly created node to a generic \"Thing\" type\n",
        "                thing_type_name = \"Thing\"\n",
        "                if thing_type_name not in type_nodes_map:\n",
        "                    type_id = f\"c0-node-{node_counter}\"\n",
        "                    type_nodes_map[thing_type_name] = type_id\n",
        "                    nodes.append({\"id\": type_id, \"labels\": [\"Type\"], \"properties\": {\"name\": \"thing\", \"displayName\": \"Thing\"}})\n",
        "                    node_counter += 1\n",
        "                relationships.append({\"source\": target_id, \"target\": type_nodes_map[thing_type_name], \"type\": \"IS_A\"})\n",
        "\n",
        "            # Create the final relationship\n",
        "            relationships.append({\n",
        "                \"source\": source_id,\n",
        "                \"target\": target_id,\n",
        "                \"type\": Utils._to_upper_snake_case(rel_type)\n",
        "            })\n",
        "        # --- MODIFICATION END ---\n",
        "\n",
        "        return {\"nodes\": nodes, \"relationships\": relationships}\n",
        "\n",
        "    def generate_from_docx(self, source_filepath: Path, output_filepath: Path):\n",
        "        \"\"\"\n",
        "        Reads a .docx file, extracts entities, transforms the output, saves the\n",
        "        final JSON, and returns statistics about the conversion.\n",
        "        \"\"\"\n",
        "        with log_step(f\"Generating LangExtract Ground Truth for {source_filepath.name}\"):\n",
        "            try:\n",
        "                # 1. Load Document\n",
        "                with Document(source_filepath) as doc:\n",
        "                    text_content = doc.get_all_text()\n",
        "\n",
        "                if not text_content:\n",
        "                    logging.error(\"Source document is empty. Aborting generation.\")\n",
        "                    return None\n",
        "\n",
        "                # 2. Call LangExtract with a simple prompt\n",
        "                logging.info(\"Extracting entities and relations with LangExtract...\")\n",
        "                examples = [\n",
        "                    lx.data.ExampleData(\n",
        "                        text=\"The Board of Supervisors shall govern the affairs of the township and may make regulations as necessary to provide for the health, safety and welfare of its citizens, as authorized by the Second Class Township Code.\",\n",
        "                        extractions=[\n",
        "                            lx.data.Extraction(extraction_class=\"governing_body\", extraction_text=\"Board of Supervisors\"),\n",
        "                            lx.data.Extraction(extraction_class=\"action\", extraction_text=\"govern the affairs of the township\"),\n",
        "                            lx.data.Extraction(\n",
        "                                extraction_class=\"relationship\",\n",
        "                                extraction_text=\"Board of Supervisors shall govern\",\n",
        "                                attributes={\n",
        "                                    \"source_entity\": \"Board of Supervisors\",\n",
        "                                    \"relationship_type\": \"has_power_to\",\n",
        "                                    \"target_entity\": \"govern the affairs of the township\"\n",
        "                                }\n",
        "                            )\n",
        "                        ]\n",
        "                    ),\n",
        "                    lx.data.ExampleData(\n",
        "                        text=\"The Township Manager shall be the Chief Administrative Officer of the Township, responsible to the Board of Supervisors for the proper administration of all affairs of the Township.\",\n",
        "                        extractions=[\n",
        "                            lx.data.Extraction(extraction_class=\"official\", extraction_text=\"Township Manager\"),\n",
        "                            lx.data.Extraction(extraction_class=\"governing_body\", extraction_text=\"Board of Supervisors\"),\n",
        "                            lx.data.Extraction(\n",
        "                                extraction_class=\"relationship\",\n",
        "                                extraction_text=\"responsible to the Board of Supervisors\",\n",
        "                                attributes={\n",
        "                                    \"source_entity\": \"Township Manager\",\n",
        "                                    \"relationship_type\": \"is_responsible_to\",\n",
        "                                    \"target_entity\": \"Board of Supervisors\"\n",
        "                                }\n",
        "                            )\n",
        "                        ]\n",
        "                    ),\n",
        "                    lx.data.ExampleData(\n",
        "                        text=\"The Zoning Officer shall administer and enforce the provisions of the Zoning Ordinance of the Township of Easttown.\",\n",
        "                        extractions=[\n",
        "                            lx.data.Extraction(extraction_class=\"official\", extraction_text=\"Zoning Officer\"),\n",
        "                            lx.data.Extraction(extraction_class=\"document\", extraction_text=\"Zoning Ordinance of the Township of Easttown\"),\n",
        "                            lx.data.Extraction(\n",
        "                                extraction_class=\"relationship\",\n",
        "                                extraction_text=\"Zoning Officer shall administer and enforce the provisions of the Zoning Ordinance\",\n",
        "                                attributes={\n",
        "                                    \"source_entity\": \"Zoning Officer\",\n",
        "                                    \"relationship_type\": \"enforces\",\n",
        "                                    \"target_entity\": \"Zoning Ordinance of the Township of Easttown\"\n",
        "                                }\n",
        "                            )\n",
        "                        ]\n",
        "                    ),\n",
        "                    lx.data.ExampleData(\n",
        "                        text=\"The Historical Commission is authorized to prepare and recommend to the Board of Supervisors the adoption of a historical preservation plan.\",\n",
        "                        extractions=[\n",
        "                            lx.data.Extraction(extraction_class=\"committee\", extraction_text=\"Historical Commission\"),\n",
        "                            lx.data.Extraction(extraction_class=\"governing_body\", extraction_text=\"Board of Supervisors\"),\n",
        "                            lx.data.Extraction(extraction_class=\"plan\", extraction_text=\"a historical preservation plan\"),\n",
        "                            lx.data.Extraction(\n",
        "                                extraction_class=\"relationship\",\n",
        "                                extraction_text=\"recommend to the Board of Supervisors\",\n",
        "                                attributes={\n",
        "                                    \"source_entity\": \"Historical Commission\",\n",
        "                                    \"relationship_type\": \"recommends_to\",\n",
        "                                    \"target_entity\": \"Board of Supervisors\"\n",
        "                                }\n",
        "                            )\n",
        "                        ]\n",
        "                    ),\n",
        "                    lx.data.ExampleData(\n",
        "                        text=\"All ordinances shall be recorded in the ordinance book of the Township within 30 days after the same shall become effective.\",\n",
        "                        extractions=[\n",
        "                            lx.data.Extraction(extraction_class=\"document_type\", extraction_text=\"All ordinances\"),\n",
        "                            lx.data.Extraction(extraction_class=\"record_book\", extraction_text=\"the ordinance book of the Township\"),\n",
        "                            lx.data.Extraction(extraction_class=\"time_requirement\", extraction_text=\"within 30 days\"),\n",
        "                            lx.data.Extraction(\n",
        "                                extraction_class=\"relationship\",\n",
        "                                extraction_text=\"shall be recorded in the ordinance book\",\n",
        "                                attributes={\n",
        "                                    \"source_entity\": \"All ordinances\",\n",
        "                                    \"relationship_type\": \"must_be_recorded_in\",\n",
        "                                    \"target_entity\": \"the ordinance book of the Township\"\n",
        "                                }\n",
        "                            )\n",
        "                        ]\n",
        "                    )\n",
        "                ]\n",
        "\n",
        "                annotated_doc = lx.extract(\n",
        "                    text_or_documents=text_content,\n",
        "                    prompt_description=self.config.Q_LE,\n",
        "                    examples=examples,\n",
        "                    model_id=self.llm_service.model_name,\n",
        "                    api_key=self.llm_service.api_key,\n",
        "                    fence_output=False\n",
        "                )\n",
        "\n",
        "                extractions_list = annotated_doc.extractions\n",
        "                if not extractions_list:\n",
        "                    raise ValueError(\"LangExtract did not return any extractions.\")\n",
        "\n",
        "                # 3. Count raw LangExtract outputs\n",
        "                le_node_count = 0\n",
        "                le_rel_count = 0\n",
        "                for extraction in extractions_list:\n",
        "                    attributes = extraction.attributes or {}\n",
        "                    if 'source_entity' in attributes and 'target_entity' in attributes:\n",
        "                        le_rel_count += 1\n",
        "                    else:\n",
        "                        le_node_count += 1\n",
        "                logging.info(f\"LangExtract discovered {le_node_count:,} potential nodes and {le_rel_count:,} potential relationships.\")\n",
        "\n",
        "                # 4. Transform the output\n",
        "                logging.info(f\"Converting {len(extractions_list):,} LangExtract items to Mnemosyne format...\")\n",
        "                mnemosyne_json = self._convert_langextract_to_mnemosyne(extractions_list)\n",
        "\n",
        "                # 5. Count final Mnemosyne outputs\n",
        "                mn_node_count = len(mnemosyne_json.get('nodes', []))\n",
        "                mn_rel_count = len(mnemosyne_json.get('relationships', []))\n",
        "                logging.info(f\"Converted to {mn_node_count:,} nodes and {mn_rel_count:,} relationships in Mnemosyne format.\")\n",
        "\n",
        "                # 6. Apply Final Normalization and Save File\n",
        "                self._normalize_and_save_json(mnemosyne_json, output_filepath)\n",
        "\n",
        "                # 7. Return the statistics for logging in the experiment record\n",
        "                return {\n",
        "                    \"le_node_count\": le_node_count,\n",
        "                    \"le_rel_count\": le_rel_count,\n",
        "                    \"mn_node_count\": mn_node_count,\n",
        "                    \"mn_rel_count\": mn_rel_count\n",
        "                }\n",
        "\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to generate ground truth file: {e}\", exc_info=True)\n",
        "                raise"
      ],
      "metadata": {
        "id": "djg0IVctsb4m"
      },
      "execution_count": 474,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IJnC56Vaazl3"
      },
      "source": [
        "## Diagram Generator Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 475,
      "metadata": {
        "id": "FlJJltyoaxn2"
      },
      "outputs": [],
      "source": [
        "# --- NEW: Diagram Generator Class ---\n",
        "class DiagramGenerator:\n",
        "    \"\"\"Handles the creation and saving of all visual diagrams for an experiment.\"\"\"\n",
        "\n",
        "    def __init__(self, diagram_path: Path, experiment_id: str, run_timestamp: str, graph_db_reader: 'GraphDBReader'):\n",
        "        self.diagram_path = diagram_path\n",
        "        self.experiment_id = experiment_id\n",
        "        self.run_timestamp = run_timestamp\n",
        "        self.reader = graph_db_reader\n",
        "        # Ensure the directory exists\n",
        "        self.diagram_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    def _generate_and_save_wordcloud(self, text: str, filename: str):\n",
        "        \"\"\"Helper function to generate and save a word cloud image.\"\"\"\n",
        "        if not text:\n",
        "            logging.warning(f\"Cannot generate word cloud for {filename}, no text provided.\")\n",
        "            return\n",
        "        try:\n",
        "            wordcloud = WordCloud(width=800, height=400, background_color='white').generate(text)\n",
        "            plt.figure(figsize=(10, 5))\n",
        "            plt.imshow(wordcloud, interpolation='bilinear')\n",
        "            plt.axis(\"off\")\n",
        "\n",
        "            output_path = self.diagram_path / filename\n",
        "            plt.savefig(output_path, bbox_inches='tight')\n",
        "            plt.close()\n",
        "            logging.info(f\"Saved word cloud to {output_path}\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Failed to generate word cloud for {filename}: {e}\", exc_info=True)\n",
        "\n",
        "    def generate_input_word_cloud(self, document: 'Document'):\n",
        "        \"\"\"Generates a word cloud from the source document's text.\"\"\"\n",
        "        with log_step(\"Generating input document word cloud\"):\n",
        "            full_text = document.get_all_text()\n",
        "            filename = f\"{self.experiment_id}_input_wordcloud.png\"\n",
        "            self._generate_and_save_wordcloud(full_text, filename)\n",
        "\n",
        "    def generate_entity_word_cloud(self):\n",
        "        \"\"\"Generates a word cloud from the displayName of all instance nodes.\"\"\"\n",
        "        with log_step(\"Generating extracted entity word cloud\"):\n",
        "            entity_names = self.reader.get_all_instance_display_names(self.experiment_id, self.run_timestamp)\n",
        "            text = ' '.join(entity_names)\n",
        "            filename = f\"{self.experiment_id}_entity_wordcloud.png\"\n",
        "            self._generate_and_save_wordcloud(text, filename)\n",
        "\n",
        "    def generate_relationship_word_cloud(self):\n",
        "        \"\"\"Generates a word cloud from the types of all relationships.\"\"\"\n",
        "        with log_step(\"Generating relationship type word cloud\"):\n",
        "            rel_types = self.reader.get_all_relationship_types(self.experiment_id, self.run_timestamp)\n",
        "            text = ' '.join(rel_types)\n",
        "            filename = f\"{self.experiment_id}_relationship_wordcloud.png\"\n",
        "            self._generate_and_save_wordcloud(text, filename)\n",
        "\n",
        "    def generate_ontology_graph(self):\n",
        "        \"\"\"Generates and saves a pyvis graph of the Type ontology.\"\"\"\n",
        "        with log_step(\"Generating ontology type graph\"):\n",
        "            graph_data = self.reader.get_ontology_graph_data(self.experiment_id, self.run_timestamp)\n",
        "\n",
        "            if not graph_data:\n",
        "                logging.warning(\"No ontology data found to generate a graph.\")\n",
        "                return\n",
        "\n",
        "            import networkx as nx\n",
        "            G = nx.DiGraph()\n",
        "            node_labels = {}\n",
        "            for n, r, m in graph_data:\n",
        "                if n:\n",
        "                    n_label = n.get('displayName', n.get('id'))\n",
        "                    node_labels[n_label] = n_label\n",
        "                    G.add_node(n_label)\n",
        "                if m:\n",
        "                    m_label = m.get('displayName', m.get('id'))\n",
        "                    node_labels[m_label] = m_label\n",
        "                    G.add_node(m_label)\n",
        "                if n and m and r:\n",
        "                    G.add_edge(n_label, m_label, label=r.type)\n",
        "\n",
        "            if not G.nodes():\n",
        "                logging.warning(\"Ontology graph is empty after processing data.\")\n",
        "                return\n",
        "\n",
        "            plt.figure(figsize=(16, 12))\n",
        "            pos = nx.spring_layout(G, k=0.9, iterations=50, seed=42) # Use a seed for reproducibility\n",
        "            nx.draw(G, pos, labels=node_labels, with_labels=True, node_size=3000, node_color=\"#a0cbe2\", font_size=10, width=0.5, edge_color=\"grey\")\n",
        "            edge_labels = nx.get_edge_attributes(G, 'label')\n",
        "            nx.draw_networkx_edge_labels(G, pos, edge_labels=edge_labels, font_color='red', font_size=8)\n",
        "            plt.title(f\"Type Ontology for {self.experiment_id}\", size=15)\n",
        "\n",
        "            filename = f\"{self.experiment_id}_ontology_graph.png\"\n",
        "            output_path = self.diagram_path / filename\n",
        "            try:\n",
        "                plt.savefig(output_path, bbox_inches='tight', dpi=300)\n",
        "                plt.close()\n",
        "                logging.info(f\"Saved ontology graph to {output_path}\")\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to save ontology graph PNG: {e}\", exc_info=True)\n",
        "                plt.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQRVO0chI3sy"
      },
      "source": [
        "## Test Result Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 476,
      "metadata": {
        "id": "Dk64HrKmI1tQ"
      },
      "outputs": [],
      "source": [
        "## Test Result Class\n",
        "# --- NEW CLASS FOR STORING TEST RESULTS ---\n",
        "class TestResult:\n",
        "    \"\"\"\n",
        "    A simple data class to hold the results of an ER test, including expanded statistics.\n",
        "    \"\"\"\n",
        "    # --- MODIFICATION START ---\n",
        "    # The constructor now accepts unique counts and the ground truth source.\n",
        "    def __init__(self, er_entities, matched_entities, er_relationships, matched_relationships,\n",
        "                 total_discovered_unique_entities, total_discovered_unique_relationships, duplicate_entity_names,\n",
        "                 ground_truth_source: str):\n",
        "        self.er_entities = er_entities\n",
        "        self.matched_entities = matched_entities # This is the same as True Positives (TP) for entities\n",
        "        self.er_relationships = er_relationships\n",
        "        self.matched_relationships = matched_relationships # This is the same as True Positives (TP) for relationships\n",
        "        self.total_discovered_unique_entities = total_discovered_unique_entities\n",
        "        self.total_discovered_unique_relationships = total_discovered_unique_relationships\n",
        "        self.duplicate_entity_names = duplicate_entity_names\n",
        "        self.ground_truth_source = ground_truth_source # NEW: Store the source file name\n",
        "\n",
        "        # --- FIX: Calculate False Positives (FP) and False Negatives (FN) using UNIQUE counts ---\n",
        "        self.fp_entities = self.total_discovered_unique_entities - self.matched_entities\n",
        "        self.fn_entities = self.er_entities - self.matched_entities\n",
        "        self.fp_relationships = self.total_discovered_unique_relationships - self.matched_relationships\n",
        "        self.fn_relationships = self.er_relationships - self.matched_relationships\n",
        "    # --- MODIFICATION END ---\n",
        "\n",
        "\n",
        "    def get_entity_precision(self):\n",
        "        \"\"\"Calculates precision for entities (TP / (TP + FP)).\"\"\"\n",
        "        tp = self.matched_entities\n",
        "        fp = self.fp_entities\n",
        "        if (tp + fp) == 0:\n",
        "            return 0.0\n",
        "        return (tp / (tp + fp)) * 100\n",
        "\n",
        "    def get_entity_recall(self):\n",
        "        \"\"\"Calculates recall for entities (TP / (TP + FN)).\"\"\"\n",
        "        tp = self.matched_entities\n",
        "        fn = self.fn_entities\n",
        "        if (tp + fn) == 0:\n",
        "            return 0.0\n",
        "        return (tp / (tp + fn)) * 100\n",
        "\n",
        "    def get_entity_f1_score(self):\n",
        "        \"\"\"Calculates the F1-score for entities.\"\"\"\n",
        "        precision = self.get_entity_precision() / 100\n",
        "        recall = self.get_entity_recall() / 100\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return (2 * (precision * recall) / (precision + recall)) * 100\n",
        "\n",
        "    def get_relationship_precision(self):\n",
        "        \"\"\"Calculates precision for relationships.\"\"\"\n",
        "        tp = self.matched_relationships\n",
        "        fp = self.fp_relationships\n",
        "        if (tp + fp) == 0:\n",
        "            return 0.0\n",
        "        return (tp / (tp + fp)) * 100\n",
        "\n",
        "    def get_relationship_recall(self):\n",
        "        \"\"\"Calculates recall for relationships.\"\"\"\n",
        "        tp = self.matched_relationships\n",
        "        fn = self.fn_relationships\n",
        "        if (tp + fn) == 0:\n",
        "            return 0.0\n",
        "        return (tp / (tp + fn)) * 100\n",
        "\n",
        "    def get_relationship_f1_score(self):\n",
        "        \"\"\"Calculates the F1-score for relationships.\"\"\"\n",
        "        precision = self.get_relationship_precision() / 100\n",
        "        recall = self.get_relationship_recall() / 100\n",
        "        if precision + recall == 0:\n",
        "            return 0.0\n",
        "        return (2 * (precision * recall) / (precision + recall)) * 100\n",
        "\n",
        "    def get_overall_score(self):\n",
        "        \"\"\"Calculates a simple combined score based on recall.\"\"\"\n",
        "        entity_score = self.get_entity_f1_score()\n",
        "        rel_score = self.get_relationship_f1_score()\n",
        "        # A weighted average could also be used here if relationships are more important.\n",
        "        return (entity_score + rel_score) / 2\n",
        "\n",
        "    def to_dict(self):\n",
        "        \"\"\"Converts the result to a dictionary for easy saving.\"\"\"\n",
        "        # --- MODIFICATION START ---\n",
        "        # Added ground_truth_source to the output dictionary\n",
        "        return {\n",
        "            \"GroundTruthSource\": self.ground_truth_source,\n",
        "            \"TotalDiscoveredUniqueEntities\": self.total_discovered_unique_entities,\n",
        "            \"TotalDiscoveredUniqueRelationships\": self.total_discovered_unique_relationships,\n",
        "            \"GroundTruthEntities\": self.er_entities,\n",
        "            \"GroundTruthRelationships\": self.er_relationships,\n",
        "            \"MatchedEntities (TP)\": self.matched_entities,\n",
        "            \"MatchedRelationships (TP)\": self.matched_relationships,\n",
        "            \"FalsePositiveEntities (FP)\": self.fp_entities,\n",
        "            \"FalseNegativeEntities (FN)\": self.fn_entities,\n",
        "            \"FalsePositiveRelationships (FP)\": self.fp_relationships,\n",
        "            \"FalseNegativeRelationships (FN)\": self.fn_relationships,\n",
        "            \"DuplicateEntityNameCount\": self.duplicate_entity_names,\n",
        "            \"EntityPrecision\": f\"{self.get_entity_precision():.2f}%\",\n",
        "            \"EntityRecall\": f\"{self.get_entity_recall():.2f}%\",\n",
        "            \"EntityF1Score\": f\"{self.get_entity_f1_score():.2f}%\",\n",
        "            \"RelationshipPrecision\": f\"{self.get_relationship_precision():.2f}%\",\n",
        "            \"RelationshipRecall\": f\"{self.get_relationship_recall():.2f}%\",\n",
        "            \"RelationshipF1Score\": f\"{self.get_relationship_f1_score():.2f}%\",\n",
        "            \"OverallScore (Avg F1)\": f\"{self.get_overall_score():.2f}%\"\n",
        "        }\n",
        "        # --- MODIFICATION END ---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q26dmMNp3ajK"
      },
      "source": [
        "## Experiment Class"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Experiment Class\n",
        "class Experiment:\n",
        "    \"\"\"\n",
        "    Encapsulates all logic and data for a single experiment run.\n",
        "    This class is responsible for managing the database connection for its run.\n",
        "    \"\"\"\n",
        "    # --- NEW: Constants for file naming ---\n",
        "    FNAME_MASTER_SUMMARY = \"00_master_run_summary.csv\"\n",
        "    FNAME_GROUP_INFO = \"00_group_info.csv\"\n",
        "    FNAME_GROUP_SUMMARY = \"01_group_summary.csv\"\n",
        "    FNAME_DOC_METRICS = \"02_document_metrics.csv\"\n",
        "    FNAME_DOC_WORDS = \"03_document_words.csv\"\n",
        "    FNAME_KG_METRICS = \"04_kg_metrics.csv\"\n",
        "    FNAME_KG_NODES = \"05_kg_nodes.csv\"\n",
        "    FNAME_KG_RELS = \"06_kg_relationships.csv\"\n",
        "    FNAME_GROUND_TRUTH = \"07_ground_truth_comparison.csv\"\n",
        "    FNAME_LATEX_SUMMARY = \"08_run_summary.tex\" # NEW\n",
        "    VALUE_NA = \"N/A\"\n",
        "\n",
        "    GOLDEN_STANDARD_EXP_ID = \"GOLDEN_STANDARD\"\n",
        "    GOLDEN_STANDARD_RUN_TS = \"STATIC\"\n",
        "\n",
        "    def __init__(self, experiment_params: dict, group_params: dict, location_map: dict, root_path: Path, run_output_path: Path, run_timestamp_str: str):\n",
        "        \"\"\"Initializes the Experiment with its parameters, the global run timestamp, and the group's random seed.\"\"\"\n",
        "        group_params[\"run_timestamp\"] = run_timestamp_str\n",
        "        self.run_timestamp_str = run_timestamp_str\n",
        "        self.random_seed = group_params.get(\"random_seed\", 42)\n",
        "        self.config = ExperimentConfig(experiment_params, group_params, location_map, root_path, run_output_path)\n",
        "        self.result = {}\n",
        "        self.test_results = []\n",
        "        self.pre_consolidation_entities = 0\n",
        "        self.pre_consolidation_relationships = 0\n",
        "        self.diagram_generator = None\n",
        "\n",
        "    def _validate_config(self):\n",
        "        \"\"\"Validates the experiment's LLM and processing stage configuration.\"\"\"\n",
        "        if self.config.RUN_MIND_WORKFLOW:\n",
        "            provider_data = self.config.LLM_PROVIDERS.get(self.config.LLM_PROVIDER)\n",
        "            if not provider_data:\n",
        "                raise ValueError(f\"Invalid LLM Provider: '{self.config.LLM_PROVIDER}'.\")\n",
        "            if self.config.LLM_MODEL not in provider_data[\"models\"]:\n",
        "                raise ValueError(f\"Invalid LLM Model: '{self.config.LLM_MODEL}' for provider '{self.config.LLM_PROVIDER}'.\")\n",
        "            if self.config.PROCESSING_STAGE not in MindConfig.PROCESSING_STAGES:\n",
        "                raise ValueError(f\"Invalid Processing Stage: '{self.config.PROCESSING_STAGE}'.\")\n",
        "        logging.info(f\"Configuration for '{self.config.EXPERIMENT_ID}' validated successfully.\")\n",
        "\n",
        "    def _load_golden_standard(self, graph_db: GraphDB, er_file_path: Path):\n",
        "        \"\"\"Loads a single golden standard knowledge graph from a JSON file into Neo4j.\"\"\"\n",
        "        logging.info(f\"--- Loading Golden Standard KG from {er_file_path.name} ---\")\n",
        "        try:\n",
        "            with open(er_file_path, 'r') as f:\n",
        "                golden_kg_str = f.read()\n",
        "            graph_db.writer.kg_to_ltm(\n",
        "                kg_data_str=golden_kg_str,\n",
        "                experiment_id=self.GOLDEN_STANDARD_EXP_ID,\n",
        "                run_timestamp=self.GOLDEN_STANDARD_RUN_TS\n",
        "            )\n",
        "            logging.info(\"Successfully loaded Golden Standard KG into Neo4j.\")\n",
        "        except FileNotFoundError:\n",
        "            logging.error(f\"Golden standard JSON file not found at {er_file_path}\")\n",
        "            raise\n",
        "        except json.JSONDecodeError as e:\n",
        "            logging.error(f\"Error parsing golden standard JSON file: {e}\")\n",
        "            raise\n",
        "        except Exception as e:\n",
        "            logging.error(f\"An unexpected error occurred while loading the golden standard KG: {e}\")\n",
        "            raise\n",
        "\n",
        "    def _run_er_tests(self, graph_db: GraphDB, a_mind: \"Mind\"):\n",
        "        \"\"\"Runs validation tests against a list of ground truth ER files.\"\"\"\n",
        "        logging.info(f\"--- Starting Ground Truth Comparisons for {self.config.EXPERIMENT_ID} ---\")\n",
        "\n",
        "        for er_file_info in self.config.ER_FILES_INFO:\n",
        "            er_file_path = er_file_info['path']\n",
        "            er_file_use = er_file_info['use']\n",
        "            with log_step(f\"Comparing against '{er_file_path.name}' (Use: {er_file_use})\"):\n",
        "                try:\n",
        "                    # 1. Clear any previous golden standard data\n",
        "                    graph_db.refiner.clear_golden_standard_data()\n",
        "\n",
        "                    # 2. Load the current golden standard\n",
        "                    self._load_golden_standard(graph_db, er_file_path)\n",
        "\n",
        "                    # 3. Run the comparison\n",
        "                    comparison_data = graph_db.reader.get_cypher_comparison_scores(\n",
        "                        experiment_id=self.config.EXPERIMENT_ID,\n",
        "                        run_timestamp=self.run_timestamp_str,\n",
        "                        golden_exp_id=self.GOLDEN_STANDARD_EXP_ID,\n",
        "                        golden_run_ts=self.GOLDEN_STANDARD_RUN_TS\n",
        "                    )\n",
        "\n",
        "                    duplicate_name_count = graph_db.reader.get_duplicate_name_count(self.config.EXPERIMENT_ID, self.run_timestamp_str)\n",
        "\n",
        "                    test_result = TestResult(\n",
        "                        er_entities=comparison_data.get(\"ground_truth_entities\", 0),\n",
        "                        matched_entities=comparison_data.get(\"matched_entities\", 0),\n",
        "                        er_relationships=comparison_data.get(\"ground_truth_relationships\", 0),\n",
        "                        matched_relationships=comparison_data.get(\"matched_relationships\", 0),\n",
        "                        total_discovered_unique_entities=comparison_data.get(\"total_discovered_unique_entities\", 0),\n",
        "                        total_discovered_unique_relationships=comparison_data.get(\"total_discovered_unique_relationships\", 0),\n",
        "                        duplicate_entity_names=duplicate_name_count,\n",
        "                        ground_truth_source=er_file_use\n",
        "                    )\n",
        "                    self.test_results.append(test_result)\n",
        "                    logging.info(f\"Comparison Complete. Overall Score: {test_result.get_overall_score():.2f}%\")\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Failed to run comparison for {er_file_path.name}: {e}\", exc_info=True)\n",
        "\n",
        "    def run(self):\n",
        "        \"\"\"Main execution function for one experiment.\"\"\"\n",
        "        workflow_start_time = datetime.now()\n",
        "        status = \"Success\"\n",
        "        error_message = \"\"\n",
        "        node_count, rel_count = 0, 0\n",
        "        doc_data, doc_words_data, kg_data, kg_nodes_data, kg_rels_data, test_result_data = [], [], [], [], [], []\n",
        "        graph_db = None\n",
        "\n",
        "        if self.config.PROCESSING_STAGE == MindConfig.STAGE_LANGEXTRACT:\n",
        "            logging.info(f\"Executing utility task: Generating Ground Truth with LangExtract.\")\n",
        "            # --- MODIFICATION START ---\n",
        "            # Initialize a dictionary to hold the stats, with default values\n",
        "            le_stats = {\n",
        "                \"le_node_count\": self.VALUE_NA, \"le_rel_count\": self.VALUE_NA,\n",
        "                \"mn_node_count\": self.VALUE_NA, \"mn_rel_count\": self.VALUE_NA\n",
        "            }\n",
        "            # --- MODIFICATION END ---\n",
        "            try:\n",
        "                llm_service = LLMService(\n",
        "                    self.config.LLM_PROVIDER, self.config.LLM_MODEL, self.config.LLM_API_KEY,\n",
        "                    self.config.TEMPERATURE, self.config.MAX_OUTPUT_TOKENS\n",
        "                )\n",
        "                generator = GroundTruthGenerator(llm_service)\n",
        "                output_path = self.config.ER_FILES_INFO[0]['path']\n",
        "                # --- MODIFICATION START ---\n",
        "                # Capture the returned statistics from the generator\n",
        "                le_stats = generator.generate_from_docx(self.config.DOC_FILE_PATH, output_path)\n",
        "                # --- MODIFICATION END ---\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to generate ground truth file: {e}\", exc_info=True)\n",
        "                status = \"Failure\"\n",
        "                error_message = str(e)\n",
        "            duration = datetime.now() - workflow_start_time\n",
        "\n",
        "            config_dict = {\n",
        "                k.lower(): v for k, v in self.config.__dict__.items()\n",
        "                if \"API_KEY\" not in k.upper()\n",
        "            }\n",
        "\n",
        "            summary_result = {\n",
        "                \"status\": status,\n",
        "                \"duration_seconds\": duration.total_seconds(),\n",
        "                \"error\": error_message,\n",
        "                **config_dict,\n",
        "                # --- MODIFICATION START ---\n",
        "                **le_stats  # Add the new stats to the summary dictionary\n",
        "                # --- MODIFICATION END ---\n",
        "            }\n",
        "            return {\"summary\": [summary_result], \"doc_metrics\": [], \"doc_words\": [], \"kg_metrics\": [], \"kg_nodes\": [], \"kg_relationships\": [], \"er_test_results\": []}\n",
        "\n",
        "        if not self.config.RUN_MIND_WORKFLOW:\n",
        "            if self.config.CLEAR_DATABASE:\n",
        "                logging.info(f\"Executing utility task: Clearing database.\")\n",
        "                try:\n",
        "                    graph_db = GraphDB(self.config.NEO4J_URI, self.config.NEO4J_AUTH)\n",
        "                    graph_db.refiner.clear_database()\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Failed to clear database: {e}\")\n",
        "                finally:\n",
        "                    if graph_db:\n",
        "                        graph_db.close()\n",
        "            return None\n",
        "\n",
        "        try:\n",
        "            self._validate_config()\n",
        "            if self.config.PROCESSING_STAGE != MindConfig.STAGE_CONSOLIDATE_ONLY and (not self.config.DOC_FILE_PATH or not self.config.DOC_FILE_PATH.exists()):\n",
        "                raise FileNotFoundError(f\"Document not found at {self.config.DOC_FILE_PATH}\")\n",
        "\n",
        "            graph_db = GraphDB(self.config.NEO4J_URI, self.config.NEO4J_AUTH)\n",
        "            self.diagram_generator = DiagramGenerator(self.config.DIAGRAM_PATH, self.config.EXPERIMENT_ID, self.run_timestamp_str, graph_db.reader)\n",
        "\n",
        "            if self.config.CLEAR_DATABASE:\n",
        "                logging.info(\"Clearing database as per experiment configuration.\")\n",
        "                graph_db.refiner.clear_database()\n",
        "\n",
        "            doc_path_name = self.VALUE_NA\n",
        "            if self.config.PROCESSING_STAGE != MindConfig.STAGE_CONSOLIDATE_ONLY:\n",
        "                with Document(\n",
        "                    self.config.DOC_FILE_PATH,\n",
        "                    firstParagraphSet=self.config.DOC_CHUNK_SIZE,\n",
        "                    remainingParagraphSet=self.config.DOC_CHUNK_SIZE,\n",
        "                    overlap=self.config.DOC_CHUNK_OVERLAP\n",
        "                ) as a_document:\n",
        "                    doc_path_name = a_document.get_file_name()\n",
        "                    self.diagram_generator.generate_input_word_cloud(a_document)\n",
        "\n",
        "                    a_mind = Mind(\n",
        "                        llm=self.config.LLM_PROVIDER, llm_model=self.config.LLM_MODEL,\n",
        "                        temperature=self.config.TEMPERATURE, max_output_tokens=self.config.MAX_OUTPUT_TOKENS,\n",
        "                        graph_db=graph_db, llm_api_key=self.config.LLM_API_KEY,\n",
        "                        stm_threshold=self.config.STM_FULLNESS_THRESHOLD,\n",
        "                        ltm_merge_sample_size=self.config.LTM_MERGE_SAMPLE_SIZE,\n",
        "                        ltm_hierarchy_sample_size=self.config.LTM_HIERARCHY_SAMPLE_SIZE,\n",
        "                        max_chunks_to_process=self.config.MAX_CHUNKS_TO_PROCESS,\n",
        "                        num_refinement_cycles=self.config.NUM_REFINEMENT_CYCLES,\n",
        "                        experiment_id=self.config.EXPERIMENT_ID, run_timestamp=self.run_timestamp_str,\n",
        "                        part_of_sample_size=self.config.PART_OF_SAMPLE_SIZE,\n",
        "                        part_of_candidate_pool_size=self.config.PART_OF_CANDIDATE_POOL_SIZE,\n",
        "                        output_path=self.config.OUTPUT_PATH\n",
        "                    )\n",
        "\n",
        "                    stage = self.config.PROCESSING_STAGE\n",
        "                    use_batch = self.config.USE_BATCH_PROCESSING\n",
        "\n",
        "                    if stage in [MindConfig.STAGE_DOC_TO_STM, MindConfig.STAGE_DOC_TO_LTM, MindConfig.STAGE_FULL_PROCESS]:\n",
        "                        a_mind.doc_to_stm(a_document, use_batch=use_batch)\n",
        "                    if stage in [MindConfig.STAGE_DOC_TO_LTM, MindConfig.STAGE_FULL_PROCESS]:\n",
        "                        a_mind.stm_to_ltm()\n",
        "\n",
        "                    self.pre_consolidation_entities = graph_db.reader.get_discovered_entity_count(self.config.EXPERIMENT_ID, self.run_timestamp_str)\n",
        "                    self.pre_consolidation_relationships = graph_db.reader.get_discovered_relationship_count(self.config.EXPERIMENT_ID, self.run_timestamp_str)\n",
        "\n",
        "                    if stage in [MindConfig.STAGE_FULL_PROCESS, MindConfig.STAGE_CONSOLIDATE_ONLY]:\n",
        "                         a_mind.consolidate_ltm()\n",
        "\n",
        "                    if self.config.RUN_ER_TEST:\n",
        "                        self._run_er_tests(graph_db, a_mind)\n",
        "                        if self.test_results:\n",
        "                            for res in self.test_results:\n",
        "                                test_result_data.append({\"ExperimentID\": self.config.EXPERIMENT_ID, \"RunTimestamp\": self.run_timestamp_str, **res.to_dict()})\n",
        "\n",
        "                    doc_data.append({\"DocumentPath\": a_document.get_file_name(), \"RunTimestamp\": self.run_timestamp_str, \"ExperimentID\": self.config.EXPERIMENT_ID, \"WordCount\": a_document.get_word_count(), \"UniqueWordCount\": a_document.get_unique_word_count(), \"PageCount\": a_document.get_page_count(), \"ParagraphCount\": a_document.get_num_paragraphs(), \"ChapterCount\": a_document.get_chapter_count()})\n",
        "                    word_freq = a_document.get_word_frequencies()\n",
        "                    doc_words_data.extend([{\"DocumentPath\": a_document.get_file_name(), \"RunTimestamp\": self.run_timestamp_str, \"ExperimentID\": self.config.EXPERIMENT_ID, \"Word\": word, \"InstanceCount\": count} for word, count in word_freq.items()])\n",
        "\n",
        "            self.diagram_generator.generate_entity_word_cloud()\n",
        "            self.diagram_generator.generate_relationship_word_cloud()\n",
        "            self.diagram_generator.generate_ontology_graph()\n",
        "\n",
        "            node_count = graph_db.reader.get_node_count(self.config.EXPERIMENT_ID, self.run_timestamp_str)\n",
        "            rel_count = graph_db.reader.get_relationship_count(self.config.EXPERIMENT_ID, self.run_timestamp_str)\n",
        "            kg_data.append({\"DocumentPath\": doc_path_name, \"RunTimestamp\": self.run_timestamp_str, \"ExperimentID\": self.config.EXPERIMENT_ID, \"NodeCount\": node_count, \"RelationshipCount\": rel_count, \"PropertyCount\": graph_db.reader.get_property_count(self.config.EXPERIMENT_ID, self.run_timestamp_str), \"RootNodeCount\": graph_db.reader.get_root_node_count(self.config.EXPERIMENT_ID, self.run_timestamp_str)})\n",
        "            kg_nodes_list = graph_db.reader.get_all_nodes_with_is_a_counts(self.config.EXPERIMENT_ID, self.run_timestamp_str)\n",
        "            kg_nodes_data.extend([{\"DocumentPath\": doc_path_name, \"RunTimestamp\": self.run_timestamp_str, \"ExperimentID\": self.config.EXPERIMENT_ID, **node} for node in kg_nodes_list])\n",
        "            kg_rels_list = graph_db.reader.get_all_relationships(self.config.EXPERIMENT_ID, self.run_timestamp_str)\n",
        "            kg_rels_data.extend([{\"DocumentPath\": doc_path_name, \"RunTimestamp\": self.run_timestamp_str, \"ExperimentID\": self.config.EXPERIMENT_ID, **rel} for rel in kg_rels_list])\n",
        "\n",
        "        except Exception as e:\n",
        "            status = \"Failure\"\n",
        "            error_message = str(e)\n",
        "            logging.error(f\"Workflow for {self.config.EXPERIMENT_ID} FAILED: {e}\", exc_info=True)\n",
        "        finally:\n",
        "            if graph_db:\n",
        "                graph_db.close()\n",
        "\n",
        "        duration = datetime.now() - workflow_start_time\n",
        "\n",
        "        summary_results = []\n",
        "        if not self.test_results:\n",
        "            test_scores = { \"EntityPrecision\": self.VALUE_NA, \"EntityRecall\": self.VALUE_NA, \"EntityF1Score\": self.VALUE_NA, \"RelationshipPrecision\": self.VALUE_NA, \"RelationshipRecall\": self.VALUE_NA, \"RelationshipF1Score\": self.VALUE_NA, \"OverallScore (Avg F1)\": self.VALUE_NA, \"DuplicateEntityNameCount\": self.VALUE_NA, \"GroundTruthSource\": self.VALUE_NA }\n",
        "            base_summary = { \"run_timestamp\": self.run_timestamp_str, \"group_id\": self.config.GROUP_ID, \"experiment_id\": self.config.EXPERIMENT_ID, \"experiment_name\": self.config.EXPERIMENT_NAME, \"description\": self.config.DESCRIPTION, \"document_name\": self.config.DOC_FILE_PATH.name if self.config.DOC_FILE_PATH else self.VALUE_NA, \"status\": status, \"duration_seconds\": duration.total_seconds(), \"llm_provider\": self.config.LLM_PROVIDER, \"llm_model\": self.config.LLM_MODEL, \"temperature\": self.config.TEMPERATURE, \"chunk_size\": self.config.DOC_CHUNK_SIZE, \"chunk_overlap\": self.config.DOC_CHUNK_OVERLAP, \"max_chunks_to_process\": self.config.MAX_CHUNKS_TO_PROCESS, \"max_output_tokens\": self.config.MAX_OUTPUT_TOKENS, \"stm_fullness_threshold\": self.config.STM_FULLNESS_THRESHOLD, \"clear_database\": self.config.CLEAR_DATABASE, \"processing_stage\": self.config.PROCESSING_STAGE, \"num_refinement_cycles\": self.config.NUM_REFINEMENT_CYCLES, \"ltm_merge_sample_size\": self.config.LTM_MERGE_SAMPLE_SIZE, \"ltm_hierarchy_sample_size\": self.config.LTM_HIERARCHY_SAMPLE_SIZE, \"part_of_sample_size\": self.config.PART_OF_SAMPLE_SIZE, \"part_of_candidate_pool_size\": self.config.PART_OF_CANDIDATE_POOL_SIZE, \"random_seed\": self.random_seed, \"discovered_nodes\": self.pre_consolidation_entities, \"discovered_relationships\": self.pre_consolidation_relationships, \"final_nodes\": node_count, \"final_relationships\": rel_count, \"duplicate_name_nodes\": test_scores.get(\"DuplicateEntityNameCount\"), \"ground_truth_source\": test_scores.get(\"GroundTruthSource\"), \"entity_precision\": test_scores.get(\"EntityPrecision\"), \"entity_recall\": test_scores.get(\"EntityRecall\"), \"entity_f1_score\": test_scores.get(\"EntityF1Score\"), \"relationship_precision\": test_scores.get(\"RelationshipPrecision\"), \"relationship_recall\": test_scores.get(\"RelationshipRecall\"), \"relationship_f1_score\": test_scores.get(\"RelationshipF1Score\"), \"overall_score\": test_scores.get(\"OverallScore (Avg F1)\"), \"error\": error_message, \"use_batch_processing\": self.config.USE_BATCH_PROCESSING }\n",
        "            summary_results.append(base_summary)\n",
        "        else:\n",
        "            for res in self.test_results:\n",
        "                test_scores = res.to_dict()\n",
        "                summary = { \"run_timestamp\": self.run_timestamp_str, \"group_id\": self.config.GROUP_ID, \"experiment_id\": self.config.EXPERIMENT_ID, \"experiment_name\": self.config.EXPERIMENT_NAME, \"description\": self.config.DESCRIPTION, \"document_name\": self.config.DOC_FILE_PATH.name if self.config.DOC_FILE_PATH else self.VALUE_NA, \"status\": status, \"duration_seconds\": duration.total_seconds(), \"llm_provider\": self.config.LLM_PROVIDER, \"llm_model\": self.config.LLM_MODEL, \"temperature\": self.config.TEMPERATURE, \"chunk_size\": self.config.DOC_CHUNK_SIZE, \"chunk_overlap\": self.config.DOC_CHUNK_OVERLAP, \"max_chunks_to_process\": self.config.MAX_CHUNKS_TO_PROCESS, \"max_output_tokens\": self.config.MAX_OUTPUT_TOKENS, \"stm_fullness_threshold\": self.config.STM_FULLNESS_THRESHOLD, \"clear_database\": self.config.CLEAR_DATABASE, \"processing_stage\": self.config.PROCESSING_STAGE, \"num_refinement_cycles\": self.config.NUM_REFINEMENT_CYCLES, \"ltm_merge_sample_size\": self.config.LTM_MERGE_SAMPLE_SIZE, \"ltm_hierarchy_sample_size\": self.config.LTM_HIERARCHY_SAMPLE_SIZE, \"part_of_sample_size\": self.config.PART_OF_SAMPLE_SIZE, \"part_of_candidate_pool_size\": self.config.PART_OF_CANDIDATE_POOL_SIZE, \"random_seed\": self.random_seed, \"discovered_nodes\": self.pre_consolidation_entities, \"discovered_relationships\": self.pre_consolidation_relationships, \"final_nodes\": node_count, \"final_relationships\": rel_count, \"duplicate_name_nodes\": test_scores.get(\"DuplicateEntityNameCount\"), \"ground_truth_source\": test_scores.get(\"GroundTruthSource\"), \"entity_precision\": test_scores.get(\"EntityPrecision\"), \"entity_recall\": test_scores.get(\"EntityRecall\"), \"entity_f1_score\": test_scores.get(\"EntityF1Score\"), \"relationship_precision\": test_scores.get(\"RelationshipPrecision\"), \"relationship_recall\": test_scores.get(\"RelationshipRecall\"), \"relationship_f1_score\": test_scores.get(\"RelationshipF1Score\"), \"overall_score\": test_scores.get(\"OverallScore (Avg F1)\"), \"error\": error_message, \"use_batch_processing\": self.config.USE_BATCH_PROCESSING }\n",
        "                summary_results.append(summary)\n",
        "\n",
        "        return {\n",
        "            \"summary\": summary_results, \"doc_metrics\": doc_data, \"doc_words\": doc_words_data,\n",
        "            \"kg_metrics\": kg_data, \"kg_nodes\": kg_nodes_data, \"kg_relationships\": kg_rels_data,\n",
        "            \"er_test_results\": test_result_data\n",
        "        }"
      ],
      "metadata": {
        "id": "5VnLTXPTkrh7"
      },
      "execution_count": 477,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2do7PL6YfUut"
      },
      "source": [
        "\n",
        "# Main Execution\n",
        "This section serves as the executable entry point for running the experiments defined in the \"Imports and Setup\" section. The main script logic iterates through each defined experiment, creating an Experiment object for each one. It then calls the run() method on each object to execute the workflow, which includes document ingestion, knowledge graph consolidation, and the collection and saving of all resulting data and performance metrics to CSV files for later analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 478,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yp7sEX5GT2BE",
        "outputId": "3e49be40-7eda-406c-aa10-ae502a8f8570"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO - Verifying Neo4j database connectivity before starting run...\n",
            "INFO - Successfully connected to Neo4j database.\n",
            "INFO - Neo4j connection closed.\n",
            "INFO - Neo4j connection successful. Proceeding with experiment run.\n",
            "INFO - --- Starting Experiment Run: 13 groups defined ---\n",
            "INFO - Run ID: RUN_2025-09-07_22-33-22\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Utility: Environment Cleanup (GRP001)\n",
            "INFO - Group Description: Contains a utility experiment to reset the Neo4j database, ensuring a clean environment before a new run.\n",
            "INFO - Using random seed 42 for group 'Utility: Environment Cleanup'.\n",
            "INFO - PROCESSING EXPERIMENT GROUP: Utility: Environment Cleanup (1 experiments)\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - --- Processing Experiment 1/1 in group 'Utility: Environment Cleanup': Clear Database (GRP001EXP001) ---\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - Executing utility task: Clearing database.\n",
            "INFO - Successfully connected to Neo4j database.\n",
            "INFO - Neo4j database cleared.\n",
            "INFO - Neo4j connection closed.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Utility: Generate Ground Truth (Test Files) (GRP101)\n",
            "INFO - Group Description: Generates the ground truth knowledge graphs for the four standardized test documents using the LangExtract library.\n",
            "INFO - SKIPPING group 'Utility: Generate Ground Truth (Test Files)' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Generate Ground Truth for base (GRP102)\n",
            "INFO - Group Description: This group contains utility experiments designed to prepare the needed ground truth files. It uses the original textfiles and LangExtract.\n",
            "INFO - SKIPPING group 'Generate Ground Truth for base' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Generate Ground Truth for Easttown (GRP103)\n",
            "INFO - Group Description: This group contains utility experiments designed to prepare the needed ground truth files. It uses the original textfiles and LangExtract.\n",
            "INFO - SKIPPING group 'Generate Ground Truth for Easttown' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Baseline Model Validation (GPT-4.1) (GRP201)\n",
            "INFO - Group Description: Establishes baseline performance of the Mnemosyne pipeline using OpenAI's GPT-4.1 model across the suite of standardized test documents.\n",
            "INFO - SKIPPING group 'Baseline Model Validation (GPT-4.1)' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Mnemosyne test all test files with GPT 4.1 (GRP202)\n",
            "INFO - Group Description: This group serves as a tool test to validate the core functionality of the Mnemosyne pipeline using the GPT 4.1 series of models. Each experiment processes a small, standardized test document to ensure that data ingestion, entity extraction, and knowledge graph refinement are all working as expected. This provides a quick verification of the system's end-to-end health.\n",
            "INFO - SKIPPING group 'Mnemosyne test all test files with GPT 4.1' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Mnemosyne test with Gemini 2.5 (GRP203)\n",
            "INFO - Group Description: This group serves as a tool test to validate the core functionality of the Mnemosyne pipeline using the Gemini 2.5 series of models. Each experiment processes a small, standardized test document to ensure that data ingestion, entity extraction, and knowledge graph refinement are all working as expected. This provides a quick verification of the system's end-to-end health.\n",
            "INFO - SKIPPING group 'Mnemosyne test with Gemini 2.5' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Verify that batch processing works with Gemini and GPT. (GRP204)\n",
            "INFO - Group Description: This group serves as a test to validate that the Mnemosyne implementation of batch processing works with Gemini and GPT.\n",
            "INFO - Using random seed 42 for group 'Verify that batch processing works with Gemini and GPT.'.\n",
            "INFO - PROCESSING EXPERIMENT GROUP: Verify that batch processing works with Gemini and GPT. (2 experiments)\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - --- Processing Experiment 1/2 in group 'Verify that batch processing works with Gemini and GPT.': Mnemosyne GPT 4.1 Batch Test (GRP204EXP001) ---\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - Configuration for 'GRP204EXP001' validated successfully.\n",
            "INFO - Successfully connected to Neo4j database.\n",
            "INFO - Clearing database as per experiment configuration.\n",
            "INFO - Neo4j database cleared.\n",
            "INFO - Document initialized for: /content/drive/MyDrive/Mnemosyne/Input/NER Test 1 - Text.docx\n",
            "INFO - Opening document context for NER Test 1 - Text.docx\n",
            "INFO - ‚ñ∂Ô∏è START: Generating input document word cloud...\n",
            "INFO -   Saved word cloud to /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/figures/appendix_fig/GRP204EXP001_input_wordcloud.png\n",
            "INFO - ‚úÖ END: Generating input document word cloud.\n",
            "INFO - Mind initialized with model: OpenAI / gpt-4.1\n",
            "INFO - Submitting 4 chunks to OpenAI Batch API.\n",
            "INFO - Batch input file created at: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204EXP001_batch_input.jsonl\n",
            "INFO - HTTP Request: POST https://api.openai.com/v1/files \"HTTP/1.1 200 OK\"\n",
            "INFO - File GRP204EXP001_batch_input.jsonl uploaded with ID: file-32Jmifa4We9EKfnvPhDXE6\n",
            "INFO - HTTP Request: POST https://api.openai.com/v1/batches \"HTTP/1.1 200 OK\"\n",
            "INFO - Batch job created with ID: batch_68be083870e48190b3ff824ce720236c\n",
            "INFO - Monitoring batch job batch_68be083870e48190b3ff824ce720236c...\n",
            "INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68be083870e48190b3ff824ce720236c \"HTTP/1.1 200 OK\"\n",
            "INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68be083870e48190b3ff824ce720236c \"HTTP/1.1 200 OK\"\n",
            "INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68be083870e48190b3ff824ce720236c \"HTTP/1.1 200 OK\"\n",
            "INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68be083870e48190b3ff824ce720236c \"HTTP/1.1 200 OK\"\n",
            "INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68be083870e48190b3ff824ce720236c \"HTTP/1.1 200 OK\"\n",
            "INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68be083870e48190b3ff824ce720236c \"HTTP/1.1 200 OK\"\n",
            "INFO - HTTP Request: GET https://api.openai.com/v1/batches/batch_68be083870e48190b3ff824ce720236c \"HTTP/1.1 200 OK\"\n",
            "INFO - Batch job batch_68be083870e48190b3ff824ce720236c completed successfully.\n",
            "INFO - HTTP Request: GET https://api.openai.com/v1/files/file-1ZiZBz3RK7PyY2QT25nonp/content \"HTTP/1.1 200 OK\"\n",
            "INFO - Batch results downloaded to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204EXP001_batch_input.jsonl_results.jsonl\n",
            "INFO - Consolidating Short-Term Memory to Long-Term Memory.\n",
            "INFO - Wrote KG fragment to LTM with experiment and run labels.\n",
            "INFO - STM to LTM consolidation complete.\n",
            "INFO - Consolidating Short-Term Memory to Long-Term Memory.\n",
            "INFO - Wrote KG fragment to LTM with experiment and run labels.\n",
            "INFO - STM to LTM consolidation complete.\n",
            "INFO - Consolidating Short-Term Memory to Long-Term Memory.\n",
            "INFO - Wrote KG fragment to LTM with experiment and run labels.\n",
            "INFO - STM to LTM consolidation complete.\n",
            "INFO - Consolidating Short-Term Memory to Long-Term Memory.\n",
            "INFO - Wrote KG fragment to LTM with experiment and run labels.\n",
            "INFO - STM to LTM consolidation complete.\n",
            "INFO - Finished processing OpenAI batch for 4 chunks.\n",
            "INFO - Consolidating Short-Term Memory to Long-Term Memory.\n",
            "INFO - STM to LTM consolidation complete.\n",
            "INFO - ‚ñ∂Ô∏è START: Running LTM Consolidation...\n",
            "INFO -   Marked 1 source nodes with 'DoNotChange' label.\n",
            "INFO -   ‚ñ∂Ô∏è START: Deterministically merging exact duplicates by name and type...\n",
            "INFO -     Completed 10 deterministic merge operations.\n",
            "INFO -   ‚úÖ END: Deterministically merging exact duplicates by name and type.\n",
            "INFO -   ‚ñ∂Ô∏è START: Deterministically merging duplicate relationships...\n",
            "INFO -     Consolidated 1 groups of duplicate relationships.\n",
            "INFO -   ‚úÖ END: Deterministically merging duplicate relationships.\n",
            "INFO -   ‚ñ∂Ô∏è START: Classifying untyped nodes to 'thing' type...\n",
            "INFO -     Classified 0 untyped nodes to 'thing' type.\n",
            "INFO -   ‚úÖ END: Classifying untyped nodes to 'thing' type.\n",
            "INFO -   ‚ñ∂Ô∏è START: Refinement Cycle 1/2...\n",
            "INFO -     ‚ñ∂Ô∏è START: Refining PART_OF relationships...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,501 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.826\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,539 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 90\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [ \"c2-node-41\", \"c3-node-8\", \"c4-node-9\" ] }--\n",
            "INFO -       LLM call duration: 00:00:01.088\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,509 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.597\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,547 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.693\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,454 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.674\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,492 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 176\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [ \"c2-node-9\", \"c2-node-41\", \"c2-node-49\", \"c3-node-6\", \"c3-node-8\", \"c3-node-18\", \"c4-node-9\" ] }--\n",
            "INFO -       LLM call duration: 00:00:01.222\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,454 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.960\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,492 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.690\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,454 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.800\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,492 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.894\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,454 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.562\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,492 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.654\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,454 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.513\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,492 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.829\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,454 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.550\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,492 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 243\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [ \"c2-node-9\", \"c2-node-41\", \"c3-node-8\", \"c3-node-15\", \"c4-node-9\", \"c4-node-17\", \"c4-node-19\", \"c4-node-31\", \"c4-node-33\", \"c4-node-35\" ] }--\n",
            "INFO -       LLM call duration: 00:00:01.818\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,454 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.498\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,492 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 60\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [\"c2-node-9\", \"c2-node-41\", \"c3-node-8\"] }--\n",
            "INFO -       LLM call duration: 00:00:01.105\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,493 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.558\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,531 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 47\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [\"c1-node-5\", \"c4-node-21\"] }--\n",
            "INFO -       LLM call duration: 00:00:00.929\n",
            "INFO -       Created 25 new PART_OF relationships.\n",
            "INFO -     ‚úÖ END: Refining PART_OF relationships.\n",
            "INFO -     ‚ñ∂Ô∏è START: Refining type system...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,757 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 92\n",
            "INFO -       C: LLM Response --{ \"merge_pairs\": [ [\"c2-node-42\", \"c3-node-9\"], [\"c2-node-16\", \"c3-node-31\"] ] }--\n",
            "INFO -       LLM call duration: 00:00:01.007\n",
            "INFO -       Merged 'Feature' (c3-node-9) into 'SoftwareFeature' (c2-node-42).\n",
            "INFO -       Merged 'Position' (c3-node-31) into 'JobTitle' (c2-node-16).\n",
            "INFO -       Refined 2 pairs of types.\n",
            "INFO -     ‚úÖ END: Refining type system.\n",
            "INFO -     ‚ñ∂Ô∏è START: Organizing ontology hierarchy...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c4-node-10\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.694\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c1-node-10\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.754\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c4-node-22\", \"parent_id\": \"c4-node-24\" }--\n",
            "INFO -       LLM call duration: 00:00:00.682\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c2-node-24\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.721\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c2-node-28\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.878\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c2-node-30\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:01.379\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c2-node-44\", \"parent_id\": \"c3-node-16\" }--\n",
            "INFO -       LLM call duration: 00:00:00.752\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 58\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c3-node-4\", \"parent_id\": \"c4-node-10\" }--\n",
            "INFO -       LLM call duration: 00:00:00.774\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c3-node-16\", \"parent_id\": \"c2-node-44\" }--\n",
            "INFO -       LLM call duration: 00:00:00.718\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c4-node-24\", \"parent_id\": \"c2-node-28\" }--\n",
            "INFO -       LLM call duration: 00:00:00.843\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c3-node-35\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.646\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,249 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c4-node-36\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.653\n",
            "INFO -       Created IS_A relationship: 'Software Product' (c4-node-10) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Investment Firm' (c1-node-10) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'City' (c4-node-22) -> 'State' (c4-node-24).\n",
            "INFO -       Created IS_A relationship: 'FundingRound' (c2-node-24) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Country' (c2-node-28) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Office' (c2-node-30) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'ResearchProject' (c2-node-44) -> 'Research Project' (c3-node-16).\n",
            "INFO -       Created IS_A relationship: 'Programming Language' (c3-node-4) -> 'Software Product' (c4-node-10).\n",
            "INFO -       Created IS_A relationship: 'Research Project' (c3-node-16) -> 'ResearchProject' (c2-node-44).\n",
            "INFO -       Created IS_A relationship: 'State' (c4-node-24) -> 'Country' (c2-node-28).\n",
            "INFO -       Created IS_A relationship: 'Document' (c3-node-35) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Role' (c4-node-36) -> 'Thing' (thing-type-global).\n",
            "INFO -       Organized 12 new hierarchical relationships.\n",
            "INFO -     ‚úÖ END: Organizing ontology hierarchy.\n",
            "INFO -     ‚ñ∂Ô∏è START: Correcting instance types...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,688 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 63\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c1-node-1\", \"new_type_id\": \"c4-node-12\" }--\n",
            "INFO -       LLM call duration: 00:00:00.666\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,680 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c1-node-12\", \"new_type_id\": \"c1-node-13\" }--\n",
            "INFO -       LLM call duration: 00:00:00.777\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,677 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 62\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-9\", \"new_type_id\": \"c4-node-8\" }--\n",
            "INFO -       LLM call duration: 00:00:00.724\n",
            "INFO -       Reclassified 'Nexus Platform' from 'softwareproduct(c2-node-10)' to 'software platform(c4-node-8)'.\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,687 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-15\", \"new_type_id\": \"c3-node-31\" }--\n",
            "INFO -       LLM call duration: 00:00:00.800\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,681 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-19\", \"new_type_id\": \"c1-node-10\" }--\n",
            "INFO -       LLM call duration: 00:00:01.619\n",
            "INFO -       Reclassified 'Quantum Financial' from 'company(c4-node-12)' to 'investment firm(c1-node-10)'.\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,671 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-27\", \"new_type_id\": \"c2-node-28\" }--\n",
            "INFO -       LLM call duration: 00:00:01.056\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,668 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-45\", \"new_type_id\": \"c4-node-26\" }--\n",
            "INFO -       LLM call duration: 00:00:00.772\n",
            "INFO -       Reclassified '2017' from 'date(c4-node-30)' to 'year(c4-node-26)'.\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,677 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 62\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-1\", \"new_type_id\": \"c4-node-8\" }--\n",
            "INFO -       LLM call duration: 00:00:00.921\n",
            "INFO -       Reclassified 'Nexus Platform' from 'product(c3-node-2)' to 'software platform(c4-node-8)'.\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,668 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 62\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-5\", \"new_type_id\": \"c3-node-4\" }--\n",
            "INFO -       LLM call duration: 00:00:00.847\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,680 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-10\", \"new_type_id\": \"c4-node-20\" }--\n",
            "INFO -       LLM call duration: 00:00:00.659\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,679 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-15\", \"new_type_id\": \"c3-node-16\" }--\n",
            "INFO -       LLM call duration: 00:00:01.658\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,687 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-20\", \"new_type_id\": \"c3-node-21\" }--\n",
            "INFO -       LLM call duration: 00:00:00.968\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,673 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-25\", \"new_type_id\": \"c4-node-22\" }--\n",
            "INFO -       LLM call duration: 00:00:01.495\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,677 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-29\", \"new_type_id\": \"c4-node-20\" }--\n",
            "INFO -       LLM call duration: 00:00:00.797\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,688 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 63\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-5\", \"new_type_id\": \"c4-node-12\" }--\n",
            "INFO -       LLM call duration: 00:00:00.641\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,677 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 62\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-7\", \"new_type_id\": \"c4-node-8\" }--\n",
            "INFO -       LLM call duration: 00:00:00.768\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,679 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-13\", \"new_type_id\": \"c4-node-20\" }--\n",
            "INFO -       LLM call duration: 00:00:01.366\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,676 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-15\", \"new_type_id\": \"c4-node-20\" }--\n",
            "INFO -       LLM call duration: 00:00:00.767\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,667 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 58\n",
            "INFO -       C: LLM Response --{\"instance_id\": \"c4-node-31\", \"new_type_id\": \"c3-node-31\"}--\n",
            "INFO -       LLM call duration: 00:00:00.759\n",
            "INFO -       Reclassified 'CEO' from 'role(c4-node-36)' to 'position(c3-node-31)'.\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,667 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-35\", \"new_type_id\": \"c3-node-31\" }--\n",
            "INFO -       LLM call duration: 00:00:00.784\n",
            "INFO -       Reclassified 'CFO' from 'role(c4-node-36)' to 'position(c3-node-31)'.\n",
            "INFO -       Corrected 6 instance types.\n",
            "INFO -     ‚úÖ END: Correcting instance types.\n",
            "INFO -     ‚ñ∂Ô∏è START: Merging similar instances...\n",
            "INFO -       Processing 20 instances in a single batch for merging...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,710 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 157\n",
            "INFO -       C: LLM Response --{ \"merge_pairs\": [ [\"c2-node-47\", \"c4-node-13\"], [\"c2-node-41\", \"c3-node-8\"], [\"c2-node-41\", \"c4-node-9\"], [\"c3-node-8\", \"c4-node-9\"] ] }--\n",
            "INFO -       LLM call duration: 00:00:01.496\n",
            "INFO -       Merged 'Dr. Aris Thorne' (c4-node-13) into 'Dr. Aris Thorne' (c2-node-47).\n",
            "INFO -       Merged 'Nexus Analytics' (c3-node-8) into 'Nexus Analytics' (c2-node-41).\n",
            "INFO -       Merged 2 pairs of instances.\n",
            "INFO -     ‚úÖ END: Merging similar instances.\n",
            "INFO -   ‚úÖ END: Refinement Cycle 1/2.\n",
            "INFO -   ‚ñ∂Ô∏è START: Refinement Cycle 2/2...\n",
            "INFO -     ‚ñ∂Ô∏è START: Refining PART_OF relationships...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,464 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.578\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,502 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 33\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [\"c2-node-7\"] }--\n",
            "INFO -       LLM call duration: 00:00:00.606\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,522 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.546\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,560 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 178\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [ \"c2-node-5\", \"c2-node-29\", \"c3-node-34\", \"c4-node-5\", \"c4-node-31\", \"c4-node-33\", \"c4-node-35\" ] }--\n",
            "INFO -       LLM call duration: 00:00:01.059\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,514 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.515\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,552 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.824\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,464 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.579\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,502 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.601\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,523 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.683\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,561 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 132\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [ \"c1-node-7\", \"c3-node-8\", \"c3-node-34\", \"c4-node-7\", \"c4-node-9\" ] }--\n",
            "INFO -       LLM call duration: 00:00:00.962\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,464 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 56\n",
            "INFO -       C: LLM Response --{ \"part_id\": \"c3-node-34\", \"whole_id\": \"c1-node-1\" }--\n",
            "INFO -       LLM call duration: 00:00:00.643\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,502 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.594\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,464 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.540\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,502 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 33\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [\"c4-node-9\"] }--\n",
            "INFO -       LLM call duration: 00:00:00.696\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,464 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:00.617\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,502 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.691\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,464 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 2\n",
            "INFO -       C: LLM Response --{}--\n",
            "INFO -       LLM call duration: 00:00:01.266\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,502 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.659\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,464 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 56\n",
            "INFO -       C: LLM Response --{ \"part_id\": \"c4-node-33\", \"whole_id\": \"c1-node-1\" }--\n",
            "INFO -       LLM call duration: 00:00:00.823\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 3,502 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 22\n",
            "INFO -       C: LLM Response --{ \"part_ids\": [] }--\n",
            "INFO -       LLM call duration: 00:00:00.661\n",
            "INFO -       Created 16 new PART_OF relationships.\n",
            "INFO -     ‚úÖ END: Refining PART_OF relationships.\n",
            "INFO -     ‚ñ∂Ô∏è START: Refining type system...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,752 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 58\n",
            "INFO -       C: LLM Response --{ \"merge_pairs\": [ [\"c2-node-36\", \"c3-node-4\"] ] }--\n",
            "INFO -       LLM call duration: 00:00:00.679\n",
            "INFO -       Merged 'Programming Language' (c3-node-4) into 'ProgrammingLanguage' (c2-node-36).\n",
            "INFO -       Refined 1 pairs of types.\n",
            "INFO -     ‚úÖ END: Refining type system.\n",
            "INFO -     ‚ñ∂Ô∏è START: Organizing ontology hierarchy...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c4-node-12\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:01.608\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 65\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c1-node-6\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.729\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c1-node-10\", \"parent_id\": \"c4-node-12\" }--\n",
            "INFO -       LLM call duration: 00:00:00.980\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 74\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"source-type-global\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.746\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c3-node-31\", \"parent_id\": \"c4-node-36\" }--\n",
            "INFO -       LLM call duration: 00:00:00.926\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c2-node-10\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.638\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c4-node-30\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.746\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c2-node-24\", \"parent_id\": \"c4-node-12\" }--\n",
            "INFO -       LLM call duration: 00:00:00.680\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 58\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c2-node-30\", \"parent_id\": \"c1-node-6\" }--\n",
            "INFO -       LLM call duration: 00:00:00.590\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 66\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c3-node-21\", \"parent_id\": \"thing-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.603\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c3-node-24\", \"parent_id\": \"c4-node-12\" }--\n",
            "INFO -       LLM call duration: 00:00:00.594\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 58\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c4-node-24\", \"parent_id\": \"c1-node-6\" }--\n",
            "INFO -       LLM call duration: 00:00:00.611\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 67\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c3-node-35\", \"parent_id\": \"source-type-global\" }--\n",
            "INFO -       LLM call duration: 00:00:00.767\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 1,336 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 59\n",
            "INFO -       C: LLM Response --{ \"child_id\": \"c4-node-36\", \"parent_id\": \"c3-node-31\" }--\n",
            "INFO -       LLM call duration: 00:00:00.787\n",
            "INFO -       Created IS_A relationship: 'Company' (c4-node-12) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Location' (c1-node-6) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Investment Firm' (c1-node-10) -> 'Company' (c4-node-12).\n",
            "INFO -       Created IS_A relationship: 'Source' (source-type-global) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Position' (c3-node-31) -> 'Role' (c4-node-36).\n",
            "INFO -       Created IS_A relationship: 'SoftwareProduct' (c2-node-10) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Date' (c4-node-30) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'FundingRound' (c2-node-24) -> 'Company' (c4-node-12).\n",
            "INFO -       Created IS_A relationship: 'Office' (c2-node-30) -> 'Location' (c1-node-6).\n",
            "INFO -       Created IS_A relationship: 'Domain' (c3-node-21) -> 'Thing' (thing-type-global).\n",
            "INFO -       Created IS_A relationship: 'Startup' (c3-node-24) -> 'Company' (c4-node-12).\n",
            "INFO -       Created IS_A relationship: 'State' (c4-node-24) -> 'Location' (c1-node-6).\n",
            "INFO -       Created IS_A relationship: 'Document' (c3-node-35) -> 'Source' (source-type-global).\n",
            "INFO -       Created IS_A relationship: 'Role' (c4-node-36) -> 'Position' (c3-node-31).\n",
            "INFO -       Organized 14 new hierarchical relationships.\n",
            "INFO -     ‚úÖ END: Organizing ontology hierarchy.\n",
            "INFO -     ‚ñ∂Ô∏è START: Correcting instance types...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,610 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 62\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c1-node-7\", \"new_type_id\": \"c4-node-8\" }--\n",
            "INFO -       LLM call duration: 00:00:00.706\n",
            "INFO -       Reclassified 'Nexus Platform' from 'software product(c4-node-10)' to 'software platform(c4-node-8)'.\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,607 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-13\", \"new_type_id\": \"c4-node-20\" }--\n",
            "INFO -       LLM call duration: 00:00:00.804\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,620 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-15\", \"new_type_id\": \"c3-node-31\" }--\n",
            "INFO -       LLM call duration: 00:00:01.701\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,614 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-19\", \"new_type_id\": \"c1-node-10\" }--\n",
            "INFO -       LLM call duration: 00:00:01.063\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,610 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-21\", \"new_type_id\": \"c4-node-22\" }--\n",
            "INFO -       LLM call duration: 00:00:00.792\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,619 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-23\", \"new_type_id\": \"c2-node-24\" }--\n",
            "INFO -       LLM call duration: 00:00:00.779\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,603 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-25\", \"new_type_id\": \"c4-node-22\" }--\n",
            "INFO -       LLM call duration: 00:00:00.726\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,610 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-29\", \"new_type_id\": \"c2-node-30\" }--\n",
            "INFO -       LLM call duration: 00:00:00.723\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,628 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c2-node-49\", \"new_type_id\": \"c3-node-19\" }--\n",
            "INFO -       LLM call duration: 00:00:00.707\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,610 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 62\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-1\", \"new_type_id\": \"c4-node-8\" }--\n",
            "INFO -       LLM call duration: 00:00:00.665\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,602 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 62\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-3\", \"new_type_id\": \"c3-node-4\" }--\n",
            "INFO -       LLM call duration: 00:00:00.723\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,601 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 62\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-5\", \"new_type_id\": \"c3-node-4\" }--\n",
            "INFO -       LLM call duration: 00:00:00.743\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,614 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-14\", \"new_type_id\": \"c4-node-12\" }--\n",
            "INFO -       LLM call duration: 00:00:00.737\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,612 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-17\", \"new_type_id\": \"c4-node-20\" }--\n",
            "INFO -       LLM call duration: 00:00:00.736\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,622 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-22\", \"new_type_id\": \"c4-node-12\" }--\n",
            "INFO -       LLM call duration: 00:00:00.636\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,606 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c3-node-25\", \"new_type_id\": \"c4-node-22\" }--\n",
            "INFO -       LLM call duration: 00:00:00.936\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,603 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 63\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-3\", \"new_type_id\": \"c4-node-22\" }--\n",
            "INFO -       LLM call duration: 00:00:00.835\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,621 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 63\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-5\", \"new_type_id\": \"c4-node-12\" }--\n",
            "INFO -       LLM call duration: 00:00:00.677\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,607 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-19\", \"new_type_id\": \"c4-node-20\" }--\n",
            "INFO -       LLM call duration: 00:00:00.843\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,608 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 64\n",
            "INFO -       C: LLM Response --{ \"instance_id\": \"c4-node-29\", \"new_type_id\": \"c4-node-30\" }--\n",
            "INFO -       LLM call duration: 00:00:00.678\n",
            "INFO -       Corrected 1 instance types.\n",
            "INFO -     ‚úÖ END: Correcting instance types.\n",
            "INFO -     ‚ñ∂Ô∏è START: Merging similar instances...\n",
            "INFO -       Processing 20 instances in a single batch for merging...\n",
            "INFO -       Querying LLM (gpt-4.1) with prompt of 2,738 characters.\n",
            "INFO -       HTTP Request: POST https://api.openai.com/v1/chat/completions \"HTTP/1.1 200 OK\"\n",
            "INFO -       Length of LLM response: 159\n",
            "INFO -       C: LLM Response --{ \"merge_pairs\": [ [\"c3-node-23\", \"c3-node-23\"], [\"c3-node-15\", \"c2-node-43\"], [\"c4-node-11\", \"c2-node-31\"], [\"c4-node-7\", \"c3-node-1\"] ] }--\n",
            "INFO -       LLM call duration: 00:00:01.338\n",
            "INFO -       Merged 'Project Chimera' (c3-node-15) into 'Project Chimera' (c2-node-43).\n",
            "INFO -       Merged 'MegaCorp Holdings' (c4-node-11) into 'Global Logistics Corp' (c2-node-31).\n",
            "INFO -       Merged 'Nexus Platform' (c4-node-7) into 'Nexus Platform' (c3-node-1).\n",
            "INFO -       Merged 3 pairs of instances.\n",
            "INFO -     ‚úÖ END: Merging similar instances.\n",
            "INFO -   ‚úÖ END: Refinement Cycle 2/2.\n",
            "INFO -   ‚ñ∂Ô∏è START: Final Hierarchy Cleanup...\n",
            "INFO -     Linked 7 orphan Type nodes to 'thing'.\n",
            "INFO -   ‚úÖ END: Final Hierarchy Cleanup.\n",
            "INFO - ‚úÖ END: Running LTM Consolidation.\n",
            "INFO - --- Starting Ground Truth Comparisons for GRP204EXP001 ---\n",
            "INFO - ‚ñ∂Ô∏è START: Comparing against 'NER Test 1 - MAN - GT - JSON.json' (Use: Manual)...\n",
            "INFO -   Cleared 0 previous Golden Standard nodes.\n",
            "INFO -   --- Loading Golden Standard KG from NER Test 1 - MAN - GT - JSON.json ---\n",
            "INFO -   Wrote KG fragment to LTM with experiment and run labels.\n",
            "INFO -   Successfully loaded Golden Standard KG into Neo4j.\n",
            "INFO -   Comparison Complete. Overall Score: 37.93%\n",
            "INFO - ‚úÖ END: Comparing against 'NER Test 1 - MAN - GT - JSON.json' (Use: Manual).\n",
            "INFO - ‚ñ∂Ô∏è START: Comparing against 'NER Test 1 - LE - GT - JSON.json' (Use: LangExtract)...\n",
            "INFO -   Cleared 46 previous Golden Standard nodes.\n",
            "INFO -   --- Loading Golden Standard KG from NER Test 1 - LE - GT - JSON.json ---\n",
            "INFO -   Wrote KG fragment to LTM with experiment and run labels.\n",
            "INFO -   Successfully loaded Golden Standard KG into Neo4j.\n",
            "INFO -   Comparison Complete. Overall Score: 20.59%\n",
            "INFO - ‚úÖ END: Comparing against 'NER Test 1 - LE - GT - JSON.json' (Use: LangExtract).\n",
            "INFO - Closing document context for NER Test 1 - Text.docx\n",
            "INFO - ‚ñ∂Ô∏è START: Generating extracted entity word cloud...\n",
            "INFO -   Saved word cloud to /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/figures/appendix_fig/GRP204EXP001_entity_wordcloud.png\n",
            "INFO - ‚úÖ END: Generating extracted entity word cloud.\n",
            "INFO - ‚ñ∂Ô∏è START: Generating relationship type word cloud...\n",
            "INFO -   Saved word cloud to /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/figures/appendix_fig/GRP204EXP001_relationship_wordcloud.png\n",
            "INFO - ‚úÖ END: Generating relationship type word cloud.\n",
            "INFO - ‚ñ∂Ô∏è START: Generating ontology type graph...\n",
            "INFO -   Saved ontology graph to /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/figures/appendix_fig/GRP204EXP001_ontology_graph.png\n",
            "INFO - ‚úÖ END: Generating ontology type graph.\n",
            "INFO - Neo4j connection closed.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - --- Processing Experiment 2/2 in group 'Verify that batch processing works with Gemini and GPT.': Mnemosyne Gemini 2.5 Pro Batch Test (GRP204EXP002) ---\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - Configuration for 'GRP204EXP002' validated successfully.\n",
            "INFO - Successfully connected to Neo4j database.\n",
            "INFO - Clearing database as per experiment configuration.\n",
            "INFO - Neo4j database cleared.\n",
            "INFO - Document initialized for: /content/drive/MyDrive/Mnemosyne/Input/NER Test 1 - Text.docx\n",
            "INFO - Opening document context for NER Test 1 - Text.docx\n",
            "INFO - ‚ñ∂Ô∏è START: Generating input document word cloud...\n",
            "INFO -   Saved word cloud to /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/figures/appendix_fig/GRP204EXP002_input_wordcloud.png\n",
            "INFO - ‚úÖ END: Generating input document word cloud.\n",
            "INFO - Mind initialized with model: Gemini / gemini-2.5-pro\n",
            "INFO - An error occurred during doc_to_stm processing: name 'asyncio' is not defined\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3650548011.py\", line 44, in doc_to_stm\n",
            "    loop = asyncio.get_running_loop()\n",
            "           ^^^^^^^\n",
            "NameError: name 'asyncio' is not defined. Did you forget to import 'asyncio'\n",
            "INFO - Closing document context for NER Test 1 - Text.docx\n",
            "INFO - Workflow for GRP204EXP002 FAILED: name 'asyncio' is not defined\n",
            "Traceback (most recent call last):\n",
            "  File \"/tmp/ipython-input-3934846550.py\", line 216, in run\n",
            "    a_mind.doc_to_stm(a_document, use_batch=use_batch)\n",
            "  File \"/tmp/ipython-input-3650548011.py\", line 44, in doc_to_stm\n",
            "    loop = asyncio.get_running_loop()\n",
            "           ^^^^^^^\n",
            "NameError: name 'asyncio' is not defined. Did you forget to import 'asyncio'\n",
            "INFO - Neo4j connection closed.\n",
            "INFO - Saved group results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204_Verify_that_batch_processing_works_with_Gemini_and_GPT./01_group_summary.csv\n",
            "INFO - Saved group results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204_Verify_that_batch_processing_works_with_Gemini_and_GPT./02_document_metrics.csv\n",
            "INFO - Saved group results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204_Verify_that_batch_processing_works_with_Gemini_and_GPT./03_document_words.csv\n",
            "INFO - Saved group results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204_Verify_that_batch_processing_works_with_Gemini_and_GPT./04_kg_metrics.csv\n",
            "INFO - Saved group results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204_Verify_that_batch_processing_works_with_Gemini_and_GPT./05_kg_nodes.csv\n",
            "INFO - Saved group results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204_Verify_that_batch_processing_works_with_Gemini_and_GPT./06_kg_relationships.csv\n",
            "INFO - Saved group results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/GRP204_Verify_that_batch_processing_works_with_Gemini_and_GPT./07_ground_truth_comparison.csv\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Ablation Study: Impact of Refinement Cycles (GRP301)\n",
            "INFO - Group Description: Quantifies the impact of the LTM consolidation process by varying the number of refinement cycles. This ablation study measures the marginal improvement in graph quality with each additional cycle.\n",
            "INFO - SKIPPING group 'Ablation Study: Impact of Refinement Cycles' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Parameter Sensitivity: Document Chunk Size (GRP302)\n",
            "INFO - Group Description: Investigates the impact of the `doc_chunk_size` hyperparameter on knowledge graph quality. This analysis aims to find the optimal balance between providing sufficient context to the LLM and processing efficiency.\n",
            "INFO - SKIPPING group 'Parameter Sensitivity: Document Chunk Size' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Comparative Analysis: OpenAI vs. Gemini Models (GRP303)\n",
            "INFO - Group Description: Conducts a direct performance and quality comparison between models from OpenAI and Google. All hyperparameters are held constant to isolate the impact of the foundation model.\n",
            "INFO - SKIPPING group 'Comparative Analysis: OpenAI vs. Gemini Models' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Comparative Analysis: OpenAI vs. Gemini Models (GRP304)\n",
            "INFO - Group Description: Conducts a direct performance and quality comparison between models from OpenAI and Google. All hyperparameters are held constant to isolate the impact of the foundation model.\n",
            "INFO - SKIPPING group 'Comparative Analysis: OpenAI vs. Gemini Models' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - EVALUATING EXPERIMENT GROUP: Primary Use Case: Legal Document Analysis (GRP401)\n",
            "INFO - Group Description: Applies the optimized Mnemosyne pipeline to the primary target document, the Conewago Township Sewer Authority (CTSA) legal text, to generate a comprehensive knowledge graph.\n",
            "INFO - SKIPPING group 'Primary Use Case: Legal Document Analysis' because 'run_group' is set to False.\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - ALL EXPERIMENTS COMPLETED\n",
            "INFO - Total execution time: 0:07:29.857970\n",
            "INFO - ==================================================================================================================================\n",
            "INFO - --- Done Experiment Run: 13 groups defined ---\n",
            "INFO - Run ID: RUN_2025-09-07_22-33-22\n",
            "INFO - Successfully saved master results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/00_master_run_summary.csv\n",
            "INFO - Successfully saved master results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/02_document_metrics.csv\n",
            "INFO - Successfully saved master results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/03_document_words.csv\n",
            "INFO - Successfully saved master results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/04_kg_metrics.csv\n",
            "INFO - Successfully saved master results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/05_kg_nodes.csv\n",
            "INFO - Successfully saved master results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/06_kg_relationships.csv\n",
            "INFO - Successfully saved master results to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/07_ground_truth_comparison.csv\n",
            "INFO - Successfully saved LaTeX summary to: /content/drive/MyDrive/Mnemosyne/Output/RUN_2025-09-07_22-33-22/08_run_summary.tex\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- MASTER EXPERIMENT RUN SUMMARY ---\n",
            "  group_id experiment_id                      experiment_name   status  duration_seconds ground_truth_source overall_score entity_f1_score relationship_f1_score                          error\n",
            "0   GRP204  GRP204EXP001         Mnemosyne GPT 4.1 Batch Test  Success        445.752089              Manual        37.93%          68.24%                 7.62%                               \n",
            "1   GRP204  GRP204EXP001         Mnemosyne GPT 4.1 Batch Test  Success        445.752089         LangExtract        20.59%          39.02%                 2.15%                               \n",
            "2   GRP204  GRP204EXP002  Mnemosyne Gemini 2.5 Pro Batch Test  Failure          2.319198                 N/A           N/A             N/A                   N/A  name 'asyncio' is not defined\n"
          ]
        }
      ],
      "source": [
        "# --- MAIN EXECUTION BLOCK ---\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # --- MODIFICATION START ---\n",
        "    # Add a pre-flight check to ensure Neo4j is available before starting the run.\n",
        "    # This prevents the program from running experiments that are guaranteed to fail.\n",
        "    db_available = False\n",
        "    try:\n",
        "        logging.info(\"Verifying Neo4j database connectivity before starting run...\")\n",
        "        # Attempt to connect using credentials from ExperimentConfig\n",
        "        graph_db_check = GraphDB(ExperimentConfig.NEO4J_URI, ExperimentConfig.NEO4J_AUTH)\n",
        "        graph_db_check.close()\n",
        "        logging.info(\"Neo4j connection successful. Proceeding with experiment run.\")\n",
        "        db_available = True\n",
        "    except Exception as e:\n",
        "        logging.critical(f\"HALTING EXECUTION: Could not connect to Neo4j database. Please ensure the service is running and credentials are correct. Error: {e}\")\n",
        "    # --- MODIFICATION END ---\n",
        "\n",
        "    if db_available:\n",
        "        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "        all_summaries = []\n",
        "        all_doc_metrics, all_doc_words, all_kg_metrics, all_kg_nodes, all_kg_rels, all_er_results = [], [], [], [], [], []\n",
        "\n",
        "        total_start_time = datetime.now(timezone.utc)\n",
        "        run_timestamp_str = total_start_time.strftime('%Y-%m-%d_%H-%M-%S')\n",
        "        run_id = f\"RUN_{run_timestamp_str}\"\n",
        "\n",
        "        # --- MODIFICATION START ---\n",
        "        # The location_map now stores a dictionary with path and use details.\n",
        "        location_map = {loc['name']: {'path': loc['path'], 'use': loc.get('use', 'N/A')} for loc in locations}\n",
        "        # --- MODIFICATION END ---\n",
        "\n",
        "        logging.info(f\"--- Starting Experiment Run: {len(experiment_groups)} groups defined ---\")\n",
        "        logging.info(f\"Run ID: {run_id}\")\n",
        "\n",
        "        run_output_path = ROOT_PATH / location_map[\"output_dir\"]['path'] / run_id\n",
        "        run_output_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        for group_params in experiment_groups:\n",
        "            group_id_str = group_params.get(\"group_id\")\n",
        "            group_name = group_params.get(\"group_name\", \"Unnamed_Group\")\n",
        "            experiments_in_group = group_params.get(\"experiments\", [])\n",
        "\n",
        "            group_context = {\n",
        "                \"group_id\": group_id_str,\n",
        "                \"group_name\": group_name,\n",
        "                \"description\": group_params.get(\"description\", \"No description provided.\"),\n",
        "                \"random_seed\": group_params.get(\"random_seed\", 42),\n",
        "                \"input_location\": group_params.get(\"input_location\"),\n",
        "                \"output_location\": group_params.get(\"output_location\"),\n",
        "                \"diagram_location\": group_params.get(\"diagram_location\"),\n",
        "                \"generate_output\": group_params.get(\"generate_output\", True),\n",
        "            }\n",
        "\n",
        "            if not group_id_str:\n",
        "                logging.info(f\"SKIPPING group '{group_name}' because it is missing a required 'group_id'.\")\n",
        "                continue\n",
        "\n",
        "            sanitized_group_name = re.sub(r'[^a-zA-Z0-9_.-]', '_', group_name)\n",
        "            group_dir_name = f\"{group_id_str}_{sanitized_group_name}\"\n",
        "            group_output_path = run_output_path / group_dir_name\n",
        "\n",
        "            logging.info(\"=\"*130)\n",
        "            logging.info(f\"EVALUATING EXPERIMENT GROUP: {group_name} ({group_id_str})\")\n",
        "            logging.info(f\"Group Description: {group_context['description']}\")\n",
        "\n",
        "            if not group_params.get(\"run_group\", False):\n",
        "                logging.warning(f\"SKIPPING group '{group_name}' because 'run_group' is set to False.\")\n",
        "                continue\n",
        "\n",
        "            if group_context[\"generate_output\"]:\n",
        "                group_output_path.mkdir(parents=True, exist_ok=True)\n",
        "                group_info = [{\"run_id\": run_id, **group_context}]\n",
        "                pd.DataFrame(group_info).to_csv(group_output_path / Experiment.FNAME_GROUP_INFO, index=False)\n",
        "\n",
        "            random_seed = group_context[\"random_seed\"]\n",
        "            if isinstance(random_seed, int) and random_seed > 0:\n",
        "                random.seed(random_seed)\n",
        "                logging.info(f\"Using random seed {random_seed} for group '{group_name}'.\")\n",
        "            else:\n",
        "                logging.info(f\"No valid random seed for group '{group_name}'. Using random initialization.\")\n",
        "\n",
        "            logging.info(f\"PROCESSING EXPERIMENT GROUP: {group_name} ({len(experiments_in_group)} experiments)\")\n",
        "            logging.info(\"=\"*130)\n",
        "\n",
        "            group_summaries, group_doc_metrics, group_doc_words, group_kg_metrics, group_kg_nodes, group_kg_rels, group_er_results = [], [], [], [], [], [], []\n",
        "\n",
        "            for exp_idx, experiment_params in enumerate(experiments_in_group):\n",
        "\n",
        "                # --- NEW CODE START ---\n",
        "                # Combine the group and experiment IDs to create a fully qualified ID\n",
        "                local_exp_id = experiment_params.get('experiment_id')\n",
        "                fully_qualified_exp_id = f\"{group_id_str}{local_exp_id}\"\n",
        "                experiment_params['experiment_id'] = fully_qualified_exp_id\n",
        "                # --- NEW CODE END ---\n",
        "\n",
        "                experiment_id_str = experiment_params.get('experiment_id')\n",
        "                experiment_name = experiment_params.get('experiment_name', 'N/A')\n",
        "\n",
        "                if not experiment_id_str:\n",
        "                    logging.warning(f\"SKIPPING experiment '{experiment_name}' in group '{group_name}' because it is missing a required 'experiment_id'.\")\n",
        "                    continue\n",
        "\n",
        "                logging.info(\"=\"*130)\n",
        "                logging.info(f\"--- Processing Experiment {exp_idx+1}/{len(experiments_in_group)} in group '{group_name}': {experiment_name} ({experiment_id_str}) ---\")\n",
        "                logging.info(\"=\"*130)\n",
        "\n",
        "                if not experiment_params.get(\"run_experiment\", True):\n",
        "                    logging.info(f\"SKIPPING Experiment '{experiment_name}' because 'run_experiment' is False.\")\n",
        "                    continue\n",
        "\n",
        "                try:\n",
        "                    experiment = Experiment(\n",
        "                        experiment_params=experiment_params,\n",
        "                        group_params=group_context,\n",
        "                        location_map=location_map,\n",
        "                        root_path=ROOT_PATH,\n",
        "                        run_output_path=run_output_path,\n",
        "                        run_timestamp_str=run_timestamp_str\n",
        "                    )\n",
        "                    run_data = experiment.run()\n",
        "                    if run_data:\n",
        "                        group_summaries.extend(run_data[\"summary\"])\n",
        "                        group_doc_metrics.extend(run_data[\"doc_metrics\"])\n",
        "                        group_doc_words.extend(run_data[\"doc_words\"])\n",
        "                        group_kg_metrics.extend(run_data[\"kg_metrics\"])\n",
        "                        group_kg_nodes.extend(run_data[\"kg_nodes\"])\n",
        "                        group_kg_rels.extend(run_data[\"kg_relationships\"])\n",
        "                        group_er_results.extend(run_data[\"er_test_results\"])\n",
        "\n",
        "                        all_summaries.extend(run_data[\"summary\"])\n",
        "                        all_doc_metrics.extend(run_data[\"doc_metrics\"])\n",
        "                        all_doc_words.extend(run_data[\"doc_words\"])\n",
        "                        all_kg_metrics.extend(run_data[\"kg_metrics\"])\n",
        "                        all_kg_nodes.extend(run_data[\"kg_nodes\"])\n",
        "                        all_kg_rels.extend(run_data[\"kg_relationships\"])\n",
        "                        all_er_results.extend(run_data[\"er_test_results\"])\n",
        "\n",
        "                except Exception as e:\n",
        "                    logging.error(f\"Critical error during experiment setup for {experiment_name}: {e}\", exc_info=True)\n",
        "                    error_summary = {\n",
        "                        \"run_timestamp\": run_timestamp_str,\n",
        "                        \"group_id\": group_id_str,\n",
        "                        \"experiment_id\": experiment_id_str,\n",
        "                        \"experiment_name\": experiment_name,\n",
        "                        \"status\": \"Critical Failure\",\n",
        "                        \"error\": str(e),\n",
        "                    }\n",
        "                    group_summaries.extend([error_summary])\n",
        "                    all_summaries.extend([error_summary])\n",
        "\n",
        "\n",
        "            if group_context[\"generate_output\"]:\n",
        "                def save_group_file(data_list, filename):\n",
        "                    if data_list:\n",
        "                        df = pd.DataFrame(data_list)\n",
        "                        filepath = group_output_path / filename\n",
        "                        df.to_csv(filepath, index=False)\n",
        "                        logging.info(f\"Saved group results to: {filepath}\")\n",
        "\n",
        "                save_group_file(group_summaries, Experiment.FNAME_GROUP_SUMMARY)\n",
        "                save_group_file(group_doc_metrics, Experiment.FNAME_DOC_METRICS)\n",
        "                save_group_file(group_doc_words, Experiment.FNAME_DOC_WORDS)\n",
        "                save_group_file(group_kg_metrics, Experiment.FNAME_KG_METRICS)\n",
        "                save_group_file(group_kg_nodes, Experiment.FNAME_KG_NODES)\n",
        "                save_group_file(group_kg_rels, Experiment.FNAME_KG_RELS)\n",
        "                save_group_file(group_er_results, Experiment.FNAME_GROUND_TRUTH)\n",
        "\n",
        "        total_end_time = datetime.now(timezone.utc)\n",
        "        logging.info(\"=\"*130)\n",
        "        logging.info(\"ALL EXPERIMENTS COMPLETED\")\n",
        "        logging.info(f\"Total execution time: {total_end_time - total_start_time}\")\n",
        "        logging.info(\"=\"*130)\n",
        "        logging.info(f\"--- Done Experiment Run: {len(experiment_groups)} groups defined ---\")\n",
        "        logging.info(f\"Run ID: {run_id}\")\n",
        "\n",
        "        if all_summaries:\n",
        "            def save_master_file(data_list, filename):\n",
        "                if data_list:\n",
        "                    df = pd.DataFrame(data_list)\n",
        "                    filepath = run_output_path / filename\n",
        "                    df.to_csv(filepath, index=False)\n",
        "                    logging.info(f\"Successfully saved master results to: {filepath}\")\n",
        "\n",
        "            save_master_file(all_summaries, Experiment.FNAME_MASTER_SUMMARY)\n",
        "            save_master_file(all_doc_metrics, Experiment.FNAME_DOC_METRICS)\n",
        "            save_master_file(all_doc_words, Experiment.FNAME_DOC_WORDS)\n",
        "            save_master_file(all_kg_metrics, Experiment.FNAME_KG_METRICS)\n",
        "            save_master_file(all_kg_nodes, Experiment.FNAME_KG_NODES)\n",
        "            save_master_file(all_kg_rels, Experiment.FNAME_KG_RELS)\n",
        "            save_master_file(all_er_results, Experiment.FNAME_GROUND_TRUTH)\n",
        "\n",
        "            # --- NEW: Generate LaTeX Report ---\n",
        "            try:\n",
        "                latex_gen = LatexGenerator(run_output_path, run_id, experiment_groups, all_summaries)\n",
        "                latex_gen.generate_report()\n",
        "            except Exception as e:\n",
        "                logging.error(f\"Failed to generate LaTeX report: {e}\", exc_info=True)\n",
        "\n",
        "\n",
        "            summary_df = pd.DataFrame(all_summaries)\n",
        "            pd.set_option('display.max_rows', 500)\n",
        "            pd.set_option('display.max_columns', 500)\n",
        "            pd.set_option('display.width', 1200)\n",
        "            print(\"\\n--- MASTER EXPERIMENT RUN SUMMARY ---\")\n",
        "\n",
        "            # --- MODIFICATION START ---\n",
        "            # Added 'ground_truth_source' to the final display.\n",
        "            cols_to_show = [\n",
        "                'group_id', 'experiment_id', 'experiment_name', 'status', 'duration_seconds',\n",
        "                'ground_truth_source',\n",
        "                'overall_score',\n",
        "                'entity_f1_score',\n",
        "                'relationship_f1_score',\n",
        "                'error'\n",
        "            ]\n",
        "            # --- MODIFICATION END ---\n",
        "\n",
        "            existing_cols = [col for col in cols_to_show if col in summary_df.columns]\n",
        "            print(summary_df[existing_cols].to_string())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}